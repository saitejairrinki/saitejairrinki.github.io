{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DevOps Engineer \u00b6 Click here About","title":"Home"},{"location":"#devops-engineer","text":"Click here About","title":"DevOps Engineer"},{"location":"about/","text":"About \u00b6 Hello, my name is SAITEJA IRRINKI, and I work as a DevOps Engineer in Build & Release. I'm experienced in Infrastructure Provisioning, Configuration Management, Version Control, Code Quality, Environment Administration, Defect Tracking, Release Management, Continuous Integration, and Continuous Delivery, Cloud Computing, Scripting for Automation, Static Web Development, Orchestration, Technologies, and Operating Systems- Linux and Windows. saitejairrinki91@gmail.com https://www.instagram.com/saitejairrinki/ https://saitejairrinki.medium.com/","title":"About me"},{"location":"about/#about","text":"Hello, my name is SAITEJA IRRINKI, and I work as a DevOps Engineer in Build & Release. I'm experienced in Infrastructure Provisioning, Configuration Management, Version Control, Code Quality, Environment Administration, Defect Tracking, Release Management, Continuous Integration, and Continuous Delivery, Cloud Computing, Scripting for Automation, Static Web Development, Orchestration, Technologies, and Operating Systems- Linux and Windows. saitejairrinki91@gmail.com https://www.instagram.com/saitejairrinki/ https://saitejairrinki.medium.com/","title":" About"},{"location":"awsdevops/","text":"DevOps \u00b6 DevOps is the combination of cultural philosophies, practices, and tools that increases an organization\u2019s ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.","title":"Awsdevops"},{"location":"awsdevops/#devops","text":"DevOps is the combination of cultural philosophies, practices, and tools that increases an organization\u2019s ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.","title":"DevOps"},{"location":"blog/","text":"My DevOps Blog \u00b6 Welcome to my DevOps Blog. Here you will find a collection of articles and tutorials on various DevOps topics. Whether you are just getting started with DevOps or a pro, I hope you find something useful here. This blog is all about my thoughts, experiences, Challanges and lessons learned in the world of DevOps. I have been working in the IT industry and have been involved in a variety of roles including InfraStructure Provisioning, system administration and operations. In recent years I have become very interested in the DevOps movement and how it can help organisations to improve the way they deliver software. I hope you enjoy reading my blog and please feel free to leave any comments or questions saitejairrinki91@gmail.com Thanks for reading!","title":"About Blog"},{"location":"blog/#my-devops-blog","text":"Welcome to my DevOps Blog. Here you will find a collection of articles and tutorials on various DevOps topics. Whether you are just getting started with DevOps or a pro, I hope you find something useful here. This blog is all about my thoughts, experiences, Challanges and lessons learned in the world of DevOps. I have been working in the IT industry and have been involved in a variety of roles including InfraStructure Provisioning, system administration and operations. In recent years I have become very interested in the DevOps movement and how it can help organisations to improve the way they deliver software. I hope you enjoy reading my blog and please feel free to leave any comments or questions saitejairrinki91@gmail.com Thanks for reading!","title":" My DevOps Blog  "},{"location":"job_roles/","text":"DevOps ENGINEER ROLES \u00b6 AWS Tasks: \u00b6 Managing Ec2 Instances, EIP, Network Interfaces, Security Groups & Key Pairs Managing EBS Volumes, AMI & Snapshots (Backup & Restore, Migration, etc.) Setup & Managing Elastic Load Balancers, ACM & Autoscaling Groups Setting & Managing Cloudwatch Alarms on metrics from Ec2, ELB & RDS Creating & Managing RDS Instances, RDS Snapshots, Updating Parameters Groups AWS CLI for any AWS Tasks Cloud Migration using Lift & Shift Strategy on AWS, Services using \u00b6 VPC, Ec2, S3, Application Load Balancer, Route53 IAM to give secure access to AWS account using MFA Tightly controlled Security Group for firewall rules of EC2 EBS volume for storage on Ec2, Snapshot for Backups of EBS Autoscaling for Automatic scaling of Ec2 instance based on CPU usage Modernization on AWS Cloud, Services used Beanstalk for PAAS for Tomcat Web App RDS for MySQL Database Object storage S3 to store and retrieve files Route53 for Private & Public Hosted zones/Records Amazon MQ for fully managed RabbitMQ ElastiCache for in-memory datastore in cloud Monitoring with CloudWatch, Grafana Notification using SNS AWS Cloud Automation using \u00b6 Ansible CloudFormation (Stacks) AWS Securities \u00b6 Inspector & Best practices IAM management Continuous Delivery of Java Web Application \u00b6 CICD Pipeline using Jenkins, Git, Maven, Nexus, S3 & SonarQube Deploying Artifact to Beanstack Jenkins Pipeline As A Code for CICD Website Automation through Jenkins whenever there is a GIT Commit Configuration Management using Ansible \u00b6 Ansible AdHoc commands to execute remote tasks Ansible playbook for Service/Server Deployments Ansible playbook to setup VPC & Bastion Host on AWS Writing our own configuration file (ansible.cfg) Ansible Roles for modular & reusable automation framework Docker Containers \u00b6 Building customized docker images using Dockerfile Docker-compose to define & run MultiContainer Docker Application Kubernetes \u00b6 Creating Production grade K8s cluster using Kops & Kubeadm Hosting Containerized Application on K8s cluster using Pod, Service, Replication Controller, Deployment, Secrets & ConfigMap MkDocs \u00b6 Designing and Building a Static Website using MkDocs (The Current Website you're watching)","title":"Job Roles"},{"location":"job_roles/#devops-engineer-roles","text":"","title":"DevOps ENGINEER ROLES"},{"location":"job_roles/#aws-tasks","text":"Managing Ec2 Instances, EIP, Network Interfaces, Security Groups & Key Pairs Managing EBS Volumes, AMI & Snapshots (Backup & Restore, Migration, etc.) Setup & Managing Elastic Load Balancers, ACM & Autoscaling Groups Setting & Managing Cloudwatch Alarms on metrics from Ec2, ELB & RDS Creating & Managing RDS Instances, RDS Snapshots, Updating Parameters Groups AWS CLI for any AWS Tasks","title":"AWS Tasks:"},{"location":"job_roles/#cloud-migration-using-lift-shift-strategy-on-aws-services-using","text":"VPC, Ec2, S3, Application Load Balancer, Route53 IAM to give secure access to AWS account using MFA Tightly controlled Security Group for firewall rules of EC2 EBS volume for storage on Ec2, Snapshot for Backups of EBS Autoscaling for Automatic scaling of Ec2 instance based on CPU usage Modernization on AWS Cloud, Services used Beanstalk for PAAS for Tomcat Web App RDS for MySQL Database Object storage S3 to store and retrieve files Route53 for Private & Public Hosted zones/Records Amazon MQ for fully managed RabbitMQ ElastiCache for in-memory datastore in cloud Monitoring with CloudWatch, Grafana Notification using SNS","title":"Cloud Migration using Lift &amp; Shift Strategy on AWS, Services using"},{"location":"job_roles/#aws-cloud-automation-using","text":"Ansible CloudFormation (Stacks)","title":"AWS Cloud Automation using "},{"location":"job_roles/#aws-securities","text":"Inspector & Best practices IAM management","title":"AWS Securities"},{"location":"job_roles/#continuous-delivery-of-java-web-application","text":"CICD Pipeline using Jenkins, Git, Maven, Nexus, S3 & SonarQube Deploying Artifact to Beanstack Jenkins Pipeline As A Code for CICD Website Automation through Jenkins whenever there is a GIT Commit","title":"Continuous Delivery of Java Web Application  "},{"location":"job_roles/#configuration-management-using-ansible","text":"Ansible AdHoc commands to execute remote tasks Ansible playbook for Service/Server Deployments Ansible playbook to setup VPC & Bastion Host on AWS Writing our own configuration file (ansible.cfg) Ansible Roles for modular & reusable automation framework","title":"Configuration Management using Ansible "},{"location":"job_roles/#docker-containers","text":"Building customized docker images using Dockerfile Docker-compose to define & run MultiContainer Docker Application","title":"Docker Containers "},{"location":"job_roles/#kubernetes","text":"Creating Production grade K8s cluster using Kops & Kubeadm Hosting Containerized Application on K8s cluster using Pod, Service, Replication Controller, Deployment, Secrets & ConfigMap","title":"Kubernetes "},{"location":"job_roles/#mkdocs","text":"Designing and Building a Static Website using MkDocs (The Current Website you're watching)","title":"MkDocs"},{"location":"profile_summary/","text":"PROFILESUMMARY \u00b6 Experience in IT area comprising the configuration management, Deploy, CI/CD pipeline, AWS, and DevOps methodologies. Expertise in troubleshooting the problems generated while building and deploying. Working Experience on Git, GitHub, and Jenkins. Debugging issues if there is any failure in broken Jenkins builds and maintaining Jenkins build pipeline. Expertise Knowledge in Source Code Management (version control system) tools using GIT. Experienced in Branching, Merging, and Tagging concepts in Version Control tools like GIT. Proficient in developing Continuous Integration / Continuous Delivery pipelines. Experience with containerization tools like Docker and Kubernetes. Implemented Docker-based Continues Integration and Deployment framework. Strong experience in building tools and packaging the source code using Maven. Scheduled builds overnight to support development needs using Jenkins, Git, and Maven. Experience in integrating Unit Tests and Code Quality Analysis Tools like SonarQube. Experience in using Nexus Repository Manager and S3 Bucket for Maven builds. Experience in orchestration tools like Ansible and Kubernetes. Experience with services IAM, Compute Engine, Kubernetes Engine, Storage Services, S3 Bucket, and VPC Network. Experience with Amazon Web services Creating, configuring, and Managing EC2, Storage, IAM, S3, VPC, ELB, EFS, SNS, Route53, and some more services in AWS. Experience using Apache Tomcat & Red Hat Server application servers for deployments. Working experience with operating systems like Linux and Windows. Performed continuous Build and Deployments to multiple DEV, QA, PRE-Prod, and PROD environments. Know about Google Cloud Platform. Have good Knowledge of Terraform. Have good Knowledge of Shell Scripting. Have Knowledge of Kubernetes Core Concepts. Experience with Styra Declarative Authorization Service (DAS). Known about Open Policy Agent (OPA). ACADEMIC DETAILS \u00b6 B.Tech - 2020 PROFESSIONAL EXPERIENCE \u00b6 Working as a DevOps Engineer from March-2021 to till-date. STRENGTHS \u00b6 Flexibility and adaptability to work in any environment. Good troubleshooting skills. Willingness to accept any challenge irrespective of its complexity. Good team player with a positive attitude.","title":"profile"},{"location":"profile_summary/#profilesummary","text":"Experience in IT area comprising the configuration management, Deploy, CI/CD pipeline, AWS, and DevOps methodologies. Expertise in troubleshooting the problems generated while building and deploying. Working Experience on Git, GitHub, and Jenkins. Debugging issues if there is any failure in broken Jenkins builds and maintaining Jenkins build pipeline. Expertise Knowledge in Source Code Management (version control system) tools using GIT. Experienced in Branching, Merging, and Tagging concepts in Version Control tools like GIT. Proficient in developing Continuous Integration / Continuous Delivery pipelines. Experience with containerization tools like Docker and Kubernetes. Implemented Docker-based Continues Integration and Deployment framework. Strong experience in building tools and packaging the source code using Maven. Scheduled builds overnight to support development needs using Jenkins, Git, and Maven. Experience in integrating Unit Tests and Code Quality Analysis Tools like SonarQube. Experience in using Nexus Repository Manager and S3 Bucket for Maven builds. Experience in orchestration tools like Ansible and Kubernetes. Experience with services IAM, Compute Engine, Kubernetes Engine, Storage Services, S3 Bucket, and VPC Network. Experience with Amazon Web services Creating, configuring, and Managing EC2, Storage, IAM, S3, VPC, ELB, EFS, SNS, Route53, and some more services in AWS. Experience using Apache Tomcat & Red Hat Server application servers for deployments. Working experience with operating systems like Linux and Windows. Performed continuous Build and Deployments to multiple DEV, QA, PRE-Prod, and PROD environments. Know about Google Cloud Platform. Have good Knowledge of Terraform. Have good Knowledge of Shell Scripting. Have Knowledge of Kubernetes Core Concepts. Experience with Styra Declarative Authorization Service (DAS). Known about Open Policy Agent (OPA).","title":"PROFILESUMMARY"},{"location":"profile_summary/#academic-details","text":"B.Tech - 2020","title":"ACADEMIC DETAILS"},{"location":"profile_summary/#professional-experience","text":"Working as a DevOps Engineer from March-2021 to till-date.","title":"PROFESSIONAL EXPERIENCE"},{"location":"profile_summary/#strengths","text":"Flexibility and adaptability to work in any environment. Good troubleshooting skills. Willingness to accept any challenge irrespective of its complexity. Good team player with a positive attitude.","title":"STRENGTHS"},{"location":"project/","text":"PROJECT 1 \u00b6 Role: DevOps \u00b6 Environment: Git, GitHub, Maven, Apache Tomcat, Jenkins, Linux, SonarQube, Nexus, AWS, Ansible. \u00b6 Roles and Responsibilities : \u00b6 Configured Git with Jenkins and scheduled jobs using the POLL SCM option Installed and configured GIT and communicated with the repositories in GitHUB. Collaborate with different teams to deploy application code into Dev, QA, and Staging. Installing and updating the Jenkins plugins to achieve CI/CD. Responsible for installing Jenkins master and slave nodes. Created Jenkins CICD pipelines for continuous build & deployment and integrated Junit and SonarQube plugins in Jenkins for automated testing and code quality check. Integrated SonarQube with Jenkins for continuous inspection of code quality and analysis with SonarQube scanner for Maven. Managed Sonatype Nexus repositories to download the artifacts (jar, war & ear) during the build. Wrote playbook manifests for deploying, configuring, and managing components. Managing the working environments through configuration management tools ansible. Working with developers and Testers to test the source code and applications through Jenkins plugins. Installation of apache, tomcat, and troubleshooting web server issues. Administration and maintenance of servers using Red Hat Linux/CentOS-7,8. Installing and configuration of ansible server. * Implemented AWS solutions using EC2, S3, EBS, ELB, Route53, Auto scaling groups. Built servers using AWS, importing volumes, launching EC2, creating Security groups, Auto-scaling, Load balancers (ELBs) using Cloud formation templates & AMIs using Infrastructure as a Service (IaaS Including EC2 and S3), focusing on high availability, fault tolerance, and auto-scaling. Configured ELB with different launch configurations using AMI and EC2 Autoscaling groups. Creating S3 buckets and S3 life cycle policies and bucket policies (Read/Write). Creating EBS Volumes and snapshots and attaching them to the EC2 instances. PROJECT 2 \u00b6 Environment: Git, GitHub, Maven, Nexus, SonarQube, Jenkins, Docker, Kubernetes, AWS, Linux. \u00b6 Roles and Responsibilities : \u00b6 Involved in CI/CD process and integrated GIT, Nexus, SonarQube, and Maven artifacts build with and and and and Jenkins and creating Docker image and using the Docker image to deploy over Kubernetes. Building and deploying various microservices in EKS. Creating and maintaining namespaces, config maps, secrets, service, ingress, RBAC in Kubernetes. Implemented and maintained the Branching and build/ release strategies utilizing GIT. Experience with container-based deployments using Docker, working with Docker Images, Docker Hub and Docker-registries and Kubernetes. Building/Maintaining Docker container clusters managed by Kubernetes Linux, Bash, GIT, Docker. Implemented docker-maven-plugin in maven pom to build docker images for all microservices and later used Docker file to build the docker images from the java jar files. Utilized Kubernetes for the runtime environment of the CI/CD system to build, and test deploy. Experience in working on AWS and its services like AWS IAM, VPC, EC2, EKS, EBS, S3, ELB, Auto Scaling, Route 53, Cloud Front, Cloud Watch, Cloud Trail, and SNS. Experienced in Cloud automation using AWS Cloud Formation templates to create customized VPC, subnets, NAT, EC2 instances, ELB, and Security groups. Experienced in creating complex IAM policies, Roles, and user management for delegated acceaccess withAWS. Identify, troubleshoot and resolve issues related to the build and deploy process. Deploying Docker images in Kubernetes cluster using Yaml files and exposing the application to the internet using service object.","title":"Projects"},{"location":"project/#project-1","text":"","title":"PROJECT 1"},{"location":"project/#role-devops","text":"","title":"Role: DevOps"},{"location":"project/#environment-git-github-maven-apache-tomcat-jenkins-linux-sonarqube-nexus-aws-ansible","text":"","title":"Environment: Git, GitHub, Maven, Apache Tomcat, Jenkins, Linux, SonarQube, Nexus, AWS, Ansible."},{"location":"project/#roles-and-responsibilities","text":"Configured Git with Jenkins and scheduled jobs using the POLL SCM option Installed and configured GIT and communicated with the repositories in GitHUB. Collaborate with different teams to deploy application code into Dev, QA, and Staging. Installing and updating the Jenkins plugins to achieve CI/CD. Responsible for installing Jenkins master and slave nodes. Created Jenkins CICD pipelines for continuous build & deployment and integrated Junit and SonarQube plugins in Jenkins for automated testing and code quality check. Integrated SonarQube with Jenkins for continuous inspection of code quality and analysis with SonarQube scanner for Maven. Managed Sonatype Nexus repositories to download the artifacts (jar, war & ear) during the build. Wrote playbook manifests for deploying, configuring, and managing components. Managing the working environments through configuration management tools ansible. Working with developers and Testers to test the source code and applications through Jenkins plugins. Installation of apache, tomcat, and troubleshooting web server issues. Administration and maintenance of servers using Red Hat Linux/CentOS-7,8. Installing and configuration of ansible server. * Implemented AWS solutions using EC2, S3, EBS, ELB, Route53, Auto scaling groups. Built servers using AWS, importing volumes, launching EC2, creating Security groups, Auto-scaling, Load balancers (ELBs) using Cloud formation templates & AMIs using Infrastructure as a Service (IaaS Including EC2 and S3), focusing on high availability, fault tolerance, and auto-scaling. Configured ELB with different launch configurations using AMI and EC2 Autoscaling groups. Creating S3 buckets and S3 life cycle policies and bucket policies (Read/Write). Creating EBS Volumes and snapshots and attaching them to the EC2 instances.","title":"Roles and Responsibilities :"},{"location":"project/#project-2","text":"","title":"PROJECT 2"},{"location":"project/#environment-git-github-maven-nexus-sonarqube-jenkins-docker-kubernetes-aws-linux","text":"","title":"Environment: Git, GitHub, Maven, Nexus, SonarQube, Jenkins, Docker, Kubernetes, AWS, Linux."},{"location":"project/#roles-and-responsibilities_1","text":"Involved in CI/CD process and integrated GIT, Nexus, SonarQube, and Maven artifacts build with and and and and Jenkins and creating Docker image and using the Docker image to deploy over Kubernetes. Building and deploying various microservices in EKS. Creating and maintaining namespaces, config maps, secrets, service, ingress, RBAC in Kubernetes. Implemented and maintained the Branching and build/ release strategies utilizing GIT. Experience with container-based deployments using Docker, working with Docker Images, Docker Hub and Docker-registries and Kubernetes. Building/Maintaining Docker container clusters managed by Kubernetes Linux, Bash, GIT, Docker. Implemented docker-maven-plugin in maven pom to build docker images for all microservices and later used Docker file to build the docker images from the java jar files. Utilized Kubernetes for the runtime environment of the CI/CD system to build, and test deploy. Experience in working on AWS and its services like AWS IAM, VPC, EC2, EKS, EBS, S3, ELB, Auto Scaling, Route 53, Cloud Front, Cloud Watch, Cloud Trail, and SNS. Experienced in Cloud automation using AWS Cloud Formation templates to create customized VPC, subnets, NAT, EC2 instances, ELB, and Security groups. Experienced in creating complex IAM policies, Roles, and user management for delegated acceaccess withAWS. Identify, troubleshoot and resolve issues related to the build and deploy process. Deploying Docker images in Kubernetes cluster using Yaml files and exposing the application to the internet using service object.","title":"Roles and Responsibilities :"},{"location":"resume/","text":"To View My Resume please click here Resume","title":"Resume"},{"location":"technical_skills/","text":"TECHNICAL SKILLS \u00b6 Category Tools Technologies & Softwares Operating Systems - Linux Windows Virtualization - Vagrant Scripting Languages - Shell Scripting Containerization Tools - Docker Kubernetes Configuration Management Tool - Ansible Infrastructure as Code - Terraform Cloud Formation Cloud Platform - AWS GCP Version Control Tool - GIT Build Software - Maven Continuous Integration Tool - Jenkins Build & Release - Azure DevOps Web/App Server - Apache Tomcat Ticketing Tool - Freshdesk Static Web Development - MkDocs Repository Manager - Nexus Repository , S3 Monitoring - CloudWatch Grafana Code Quality Analysis - SonarQube Databases - RDS MySQL","title":"skills"},{"location":"technical_skills/#technical-skills","text":"Category Tools Technologies & Softwares Operating Systems - Linux Windows Virtualization - Vagrant Scripting Languages - Shell Scripting Containerization Tools - Docker Kubernetes Configuration Management Tool - Ansible Infrastructure as Code - Terraform Cloud Formation Cloud Platform - AWS GCP Version Control Tool - GIT Build Software - Maven Continuous Integration Tool - Jenkins Build & Release - Azure DevOps Web/App Server - Apache Tomcat Ticketing Tool - Freshdesk Static Web Development - MkDocs Repository Manager - Nexus Repository , S3 Monitoring - CloudWatch Grafana Code Quality Analysis - SonarQube Databases - RDS MySQL","title":"TECHNICAL SKILLS"},{"location":"devops/ansible/","text":"Ansible \u00b6 Ansible is an open-source IT Configuration Management, Deployment & Orchestration tool. It aims to provide large productivity gains to a wide variety of automation challenges. This tool is very simple to use yet powerful enough to automate complex multi-tier IT application environments. Installing Ansible \u00b6 Ubuntu CentOS sudo apt update sudo apt install software-properties-common -y sudo add-apt-repository --yes --update ppa:ansible/ansible sudo apt install ansible -y ansible --version sudo yum install epel-release -y sudo yum install ansible -y ansible --version SCP Commands \u00b6 To Copy files From Localmachine to Remote machine scp -i <private-key-file-path> <files to copy> <user>@<IP>:<Remote-machine-path> Example scp -i Downloads/control.pem sample.txt ubuntu@54.165.128.104:/home/ubuntu/ To Copy Files from Remotemachine to Local machine scp -i <private-key-file-path> <user>@<IP>:<remote-files-path> <Localmachine-path> Example scp -i Downloads/control.pem ubuntu@54.165.128.104:/home/ubuntu/sample.txt Desktop/ Sample Inventory File \u00b6 Inventory ##Host Level webserver01 ansible_host =< Private IP > webserver02 ansible_host =< Private IP > webserver03 ansible_host =< Private IP > dbserver01 ansible_host =< Private IP > dbserver02 ansible_host =< Private IP > ansible_user = ubuntu ##Group Level [ Group1 ] webserverserver01 webserverserver02 webserverserver03 [ Group2 ] dbserver01 dbserver02 ##Parent Level [ dc_mumbai : children ] webservergrp dbsrvgrp ##Variables [ dc_mumbai : vars ] ansible_user =< user > ansible_ssh_private_key_file =< key - path > Info Host level has the highest priority, If you mention anything like username or Keyfile etc. It will take only, which are mentioned at the host level. Ansible Commands \u00b6 To test the connection of a particular Remote Machine ansible -i <Inventoryfile path> -m ping <hostname> To test the connection of a particular Group of Remote Machines ansible -i <Inventoryfile path> -m ping <Groupname> To test the connection of All Remote Machine ansible -i <Inventoryfile path> -m ping all To see details about the machine ansible -i <Inventoryfile path> -m setup <hostname> Some Example Ad hoc Commands \u00b6 Ad hoc Commands Copy files to Remote machines name start with web ansible -i <Inventoryfile path> -m copy -a \"src=index.html dest=/var/www/html/index.html\" 'web*' --become Installing httpd in centos Remote machine ansible -i <Inventoryfile path> -m yum -a \"name=httpd state=present\" websrvgrp --become Start & Enable httpd in centos Remote machine ansible -i <Inventoryfile path> -m service -a \"name=httpd state=started enabled=yes\" websrvgrp --become Info Ansible Playbooks should be with .yml or .yaml Extension for example vim sample.yml Playbook For Creating Files & Directories \u00b6 Sample Ansible Playbooks - name : Creating Files & Directories hosts : <host> become : yes tasks : - name : Creating a Directory file : path : /tmp/welcome state : directory - name : Creating a File file : path : /tmp/sample.txt state : touch To Execute the playbook ansible-playbook -i <Inventory file path> sample.yml Writing Playbook For Installing Httpd service in remote machines with start and enable and copying index.html files from local machine to the remote machine \u00b6 - name : Install httpd and start the service hosts : all tasks : - name : Installing the Apache package yum : name : httpd state : present - name : Starting service service : name : httpd state : started enabled : yes - name : Copy file with owner and permissions copy : src : ./index.html dest : /var/www/html/index.html - name : Restarting service service : name : httpd state : restarted Writing Playbook for Setting Up Website in Remote Machine \u00b6 - name : Setting up Website hosts : websrv gather_facts : False become : True tasks : - name : Installing Packages in CentOS yum : name : \"{{item}}\" state : present when : ansible_distribution == \"CentOS\" loop : - httpd - wget - unzip - name : Start & Enable httpd service : name : httpd state : started enabled : yes - name : Downloading Source code get_url : url : https://www.tooplate.com/zip-templates/2114_pixie.zip dest : /opt - name : Unarchive a file that is already on the remote machine unarchive : src : /opt/2114_pixie.zip dest : /opt remote_src : yes - name : Deploy Website copy : src : /opt/2114_pixie/ dest : /var/www/html/ remote_src : yes - name : Restarting httpd service service : name : httpd state : restarted Writing Playbook for Setting Up Website in Remote Machine with Conditions & Handlers. \u00b6 - name : Writing playbook for loops and conditions hosts : all tasks : - name : Install packages on centos yum : name : \"{{item}}\" state : present when : ansible_distribution == \"CentOS\" loop : - httpd - wget - unzip - zip - git - name : Install packages on Ubuntu apt : name : \"{{item}}\" state : present update_cache : yes when : ansible_distribution == \"Ubuntu\" loop : - apache2 - wget - unzip - zip - git - name : Start & enable service on CentOS service : name : httpd state : started enabled : yes when : ansible_distribution == \"CentOS\" - name : Start & enable service on Ubuntu service : name : apache2 state : started enabled : yes when : ansible_distribution == \"Ubuntu\" - name : Push index.html on centos copy : src : index.html dest : /var/www/html/ backup : yes when : ansible_distribution == \"CentOS\" notify : - Restart service on CentOS - name : Push index.html on ubuntu copy : src : index.html dest : /var/www/html/ backup : yes when : ansible_distribution == \"Ubuntu\" notify : - Restart service on Ubuntu handlers : - name : Restart service on CentOS service : name : httpd state : restarted enabled : yes when : ansible_distribution == \"CentOS\" - name : Restart service on Ubuntu service : name : apache2 state : restarted enabled : yes when : ansible_distribution == \"Ubuntu\" Writing Playbook to Create VPC in AWS Cloud and Including Variables from different File \u00b6 Requirements python >= 3.6 boto3 >= 1.15.0 botocore >= 1.18.0 apt install python3-pip pip install boto pip install boto3 pip install botocore Creating File to store Variables vim vpc_setup.txt vpc_name : \"Vprofile-vpc\" #Vpc-range vpcrange : ' 172.21.0.0 / 16 ' #subnet range pubip1 : ' 172.21.1.0 / 24 ' pubip2 : ' 172.21.2.0 / 24 ' pubip3 : ' 172.21.3.0 / 24 ' pvtip1 : ' 172.21.4.0 / 24 ' pvtip2 : ' 172.21.5.0 / 24 ' pvtip3 : ' 172.21.6.0 / 24 ' #region region : ' us - east -2 ' #zone names zone1 : us - east -2 a zone2 : us - east -2 b zone3 : us - east -2 c state : present Playbook - hosts : localhost connection : local gather_facts : False tasks : - name : Import vpc variables include_vars : /path/vpc_setup - name : create vpc ec2_vpc_net : name : \"{{vpc_name}}\" cidr_block : \"{{vpcrange}}\" region : \"{{region}}\" dns_support : yes dns_hostnames : yes tenancy : default state : \"{{state}}\" register : vpcout - name : Create a public subnet for zone1 ec2_vpc_subnet : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" az : \"{{zone1}}\" state : \"{{state}}\" cidr : \"{{pubip1}}\" map_public : yes tags : Name : vprofile_pubsub1 register : pubsub1_out - name : Create a public subnet for zone2 ec2_vpc_subnet : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" az : \"{{zone2}}\" state : \"{{state}}\" cidr : \"{{pubip2}}\" map_public : yes tags : Name : vprofile_pubsub2 register : pubsub2_out - name : Create a public subnet for zone3 ec2_vpc_subnet : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" az : \"{{zone3}}\" state : \"{{state}}\" cidr : \"{{pubip3}}\" map_public : yes tags : Name : vprofile_pubsub3 register : pubsub3_out - name : Create a private subnet for zone1 ec2_vpc_subnet : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" az : \"{{zone1}}\" state : \"{{state}}\" cidr : \"{{pvtip1}}\" map_public : yes tags : Name : vprofile_pvtsub1 register : pvtsub1_out - name : Create a private subnet for zone2 ec2_vpc_subnet : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" az : \"{{zone2}}\" state : \"{{state}}\" cidr : \"{{pvtip2}}\" map_public : yes tags : Name : vprofile_pvtsub2 register : pvtsub2_out - name : Create a private subnet for zone3 ec2_vpc_subnet : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" az : \"{{zone3}}\" state : \"{{state}}\" cidr : \"{{pvtip3}}\" map_public : yes tags : Name : vprofile_pvtsub3 register : pvtsub3_out - name : Internet gateway setup ec2_vpc_igw : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" state : \"{{state}}\" tags : Name : vprofile_IGW register : igw_out - name : public subnet route table ec2_vpc_route_table : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" tags : Name : vprofile_Public subnets : - \"{{pubsub1_out.subnet.id}}\" - \"{{pubsub2_out.subnet.id}}\" - \"{{pubsub3_out.subnet.id}}\" routes : - dest : 0.0.0.0/0 gateway_id : \"{{ igw_out.gateway_id }}\" register : public_route_table - name : Create a new nat gateway and allocate a new EIP if a nat gateway does not yet exist in the subnet. ec2_vpc_nat_gateway : state : \"{{state}}\" subnet_id : \"{{pubsub1_out.subnet.id}}\" wait : true region : \"{{region}}\" if_exist_do_not_create : true register : nat_out - name : private subnet route table ec2_vpc_route_table : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" tags : Name : vprofile_Private subnets : - \"{{pvtsub1_out.subnet.id}}\" - \"{{pvtsub2_out.subnet.id}}\" - \"{{pvtsub3_out.subnet.id}}\" routes : - dest : 0.0.0.0/0 gateway_id : \"{{nat_out.nat_gateway_id}}\" register : private_route_table - debug : var : \"{{item}}\" loop : - vpcout.vpc.id - pubsub1_out.subnet.id - pubsub2_out.subnet.id - pubsub3_out.subnet.id - pvtsub1_out.subnet.id - pvtsub2_out.subnet.id - pvtsub3_out.subnet.id - igw_out.gateway_id - public_route_table.route_table.id - nat_out.nat_gateway_id - private_route_table.route_table.id - set_fact : vpcid : \"{{vpcout.vpc.id}}\" pubsublid : \"{{ pubsub1_out.subnet.id }}\" pubsub2id : \"{{ pubsub2_out.subnet.id }}\" pubsub3id : \"{{ pubsub3_out.subnet.id }}\" privsublid : \"{{ pvtsub1_out.subnet.id }}\" privsub2id : \"{{ pvtsub2_out.subnet.id }}\" privsub3id : \"{{ pvtsub3_out.subnet.id }}\" igwid : \"{{ igw_out.gateway_id }}\" pubRTid : \"{{ public_route_table.route_table.id }}\" NATGWid : \"{{ nat_out.nat_gateway_id }}\" privRTid : \"{{ private_route_table.route_table.id }}\" cacheable : yes - name : creating file for vpc output copy : content : \"vpcid: {{vpcout.vpc.id}}\\n pubsublid: {{ pubsub1_out.subnet.id }}\\npubsub2id: {{ pubsub2_out.subnet.id }}\\npubsub3id: {{ pubsub3_out.subnet.id }}\\nprivsublid: {{ pvtsub1_out.subnet.id }}\\nprivsub2id: {{ pvtsub2_out.subnet.id }}\\nprivsub3id: {{ pvtsub3_out.subnet.id }}\\nigwid: {{ igw_out.gateway_id }}\\npubRTid: {{ public_route_table.route_table.id }}\\nNATGWid: {{ nat_out.nat_gateway_id }}\\nprivRTid: {{ private_route_table.route_table.id }}\" dest : /home/ubuntu/Vprofile/vars/output_vars Launching Ec2 Instance in AWS Cloud \u00b6 Requirements python >= 3.6 boto3 >= 1.15.0 botocore >= 1.18.0 apt install python3-pip pip install boto pip install boto3 pip install botocore Launching Instance Playbook - name : Launching Ec2 Instance hosts : localhost connection : local tasks : - name : Creating Key pair amazon.aws.ec2_key : name : samplekey region : us-west-1 register : key - debug : var : key - name : Storing private key into a file copy : content : \"{{key.key.private_key}}\" dest : \"./sample.pem\" mode : 0600 when : key.changed - name : Creating Security Group amazon.aws.ec2_group : name : mysg description : Allowing 22 and 80 vpc_id : vpc-0c8e70cf05b1342ac region : us-west-1 rules : - proto : tcp from_port : 22 to_port : 22 cidr_ip : 0.0.0.0/0 rule_desc : allow all on port 80 & 22 register : sg_out - name : Launching bastion_host ec2 : key_name : \"samplekey\" region : us-west-1 instance_type : t2.micro image : ami-0573b70afecda915d wait : yes wait_timeout : 300 instance_tags : name : \"Ansible Instance\" project : vprofile owner : devops team exact_count : 1 count_tag : name : \"Ansible Instance\" project : vprofile owner : devops team group_id : \"{{sg_out.group_id}}\" vpc_subnet_id : subnet-0c4734c845e549cda register : instance Ansible Configuration File \u00b6 Note You should save the configuration file with name ansible.cfg vim ansible.cfg Ansible Configuration File [ defaults ] host_key_checking = False inventory = <Inventory File Path> timeout = 20 log_path = /var/log/ansible_world.log remote_port = 22 remote_user = <username> [ privilege_escalation ] become = True become_method = sudo become_user = root become_ask_pass = False","title":"Ansible"},{"location":"devops/ansible/#ansible","text":"Ansible is an open-source IT Configuration Management, Deployment & Orchestration tool. It aims to provide large productivity gains to a wide variety of automation challenges. This tool is very simple to use yet powerful enough to automate complex multi-tier IT application environments.","title":"Ansible"},{"location":"devops/ansible/#installing-ansible","text":"Ubuntu CentOS sudo apt update sudo apt install software-properties-common -y sudo add-apt-repository --yes --update ppa:ansible/ansible sudo apt install ansible -y ansible --version sudo yum install epel-release -y sudo yum install ansible -y ansible --version","title":"Installing Ansible"},{"location":"devops/ansible/#scp-commands","text":"To Copy files From Localmachine to Remote machine scp -i <private-key-file-path> <files to copy> <user>@<IP>:<Remote-machine-path> Example scp -i Downloads/control.pem sample.txt ubuntu@54.165.128.104:/home/ubuntu/ To Copy Files from Remotemachine to Local machine scp -i <private-key-file-path> <user>@<IP>:<remote-files-path> <Localmachine-path> Example scp -i Downloads/control.pem ubuntu@54.165.128.104:/home/ubuntu/sample.txt Desktop/","title":"SCP Commands"},{"location":"devops/ansible/#sample-inventory-file","text":"Inventory ##Host Level webserver01 ansible_host =< Private IP > webserver02 ansible_host =< Private IP > webserver03 ansible_host =< Private IP > dbserver01 ansible_host =< Private IP > dbserver02 ansible_host =< Private IP > ansible_user = ubuntu ##Group Level [ Group1 ] webserverserver01 webserverserver02 webserverserver03 [ Group2 ] dbserver01 dbserver02 ##Parent Level [ dc_mumbai : children ] webservergrp dbsrvgrp ##Variables [ dc_mumbai : vars ] ansible_user =< user > ansible_ssh_private_key_file =< key - path > Info Host level has the highest priority, If you mention anything like username or Keyfile etc. It will take only, which are mentioned at the host level.","title":"Sample Inventory File"},{"location":"devops/ansible/#ansible-commands","text":"To test the connection of a particular Remote Machine ansible -i <Inventoryfile path> -m ping <hostname> To test the connection of a particular Group of Remote Machines ansible -i <Inventoryfile path> -m ping <Groupname> To test the connection of All Remote Machine ansible -i <Inventoryfile path> -m ping all To see details about the machine ansible -i <Inventoryfile path> -m setup <hostname>","title":"Ansible Commands"},{"location":"devops/ansible/#some-example-ad-hoc-commands","text":"Ad hoc Commands Copy files to Remote machines name start with web ansible -i <Inventoryfile path> -m copy -a \"src=index.html dest=/var/www/html/index.html\" 'web*' --become Installing httpd in centos Remote machine ansible -i <Inventoryfile path> -m yum -a \"name=httpd state=present\" websrvgrp --become Start & Enable httpd in centos Remote machine ansible -i <Inventoryfile path> -m service -a \"name=httpd state=started enabled=yes\" websrvgrp --become Info Ansible Playbooks should be with .yml or .yaml Extension for example vim sample.yml","title":"Some Example Ad hoc Commands"},{"location":"devops/ansible/#playbook-for-creating-files-directories","text":"Sample Ansible Playbooks - name : Creating Files & Directories hosts : <host> become : yes tasks : - name : Creating a Directory file : path : /tmp/welcome state : directory - name : Creating a File file : path : /tmp/sample.txt state : touch To Execute the playbook ansible-playbook -i <Inventory file path> sample.yml","title":"Playbook For Creating Files &amp; Directories"},{"location":"devops/ansible/#writing-playbook-for-installing-httpd-service-in-remote-machines-with-start-and-enable-and-copying-indexhtml-files-from-local-machine-to-the-remote-machine","text":"- name : Install httpd and start the service hosts : all tasks : - name : Installing the Apache package yum : name : httpd state : present - name : Starting service service : name : httpd state : started enabled : yes - name : Copy file with owner and permissions copy : src : ./index.html dest : /var/www/html/index.html - name : Restarting service service : name : httpd state : restarted","title":"Writing Playbook For Installing Httpd service in remote machines with start and enable and copying index.html files from local machine to the remote machine"},{"location":"devops/ansible/#writing-playbook-for-setting-up-website-in-remote-machine","text":"- name : Setting up Website hosts : websrv gather_facts : False become : True tasks : - name : Installing Packages in CentOS yum : name : \"{{item}}\" state : present when : ansible_distribution == \"CentOS\" loop : - httpd - wget - unzip - name : Start & Enable httpd service : name : httpd state : started enabled : yes - name : Downloading Source code get_url : url : https://www.tooplate.com/zip-templates/2114_pixie.zip dest : /opt - name : Unarchive a file that is already on the remote machine unarchive : src : /opt/2114_pixie.zip dest : /opt remote_src : yes - name : Deploy Website copy : src : /opt/2114_pixie/ dest : /var/www/html/ remote_src : yes - name : Restarting httpd service service : name : httpd state : restarted","title":"Writing Playbook for Setting Up Website in Remote Machine"},{"location":"devops/ansible/#writing-playbook-for-setting-up-website-in-remote-machine-with-conditions-handlers","text":"- name : Writing playbook for loops and conditions hosts : all tasks : - name : Install packages on centos yum : name : \"{{item}}\" state : present when : ansible_distribution == \"CentOS\" loop : - httpd - wget - unzip - zip - git - name : Install packages on Ubuntu apt : name : \"{{item}}\" state : present update_cache : yes when : ansible_distribution == \"Ubuntu\" loop : - apache2 - wget - unzip - zip - git - name : Start & enable service on CentOS service : name : httpd state : started enabled : yes when : ansible_distribution == \"CentOS\" - name : Start & enable service on Ubuntu service : name : apache2 state : started enabled : yes when : ansible_distribution == \"Ubuntu\" - name : Push index.html on centos copy : src : index.html dest : /var/www/html/ backup : yes when : ansible_distribution == \"CentOS\" notify : - Restart service on CentOS - name : Push index.html on ubuntu copy : src : index.html dest : /var/www/html/ backup : yes when : ansible_distribution == \"Ubuntu\" notify : - Restart service on Ubuntu handlers : - name : Restart service on CentOS service : name : httpd state : restarted enabled : yes when : ansible_distribution == \"CentOS\" - name : Restart service on Ubuntu service : name : apache2 state : restarted enabled : yes when : ansible_distribution == \"Ubuntu\"","title":"Writing Playbook for Setting Up Website in Remote Machine with Conditions &amp; Handlers."},{"location":"devops/ansible/#writing-playbook-to-create-vpc-in-aws-cloud-and-including-variables-from-different-file","text":"Requirements python >= 3.6 boto3 >= 1.15.0 botocore >= 1.18.0 apt install python3-pip pip install boto pip install boto3 pip install botocore Creating File to store Variables vim vpc_setup.txt vpc_name : \"Vprofile-vpc\" #Vpc-range vpcrange : ' 172.21.0.0 / 16 ' #subnet range pubip1 : ' 172.21.1.0 / 24 ' pubip2 : ' 172.21.2.0 / 24 ' pubip3 : ' 172.21.3.0 / 24 ' pvtip1 : ' 172.21.4.0 / 24 ' pvtip2 : ' 172.21.5.0 / 24 ' pvtip3 : ' 172.21.6.0 / 24 ' #region region : ' us - east -2 ' #zone names zone1 : us - east -2 a zone2 : us - east -2 b zone3 : us - east -2 c state : present Playbook - hosts : localhost connection : local gather_facts : False tasks : - name : Import vpc variables include_vars : /path/vpc_setup - name : create vpc ec2_vpc_net : name : \"{{vpc_name}}\" cidr_block : \"{{vpcrange}}\" region : \"{{region}}\" dns_support : yes dns_hostnames : yes tenancy : default state : \"{{state}}\" register : vpcout - name : Create a public subnet for zone1 ec2_vpc_subnet : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" az : \"{{zone1}}\" state : \"{{state}}\" cidr : \"{{pubip1}}\" map_public : yes tags : Name : vprofile_pubsub1 register : pubsub1_out - name : Create a public subnet for zone2 ec2_vpc_subnet : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" az : \"{{zone2}}\" state : \"{{state}}\" cidr : \"{{pubip2}}\" map_public : yes tags : Name : vprofile_pubsub2 register : pubsub2_out - name : Create a public subnet for zone3 ec2_vpc_subnet : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" az : \"{{zone3}}\" state : \"{{state}}\" cidr : \"{{pubip3}}\" map_public : yes tags : Name : vprofile_pubsub3 register : pubsub3_out - name : Create a private subnet for zone1 ec2_vpc_subnet : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" az : \"{{zone1}}\" state : \"{{state}}\" cidr : \"{{pvtip1}}\" map_public : yes tags : Name : vprofile_pvtsub1 register : pvtsub1_out - name : Create a private subnet for zone2 ec2_vpc_subnet : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" az : \"{{zone2}}\" state : \"{{state}}\" cidr : \"{{pvtip2}}\" map_public : yes tags : Name : vprofile_pvtsub2 register : pvtsub2_out - name : Create a private subnet for zone3 ec2_vpc_subnet : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" az : \"{{zone3}}\" state : \"{{state}}\" cidr : \"{{pvtip3}}\" map_public : yes tags : Name : vprofile_pvtsub3 register : pvtsub3_out - name : Internet gateway setup ec2_vpc_igw : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" state : \"{{state}}\" tags : Name : vprofile_IGW register : igw_out - name : public subnet route table ec2_vpc_route_table : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" tags : Name : vprofile_Public subnets : - \"{{pubsub1_out.subnet.id}}\" - \"{{pubsub2_out.subnet.id}}\" - \"{{pubsub3_out.subnet.id}}\" routes : - dest : 0.0.0.0/0 gateway_id : \"{{ igw_out.gateway_id }}\" register : public_route_table - name : Create a new nat gateway and allocate a new EIP if a nat gateway does not yet exist in the subnet. ec2_vpc_nat_gateway : state : \"{{state}}\" subnet_id : \"{{pubsub1_out.subnet.id}}\" wait : true region : \"{{region}}\" if_exist_do_not_create : true register : nat_out - name : private subnet route table ec2_vpc_route_table : vpc_id : \"{{vpcout.vpc.id}}\" region : \"{{region}}\" tags : Name : vprofile_Private subnets : - \"{{pvtsub1_out.subnet.id}}\" - \"{{pvtsub2_out.subnet.id}}\" - \"{{pvtsub3_out.subnet.id}}\" routes : - dest : 0.0.0.0/0 gateway_id : \"{{nat_out.nat_gateway_id}}\" register : private_route_table - debug : var : \"{{item}}\" loop : - vpcout.vpc.id - pubsub1_out.subnet.id - pubsub2_out.subnet.id - pubsub3_out.subnet.id - pvtsub1_out.subnet.id - pvtsub2_out.subnet.id - pvtsub3_out.subnet.id - igw_out.gateway_id - public_route_table.route_table.id - nat_out.nat_gateway_id - private_route_table.route_table.id - set_fact : vpcid : \"{{vpcout.vpc.id}}\" pubsublid : \"{{ pubsub1_out.subnet.id }}\" pubsub2id : \"{{ pubsub2_out.subnet.id }}\" pubsub3id : \"{{ pubsub3_out.subnet.id }}\" privsublid : \"{{ pvtsub1_out.subnet.id }}\" privsub2id : \"{{ pvtsub2_out.subnet.id }}\" privsub3id : \"{{ pvtsub3_out.subnet.id }}\" igwid : \"{{ igw_out.gateway_id }}\" pubRTid : \"{{ public_route_table.route_table.id }}\" NATGWid : \"{{ nat_out.nat_gateway_id }}\" privRTid : \"{{ private_route_table.route_table.id }}\" cacheable : yes - name : creating file for vpc output copy : content : \"vpcid: {{vpcout.vpc.id}}\\n pubsublid: {{ pubsub1_out.subnet.id }}\\npubsub2id: {{ pubsub2_out.subnet.id }}\\npubsub3id: {{ pubsub3_out.subnet.id }}\\nprivsublid: {{ pvtsub1_out.subnet.id }}\\nprivsub2id: {{ pvtsub2_out.subnet.id }}\\nprivsub3id: {{ pvtsub3_out.subnet.id }}\\nigwid: {{ igw_out.gateway_id }}\\npubRTid: {{ public_route_table.route_table.id }}\\nNATGWid: {{ nat_out.nat_gateway_id }}\\nprivRTid: {{ private_route_table.route_table.id }}\" dest : /home/ubuntu/Vprofile/vars/output_vars","title":"Writing Playbook to Create VPC in AWS Cloud and Including Variables from different File"},{"location":"devops/ansible/#launching-ec2-instance-in-aws-cloud","text":"Requirements python >= 3.6 boto3 >= 1.15.0 botocore >= 1.18.0 apt install python3-pip pip install boto pip install boto3 pip install botocore Launching Instance Playbook - name : Launching Ec2 Instance hosts : localhost connection : local tasks : - name : Creating Key pair amazon.aws.ec2_key : name : samplekey region : us-west-1 register : key - debug : var : key - name : Storing private key into a file copy : content : \"{{key.key.private_key}}\" dest : \"./sample.pem\" mode : 0600 when : key.changed - name : Creating Security Group amazon.aws.ec2_group : name : mysg description : Allowing 22 and 80 vpc_id : vpc-0c8e70cf05b1342ac region : us-west-1 rules : - proto : tcp from_port : 22 to_port : 22 cidr_ip : 0.0.0.0/0 rule_desc : allow all on port 80 & 22 register : sg_out - name : Launching bastion_host ec2 : key_name : \"samplekey\" region : us-west-1 instance_type : t2.micro image : ami-0573b70afecda915d wait : yes wait_timeout : 300 instance_tags : name : \"Ansible Instance\" project : vprofile owner : devops team exact_count : 1 count_tag : name : \"Ansible Instance\" project : vprofile owner : devops team group_id : \"{{sg_out.group_id}}\" vpc_subnet_id : subnet-0c4734c845e549cda register : instance","title":"Launching Ec2 Instance in AWS Cloud"},{"location":"devops/ansible/#ansible-configuration-file","text":"Note You should save the configuration file with name ansible.cfg vim ansible.cfg Ansible Configuration File [ defaults ] host_key_checking = False inventory = <Inventory File Path> timeout = 20 log_path = /var/log/ansible_world.log remote_port = 22 remote_user = <username> [ privilege_escalation ] become = True become_method = sudo become_user = root become_ask_pass = False","title":"Ansible Configuration File"},{"location":"devops/aws-cli/","text":"AWS \u00b6 Amazon Web Services (AWS) is the world\u2019s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. AWS CLI (Command Line Interface) \u00b6 Setting up IAM user \u00b6 Creating User with Access-key \u00b6 Set permissions & Attaching Policies \u00b6 Installing AWS-CLI \u00b6 sudo -i sudo apt update apt install awscli -y Configure AWS CLI with IAM user Credentials with a specific Region \u00b6 aws configure Once it is done try some aws cli commands like aws s3 ls If u have any buckets in your s3 it will list EC2 \u2013 Elastic Compute Cloud \u00b6 Create a key pair \u00b6 aws ec2 create-key-pair --key-name <keypair-Name> --query 'KeyMaterial' --output text > <keypair-Name.pem> Delete a key pair \u00b6 To delete a key pair, run the aws ec2 delete-key-pair command, substituting MyKeyPair with the name of the pair to delete. aws ec2 delete-key-pair --key-name <keypair-Name> Create a Security Group & Adding Inbound rules \u00b6 aws ec2 create-security-group --group-name <security grp Name> --description \"<Description>\" curl https://checkip.amazonaws.com aws ec2 authorize-security-group-ingress --group-id <security group Id> --protocol tcp --port <port Number> --cidr <ip address> aws ec2 authorize-security-group-ingress --group-id <security grp Id>--protocol tcp --port 22 -8000 --cidr 0 .0.0.0/0 To view the initial information for my-sg, run the aws ec2 describe-security-groups command. For an EC2-Classic security group, you can reference it by its name. aws ec2 describe-security-groups --group-names <security grp Name> Delete your security group \u00b6 The following command example deletes the EC2-Classic security group named. aws ec2 delete-security-group --group-name <security grp Name> Launch Instance \u00b6 You can use the following command to launch a t2.micro instance in EC2-Classic. Replace the italicized parameter values with your own. You can get the AMI IDs from documentation or console for your required Instance. aws ec2 run-instances --image-id <ami-Id> --count 1 --instance-type <type> --key-name <keypair-Name> --security-groups <security grp Name> Add a tag to your Instance \u00b6 aws ec2 create-tags --resources <Instance-Id>--tags Key = Name,Value = <value> Terminate your Instance \u00b6 To delete an instance, you use the command aws ec2 terminate-instances to delete it. aws ec2 terminate-instances --instance-ids <Instance-Id> Create Launch Template \u00b6 aws ec2 create-launch-template --launch-template-name <Name> \":[{\" AssociatePublicIpAddress \":true,\" DeviceIndex \":0,\" Ipv6AddressCount \":1,\" SubnetId \":\" pe \":\" <Instance type \",\" TagSpecifications \":[{\" ResourceType \":\" instance \",\" Tags \":[{\" Key \":\" Name \",\" Value \":\" <value> \"}]}]}' Delete Launch Template \u00b6 aws ec2 delete-launch-template --launch-template-id < template id> --region <region> Creating Auto-Scaling group \u00b6 aws autoscaling create-auto-scaling-group --auto-scaling-group-name <Name> --launch-LaunchTemplateId = <template \u2013 id > --min-size 2 --max-size 5 --vpc-zone-identifier \"subnet1-id,subnet2-id,subnet3-id\" Delete your Auto-Scaling Group \u00b6 aws autoscaling delete-auto-scaling-group --auto-scaling-group-name < Auto -Scaling group Name > EBS \u2013 Elastic Block Storage \u00b6 Create EBS Volume \u00b6 To create an empty General Purpose SSD (gp2) volume aws ec2 create-volume --volume-type <volume type> --size <size in number> --availability-zone <zone> To create an encrypted volume \u00b6 aws ec2 create-volume --volume-type <volume type> --size <size in number> --encrypted --availability-zone <zone> To create a volume with tags \u00b6 aws ec2 create-tags --resources <volume-id> --tags Key = Name,Value = <value> To Delete a Volume \u00b6 aws ec2 delete-volume --volume-id <volume Id> Output Output: None To create a snapshot \u00b6 This example command creates a snapshot of the volume with a volume ID of and a short description to identify the snapshot. aws ec2 create-snapshot --volume-id <volume Id> --description \"<Description>\" To create a snapshot with tags \u00b6 aws ec2 create-snapshot --volume-id <volume Id> --description 'Prod backup' --tag-specifications 'ResourceType=snapshot,Tags=[{Key=Name,Value=<value>},{Key=Database,Value=Mysql}]' To allocate an Elastic IP address for EC2-Classic \u00b6 The following allocate-address example allocates an Elastic IP address to use with an instance in EC2-Classic. aws ec2 allocate-address ELB \u2013 Elastic Load Balancer \u00b6 Create-load-balancer \u00b6 To create an Application load balancer \u00b6 The below commands to find subnet id & Instance Id aws ec2 describe-subnets aws ec2 describe-instances aws elbv2 create-load-balancer --name <Load balancer Name>--type <type> --subnets <subnet-Id> <subnet-Id> To create a Network load balancer \u00b6 aws elbv2 create-load-balancer --name <Load balancer Name>--type <type> --subnets <subnet-Id> To register instances with a load balancer \u00b6 aws elb register-instances-with-load-balancer --load-balancer-name <Load balancer Name> --instances <Instance-Id> To Delete a Specific Load balancer \u00b6 aws elbv2 delete-load-balancer --load-balancer-arn <arn end point> RDS - Relational Database Service \u00b6 Create-db-Instance \u00b6 aws rds create-db-instance --db-instance-identifier <db - Name> --db-instance-class <db.type> --engine <Database Engine> --master-username <username> --master-user-password <password> --allocated-storage <storage in numbers> To delete your db-Instance \u00b6 aws rds delete-db-instance --db-instance-identifier <db - Name> --final-db-snapshot-identifier <db - Name>-final-snap S3 \u2013 Simple Storage Service \u00b6 List Buckets & Objects \u00b6 To list your buckets, folders, or objects, use the s3 ls command. Using the command without a target or options lists all buckets. aws s3 ls aws s3 ls s3://<bucket name> Create a bucket \u00b6 Use the s3 mb command to make a bucket. Bucket names must be globally unique (unique across all of Amazon S3) and should be DNS compliant. aws s3 mb s3:// <bucket name> Copy objects \u00b6 Use the s3 cp command to copy objects from a bucket or a local directory aws s3 cp <file> s3:// <bucket name> aws s3 cp s3://< source bucket/file> s3://<destination-bucket> Move objects \u00b6 Use the s3 mv command to move objects from a bucket or a local directory. aws s3 mv < local file> s3:// <bucket name> aws s3 mv s3:// < source bucket/file> s3://<destination-bucket> Sync Objects \u00b6 aws s3 sync . s3://<bucket name> Delete Objects \u00b6 aws s3 rm s3://<bucket name/file> --recursive Empty Bucket \u00b6 aws s3 rm s3://<bucket name> --recursive Delete Bucket \u00b6 aws s3 rb s3://<bucket name> VPC \u2013 Virtual Private Cloud \u00b6 To create a VPC and subnets using the AWS CLI \u00b6 Create a VPC with a 10.0.0.0/16 CIDR block using the following create-vpc command. \u00b6 aws ec2 create-vpc --cidr-block <Ip address> --query Vpc.VpcId --output text Using the VPC ID from the previous step, create a subnet with a 10.0.1.0/24 CIDR block using the following create-subnet command. \u00b6 aws ec2 create-subnet --vpc-id <vpc - Id>--cidr-block <Ip address> Create a second subnet in your VPC with a 10.0.2.0/24 CIDR block. \u00b6 aws ec2 create-subnet --vpc-id <vpc - Id>--cidr-block <Ip address> Create an internet gateway using the following create-internet-gateway command. \u00b6 aws ec2 create-internet-gateway --query InternetGateway.InternetGatewayId --output text Using the ID from the previous step, attach the internet gateway to your VPC using the following attach-internet-gateway command. \u00b6 aws ec2 attach-internet-gateway --vpc-id <vpc - Id>--internet-gateway-id <IGW - Id> Create a custom route table for your VPC using the following create-route-table command. \u00b6 aws ec2 create-route-table --vpc-id <vpc - Id>--query RouteTable.RouteTableId --output text Create a route in the route table that points all traffic (0.0.0.0/0) to the internet gateway using the following create-route command. \u00b6 aws ec2 create-route --route-table-id <route table - Id>--destination-cidr-block 0 .0.0.0/0 --gateway-id <Igw - Id> You can describe the route table using the following describe-route-tables command. \u00b6 aws ec2 describe-route-tables --route-table-id <route table - Id> The route table is currently not associated with any subnet. You need to associate it with a subnet in your VPC so that traffic from that subnet is routed to the internet gateway. \u00b6 aws ec2 describe-subnets --filters \"Name=vpc-id,Values=<vpc \u2013Id> --query \" Subnets [ * ] . { ID:SubnetId,CIDR:CidrBlock } \" You can choose which subnet to associate with the custom route table, for example, subnet-0c312202b3f26703a, and associate it using the associate-route-table command. This subnet is your public subnet. \u00b6 aws ec2 associate-route-table --subnet-id <subnet-Id> --route-table-id <route table - Id> CLEAN UP \u00b6 Delete your custom route table: \u00b6 aws ec2 delete-route-table --route-table-id <route table - Id> Delete your subnets: \u00b6 aws ec2 delete-subnet --subnet-id <subnet-Id> Detach your internet gateway from your VPC: \u00b6 aws ec2 detach-internet-gateway --internet-gateway-id <Igw -Id> --vpc-id <vpc- Id> Delete your internet gateway: \u00b6 aws ec2 delete-internet-gateway --internet-gateway-id <Igw - Id> Delete your VPC: \u00b6 aws ec2 delete-vpc --vpc-id <vpc- Id> Cloud Watch \u00b6 Creating Alarm \u00b6 aws cloudwatch put-metric-alarm --alarm-name <Alarm name> --alarm-description \"<Description>\" --metric-name <Metric> --namespace AWS/EC2 --statistic Average --period 300 --threshold < 70 > --comparison-operator <GreaterThanThreshold> --dimensions \"Name=InstanceId,Value=<Id>\" --evaluation-periods 2 --alarm-actions <SNS \u2013 arn > --unit Percent Delete Your Alarm \u00b6 aws cloudwatch delete-alarms --alarm-names <Alarm name> Disable your Alarm \u00b6 aws cloudwatch disable-alarm-actions --alarm-names <Alarm name> Enable your Alarm \u00b6 aws cloudwatch enable-alarm-actions --alarm-names <Alarm name>","title":"AWS"},{"location":"devops/aws-cli/#aws","text":"Amazon Web Services (AWS) is the world\u2019s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally.","title":"AWS"},{"location":"devops/aws-cli/#aws-cli-command-line-interface","text":"","title":"AWS CLI (Command Line Interface)"},{"location":"devops/aws-cli/#setting-up-iam-user","text":"","title":"Setting up IAM user"},{"location":"devops/aws-cli/#creating-user-with-access-key","text":"","title":"Creating User with Access-key"},{"location":"devops/aws-cli/#set-permissions-attaching-policies","text":"","title":"Set permissions &amp; Attaching Policies"},{"location":"devops/aws-cli/#installing-aws-cli","text":"sudo -i sudo apt update apt install awscli -y","title":"Installing AWS-CLI"},{"location":"devops/aws-cli/#configure-aws-cli-with-iam-user-credentials-with-a-specific-region","text":"aws configure Once it is done try some aws cli commands like aws s3 ls If u have any buckets in your s3 it will list","title":"Configure AWS CLI with IAM user Credentials with a specific Region"},{"location":"devops/aws-cli/#ec2-elastic-compute-cloud","text":"","title":"EC2 \u2013 Elastic Compute Cloud"},{"location":"devops/aws-cli/#create-a-key-pair","text":"aws ec2 create-key-pair --key-name <keypair-Name> --query 'KeyMaterial' --output text > <keypair-Name.pem>","title":"Create a key pair"},{"location":"devops/aws-cli/#delete-a-key-pair","text":"To delete a key pair, run the aws ec2 delete-key-pair command, substituting MyKeyPair with the name of the pair to delete. aws ec2 delete-key-pair --key-name <keypair-Name>","title":"Delete a key pair"},{"location":"devops/aws-cli/#create-a-security-group-adding-inbound-rules","text":"aws ec2 create-security-group --group-name <security grp Name> --description \"<Description>\" curl https://checkip.amazonaws.com aws ec2 authorize-security-group-ingress --group-id <security group Id> --protocol tcp --port <port Number> --cidr <ip address> aws ec2 authorize-security-group-ingress --group-id <security grp Id>--protocol tcp --port 22 -8000 --cidr 0 .0.0.0/0 To view the initial information for my-sg, run the aws ec2 describe-security-groups command. For an EC2-Classic security group, you can reference it by its name. aws ec2 describe-security-groups --group-names <security grp Name>","title":"Create a Security Group &amp; Adding Inbound rules"},{"location":"devops/aws-cli/#delete-your-security-group","text":"The following command example deletes the EC2-Classic security group named. aws ec2 delete-security-group --group-name <security grp Name>","title":"Delete your security group"},{"location":"devops/aws-cli/#launch-instance","text":"You can use the following command to launch a t2.micro instance in EC2-Classic. Replace the italicized parameter values with your own. You can get the AMI IDs from documentation or console for your required Instance. aws ec2 run-instances --image-id <ami-Id> --count 1 --instance-type <type> --key-name <keypair-Name> --security-groups <security grp Name>","title":"Launch Instance"},{"location":"devops/aws-cli/#add-a-tag-to-your-instance","text":"aws ec2 create-tags --resources <Instance-Id>--tags Key = Name,Value = <value>","title":"Add a tag to your Instance"},{"location":"devops/aws-cli/#terminate-your-instance","text":"To delete an instance, you use the command aws ec2 terminate-instances to delete it. aws ec2 terminate-instances --instance-ids <Instance-Id>","title":"Terminate your Instance"},{"location":"devops/aws-cli/#create-launch-template","text":"aws ec2 create-launch-template --launch-template-name <Name> \":[{\" AssociatePublicIpAddress \":true,\" DeviceIndex \":0,\" Ipv6AddressCount \":1,\" SubnetId \":\" pe \":\" <Instance type \",\" TagSpecifications \":[{\" ResourceType \":\" instance \",\" Tags \":[{\" Key \":\" Name \",\" Value \":\" <value> \"}]}]}'","title":"Create Launch Template"},{"location":"devops/aws-cli/#delete-launch-template","text":"aws ec2 delete-launch-template --launch-template-id < template id> --region <region>","title":"Delete Launch Template"},{"location":"devops/aws-cli/#creating-auto-scaling-group","text":"aws autoscaling create-auto-scaling-group --auto-scaling-group-name <Name> --launch-LaunchTemplateId = <template \u2013 id > --min-size 2 --max-size 5 --vpc-zone-identifier \"subnet1-id,subnet2-id,subnet3-id\"","title":"Creating Auto-Scaling group"},{"location":"devops/aws-cli/#delete-your-auto-scaling-group","text":"aws autoscaling delete-auto-scaling-group --auto-scaling-group-name < Auto -Scaling group Name >","title":"Delete your Auto-Scaling Group"},{"location":"devops/aws-cli/#ebs-elastic-block-storage","text":"","title":"EBS \u2013 Elastic Block Storage"},{"location":"devops/aws-cli/#create-ebs-volume","text":"To create an empty General Purpose SSD (gp2) volume aws ec2 create-volume --volume-type <volume type> --size <size in number> --availability-zone <zone>","title":"Create EBS Volume"},{"location":"devops/aws-cli/#to-create-an-encrypted-volume","text":"aws ec2 create-volume --volume-type <volume type> --size <size in number> --encrypted --availability-zone <zone>","title":"To create an encrypted volume"},{"location":"devops/aws-cli/#to-create-a-volume-with-tags","text":"aws ec2 create-tags --resources <volume-id> --tags Key = Name,Value = <value>","title":"To create a volume with tags"},{"location":"devops/aws-cli/#to-delete-a-volume","text":"aws ec2 delete-volume --volume-id <volume Id> Output Output: None","title":"To Delete a Volume"},{"location":"devops/aws-cli/#to-create-a-snapshot","text":"This example command creates a snapshot of the volume with a volume ID of and a short description to identify the snapshot. aws ec2 create-snapshot --volume-id <volume Id> --description \"<Description>\"","title":"To create a snapshot"},{"location":"devops/aws-cli/#to-create-a-snapshot-with-tags","text":"aws ec2 create-snapshot --volume-id <volume Id> --description 'Prod backup' --tag-specifications 'ResourceType=snapshot,Tags=[{Key=Name,Value=<value>},{Key=Database,Value=Mysql}]'","title":"To create a snapshot with tags"},{"location":"devops/aws-cli/#to-allocate-an-elastic-ip-address-for-ec2-classic","text":"The following allocate-address example allocates an Elastic IP address to use with an instance in EC2-Classic. aws ec2 allocate-address","title":"To allocate an Elastic IP address for EC2-Classic"},{"location":"devops/aws-cli/#elb-elastic-load-balancer","text":"","title":"ELB \u2013 Elastic Load Balancer"},{"location":"devops/aws-cli/#create-load-balancer","text":"","title":"Create-load-balancer"},{"location":"devops/aws-cli/#to-create-an-application-load-balancer","text":"The below commands to find subnet id & Instance Id aws ec2 describe-subnets aws ec2 describe-instances aws elbv2 create-load-balancer --name <Load balancer Name>--type <type> --subnets <subnet-Id> <subnet-Id>","title":"To create an Application load balancer"},{"location":"devops/aws-cli/#to-create-a-network-load-balancer","text":"aws elbv2 create-load-balancer --name <Load balancer Name>--type <type> --subnets <subnet-Id>","title":"To create a Network load balancer"},{"location":"devops/aws-cli/#to-register-instances-with-a-load-balancer","text":"aws elb register-instances-with-load-balancer --load-balancer-name <Load balancer Name> --instances <Instance-Id>","title":"To register instances with a load balancer"},{"location":"devops/aws-cli/#to-delete-a-specific-load-balancer","text":"aws elbv2 delete-load-balancer --load-balancer-arn <arn end point>","title":"To Delete a Specific Load balancer"},{"location":"devops/aws-cli/#rds-relational-database-service","text":"","title":"RDS - Relational Database Service"},{"location":"devops/aws-cli/#create-db-instance","text":"aws rds create-db-instance --db-instance-identifier <db - Name> --db-instance-class <db.type> --engine <Database Engine> --master-username <username> --master-user-password <password> --allocated-storage <storage in numbers>","title":"Create-db-Instance"},{"location":"devops/aws-cli/#to-delete-your-db-instance","text":"aws rds delete-db-instance --db-instance-identifier <db - Name> --final-db-snapshot-identifier <db - Name>-final-snap","title":"To delete your db-Instance"},{"location":"devops/aws-cli/#s3-simple-storage-service","text":"","title":"S3 \u2013 Simple Storage Service"},{"location":"devops/aws-cli/#list-buckets-objects","text":"To list your buckets, folders, or objects, use the s3 ls command. Using the command without a target or options lists all buckets. aws s3 ls aws s3 ls s3://<bucket name>","title":"List Buckets &amp; Objects"},{"location":"devops/aws-cli/#create-a-bucket","text":"Use the s3 mb command to make a bucket. Bucket names must be globally unique (unique across all of Amazon S3) and should be DNS compliant. aws s3 mb s3:// <bucket name>","title":"Create a bucket"},{"location":"devops/aws-cli/#copy-objects","text":"Use the s3 cp command to copy objects from a bucket or a local directory aws s3 cp <file> s3:// <bucket name> aws s3 cp s3://< source bucket/file> s3://<destination-bucket>","title":"Copy objects"},{"location":"devops/aws-cli/#move-objects","text":"Use the s3 mv command to move objects from a bucket or a local directory. aws s3 mv < local file> s3:// <bucket name> aws s3 mv s3:// < source bucket/file> s3://<destination-bucket>","title":"Move objects"},{"location":"devops/aws-cli/#sync-objects","text":"aws s3 sync . s3://<bucket name>","title":"Sync Objects"},{"location":"devops/aws-cli/#delete-objects","text":"aws s3 rm s3://<bucket name/file> --recursive","title":"Delete Objects"},{"location":"devops/aws-cli/#empty-bucket","text":"aws s3 rm s3://<bucket name> --recursive","title":"Empty Bucket"},{"location":"devops/aws-cli/#delete-bucket","text":"aws s3 rb s3://<bucket name>","title":"Delete Bucket"},{"location":"devops/aws-cli/#vpc-virtual-private-cloud","text":"","title":"VPC \u2013 Virtual Private Cloud"},{"location":"devops/aws-cli/#to-create-a-vpc-and-subnets-using-the-aws-cli","text":"","title":"To create a VPC and subnets using the AWS CLI"},{"location":"devops/aws-cli/#create-a-vpc-with-a-1000016-cidr-block-using-the-following-create-vpc-command","text":"aws ec2 create-vpc --cidr-block <Ip address> --query Vpc.VpcId --output text","title":"Create a VPC with a 10.0.0.0/16 CIDR block using the following create-vpc command."},{"location":"devops/aws-cli/#using-the-vpc-id-from-the-previous-step-create-a-subnet-with-a-1001024-cidr-block-using-the-following-create-subnet-command","text":"aws ec2 create-subnet --vpc-id <vpc - Id>--cidr-block <Ip address>","title":"Using the VPC ID from the previous step, create a subnet with a 10.0.1.0/24 CIDR block using the following create-subnet command."},{"location":"devops/aws-cli/#create-a-second-subnet-in-your-vpc-with-a-1002024-cidr-block","text":"aws ec2 create-subnet --vpc-id <vpc - Id>--cidr-block <Ip address>","title":"Create a second subnet in your VPC with a 10.0.2.0/24 CIDR block."},{"location":"devops/aws-cli/#create-an-internet-gateway-using-the-following-create-internet-gateway-command","text":"aws ec2 create-internet-gateway --query InternetGateway.InternetGatewayId --output text","title":"Create an internet gateway using the following create-internet-gateway command."},{"location":"devops/aws-cli/#using-the-id-from-the-previous-step-attach-the-internet-gateway-to-your-vpc-using-the-following-attach-internet-gateway-command","text":"aws ec2 attach-internet-gateway --vpc-id <vpc - Id>--internet-gateway-id <IGW - Id>","title":"Using the ID from the previous step, attach the internet gateway to your VPC using the following attach-internet-gateway command."},{"location":"devops/aws-cli/#create-a-custom-route-table-for-your-vpc-using-the-following-create-route-table-command","text":"aws ec2 create-route-table --vpc-id <vpc - Id>--query RouteTable.RouteTableId --output text","title":"Create a custom route table for your VPC using the following create-route-table command."},{"location":"devops/aws-cli/#create-a-route-in-the-route-table-that-points-all-traffic-00000-to-the-internet-gateway-using-the-following-create-route-command","text":"aws ec2 create-route --route-table-id <route table - Id>--destination-cidr-block 0 .0.0.0/0 --gateway-id <Igw - Id>","title":"Create a route in the route table that points all traffic (0.0.0.0/0) to the internet gateway using the following create-route command."},{"location":"devops/aws-cli/#you-can-describe-the-route-table-using-the-following-describe-route-tables-command","text":"aws ec2 describe-route-tables --route-table-id <route table - Id>","title":"You can describe the route table using the following describe-route-tables command."},{"location":"devops/aws-cli/#the-route-table-is-currently-not-associated-with-any-subnet-you-need-to-associate-it-with-a-subnet-in-your-vpc-so-that-traffic-from-that-subnet-is-routed-to-the-internet-gateway","text":"aws ec2 describe-subnets --filters \"Name=vpc-id,Values=<vpc \u2013Id> --query \" Subnets [ * ] . { ID:SubnetId,CIDR:CidrBlock } \"","title":"The route table is currently not associated with any subnet. You need to associate it with a subnet in your VPC so that traffic from that subnet is routed to the internet gateway."},{"location":"devops/aws-cli/#you-can-choose-which-subnet-to-associate-with-the-custom-route-table-for-example-subnet-0c312202b3f26703a-and-associate-it-using-the-associate-route-table-command-this-subnet-is-your-public-subnet","text":"aws ec2 associate-route-table --subnet-id <subnet-Id> --route-table-id <route table - Id>","title":"You can choose which subnet to associate with the custom route table, for example, subnet-0c312202b3f26703a, and associate it using the associate-route-table command. This subnet is your public subnet."},{"location":"devops/aws-cli/#clean-up","text":"","title":"CLEAN UP"},{"location":"devops/aws-cli/#delete-your-custom-route-table","text":"aws ec2 delete-route-table --route-table-id <route table - Id>","title":"Delete your custom route table:"},{"location":"devops/aws-cli/#delete-your-subnets","text":"aws ec2 delete-subnet --subnet-id <subnet-Id>","title":"Delete your subnets:"},{"location":"devops/aws-cli/#detach-your-internet-gateway-from-your-vpc","text":"aws ec2 detach-internet-gateway --internet-gateway-id <Igw -Id> --vpc-id <vpc- Id>","title":"Detach your internet gateway from your VPC:"},{"location":"devops/aws-cli/#delete-your-internet-gateway","text":"aws ec2 delete-internet-gateway --internet-gateway-id <Igw - Id>","title":"Delete your internet gateway:"},{"location":"devops/aws-cli/#delete-your-vpc","text":"aws ec2 delete-vpc --vpc-id <vpc- Id>","title":"Delete your VPC:"},{"location":"devops/aws-cli/#cloud-watch","text":"","title":"Cloud Watch"},{"location":"devops/aws-cli/#creating-alarm","text":"aws cloudwatch put-metric-alarm --alarm-name <Alarm name> --alarm-description \"<Description>\" --metric-name <Metric> --namespace AWS/EC2 --statistic Average --period 300 --threshold < 70 > --comparison-operator <GreaterThanThreshold> --dimensions \"Name=InstanceId,Value=<Id>\" --evaluation-periods 2 --alarm-actions <SNS \u2013 arn > --unit Percent","title":"Creating Alarm"},{"location":"devops/aws-cli/#delete-your-alarm","text":"aws cloudwatch delete-alarms --alarm-names <Alarm name>","title":"Delete Your Alarm"},{"location":"devops/aws-cli/#disable-your-alarm","text":"aws cloudwatch disable-alarm-actions --alarm-names <Alarm name>","title":"Disable your Alarm"},{"location":"devops/aws-cli/#enable-your-alarm","text":"aws cloudwatch enable-alarm-actions --alarm-names <Alarm name>","title":"Enable your Alarm"},{"location":"devops/awsdevops/","text":"DevOps \u00b6 DevOps is the combination of cultural philosophies, practices, and tools that increase an organization\u2019s ability to deliver applications and services at high velocity: Evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.","title":"DevOps"},{"location":"devops/awsdevops/#devops","text":"DevOps is the combination of cultural philosophies, practices, and tools that increase an organization\u2019s ability to deliver applications and services at high velocity: Evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.","title":"DevOps"},{"location":"devops/azure/","text":"Updating...","title":"Azure"},{"location":"devops/docker-setup/","text":"Docker \u00b6 Docker is a containerized tool designed to make it easier to create, deploy and run applications using containers. Containers allow a developer to package up an application with libraries and other dependencies and deploy it as one package. Containers are OS virtualization. We don't need an OS in the container to install our application. It depends on the Host OS kernel. Install Docker Engine & Docker-Compose on Ubuntu & CentOS \u00b6 Docker Setup Create a file name docker_setup.sh and copy the below script vim docker_setup.sh #!/bin/bash apt --help >>/dev/null if [ $? -eq 0 ] then echo \" INSTALLING DOCKER IN UBUNTU\" echo sudo apt update sudo apt-get remove docker docker-engine docker.io containerd runc sudo apt-get update sudo apt-get -y install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io -y sudo docker run hello-world else echo \" INSTALLING DOCKER IN CENTOS\" echo sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine sudo yum install -y yum-utils sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce docker-ce-cli containerd.io -y sudo systemctl start docker sudo docker run hello-world fi echo \" Installing Docker Compose\" sudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker-compose --version Give Execute permission for script sudo chmod +x docker_setup.sh Now run the Script ./docker_setup.sh Docker Commands \u00b6 Docker Images List all images that are locally stored with the Docker Engine docker image ls Build an image from the Dockerfile in the current directory and tag the image docker build -t <imagename>:<tag> Delete an image from the local image store docker image rm <imagename>:<tag> Containers Run a container in an interactive mode: docker run -it <imagename>:<tag> Run a container from the Image nginx:latest, name the running container \u201cweb\u201d and expose port 5000 externally, mapped to port 80 inside the container in detached mode. docker run --name web -d -p 5000 :80 nginx:latest Run a detached container in a previously created container network: docker network create <mynetwork> docker run --name web -d --net mynetwork -p 5000 :80 nginx:latest Follow the logs of a specific container: docker logs -f <container name or container container-id> List only active containers docker ps List all containers docker ps -a Stop a container docker stop <container name or container container-id> Stop a container (timeout = 1 second) docker stop -t1 Remove a stopped container docker rm <container name or container container-id> Force stop and remove a container docker rm -f <container name or container container-id> Remove all containers docker rm -f $( docker ps-aq ) Remove all stopped containers docker rm $( docker ps -q -f \u201cstatus = exited\u201d ) Execute a new process in an existing container: Execute and access bash inside a container docker exec -it <container name or container-id> bash To inspect the container docker inspect <container name or container container-id> Share To Establish Connections from Local to Remote. log in with your Dockerhub Credentials. docker login Pull an image from a registry docker pull <imagename>:<tag> Retag a local image with a new image name and tag docker tag myimage:1.0 myrepo/myimage:2.0 Push an image to a registry. docker push myrepo/myimage:2.0 Dockerfile Sample Dockerfile for Deploying a Static website. vim Dockerfile FROM centos:7 LABEL \"Author\" = \"saiteja Irrinki\" LABEL \"Project\" = \"Wave\" RUN yum install httpd wget unzip -y RUN wget https://www.tooplate.com/zip-templates/2121_wave_cafe.zip RUN unzip 2121_wave_cafe.zip RUN cp -r 2121_wave_cafe/* /var/www/html/ CMD [ \"/usr/sbin/httpd\" , \"-D\" , \"FOREGROUND\" ] EXPOSE 80 WORKDIR /var/www/html VOLUME /var/log/httpd Build the image from the Docker file, Here Change my Name with your registry name and Image. docker build -t saitejairrinki/wavecafe:v1 . Run Container From the Image docker run --name wavecafe -d -p 9699 :80 saitejairrinki/wavecafe:v1 Now Access From the Browser, Make sure you have to allow the port number in my case 9699 in your security group if you are using cloud VM. Public-IPaddress:9699 Docker Image You can pull my image and you can also run a container from my image without creating Dockerfile. docker pull saitejairrinki/wavecafe:v1 docker run --name wavecafe -d -p 9999 :80 saitejairrinki/wavecafe:v1 Now Access From the Browser, Make sure you have to allow the port number in my case 9999 in your security group if you are using cloud VM. Public-IPaddress:9999 Docker Compose Creating Docker Compose for local Docker File version : \"3\" services : Wavecafe : build : context : /Dockerfile_path/ ports : - \"5555:80\" container_name : wavecafe Creating Docker Compose for DockerHub Images version : '3' services : website : image : saitejairrinki/wavecafe:v1 ports : - \"8085:80\" Docker Volume Creating a Separate Directory to Store Container data mkdir mountbind Now link your Directory while running the container docker run --name db01 -e MYSQL_ROOT_PASSWORD = secret123 -p 3300 :3306 -v /root/mountbind:/var/lib/mysql -d mysql:5.7 Now do ls to the Directory there you can see the containers data ls mountbind Creating docker Volume, use the below command to see all the available options of docker volume docker volume --help Creating a new docker volume with name datadb docker volume create datadb Now run your container with that volume docker run --name db02 -e MYSQL_ROOT_PASSWORD = secret123 -p 3301 :3306 -v datadb:/var/lib/mysql -d mysql:5.7 Now check ls /var/lib/docker/volumes/datadb/_data/ Now for testing Create any file with any name of your choice, here I'm creating a file with the name Milkyway touch /var/lib/docker/volumes/datadb/_data/milkyway Now log in into the container and Verify your file. docker exec -it db02 /bin/bash ls /var/lib/mysql/ Now exit from the container exit If you want to access the MySQL database with a MySQL client then follow the below steps sudo apt update sudo apt install mysql-client Now fetch the container IP by doing Docker Inspect docker inspect db02 | grep -i ipaddress Now Connect with that IP mysql -h 172 .17.0.4 -u root -psecret123","title":"Docker"},{"location":"devops/docker-setup/#docker","text":"Docker is a containerized tool designed to make it easier to create, deploy and run applications using containers. Containers allow a developer to package up an application with libraries and other dependencies and deploy it as one package. Containers are OS virtualization. We don't need an OS in the container to install our application. It depends on the Host OS kernel.","title":"Docker"},{"location":"devops/docker-setup/#install-docker-engine-docker-compose-on-ubuntu-centos","text":"Docker Setup Create a file name docker_setup.sh and copy the below script vim docker_setup.sh #!/bin/bash apt --help >>/dev/null if [ $? -eq 0 ] then echo \" INSTALLING DOCKER IN UBUNTU\" echo sudo apt update sudo apt-get remove docker docker-engine docker.io containerd runc sudo apt-get update sudo apt-get -y install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io -y sudo docker run hello-world else echo \" INSTALLING DOCKER IN CENTOS\" echo sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine sudo yum install -y yum-utils sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce docker-ce-cli containerd.io -y sudo systemctl start docker sudo docker run hello-world fi echo \" Installing Docker Compose\" sudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker-compose --version Give Execute permission for script sudo chmod +x docker_setup.sh Now run the Script ./docker_setup.sh","title":"Install Docker Engine &amp; Docker-Compose on Ubuntu &amp; CentOS"},{"location":"devops/docker-setup/#docker-commands","text":"Docker Images List all images that are locally stored with the Docker Engine docker image ls Build an image from the Dockerfile in the current directory and tag the image docker build -t <imagename>:<tag> Delete an image from the local image store docker image rm <imagename>:<tag> Containers Run a container in an interactive mode: docker run -it <imagename>:<tag> Run a container from the Image nginx:latest, name the running container \u201cweb\u201d and expose port 5000 externally, mapped to port 80 inside the container in detached mode. docker run --name web -d -p 5000 :80 nginx:latest Run a detached container in a previously created container network: docker network create <mynetwork> docker run --name web -d --net mynetwork -p 5000 :80 nginx:latest Follow the logs of a specific container: docker logs -f <container name or container container-id> List only active containers docker ps List all containers docker ps -a Stop a container docker stop <container name or container container-id> Stop a container (timeout = 1 second) docker stop -t1 Remove a stopped container docker rm <container name or container container-id> Force stop and remove a container docker rm -f <container name or container container-id> Remove all containers docker rm -f $( docker ps-aq ) Remove all stopped containers docker rm $( docker ps -q -f \u201cstatus = exited\u201d ) Execute a new process in an existing container: Execute and access bash inside a container docker exec -it <container name or container-id> bash To inspect the container docker inspect <container name or container container-id> Share To Establish Connections from Local to Remote. log in with your Dockerhub Credentials. docker login Pull an image from a registry docker pull <imagename>:<tag> Retag a local image with a new image name and tag docker tag myimage:1.0 myrepo/myimage:2.0 Push an image to a registry. docker push myrepo/myimage:2.0 Dockerfile Sample Dockerfile for Deploying a Static website. vim Dockerfile FROM centos:7 LABEL \"Author\" = \"saiteja Irrinki\" LABEL \"Project\" = \"Wave\" RUN yum install httpd wget unzip -y RUN wget https://www.tooplate.com/zip-templates/2121_wave_cafe.zip RUN unzip 2121_wave_cafe.zip RUN cp -r 2121_wave_cafe/* /var/www/html/ CMD [ \"/usr/sbin/httpd\" , \"-D\" , \"FOREGROUND\" ] EXPOSE 80 WORKDIR /var/www/html VOLUME /var/log/httpd Build the image from the Docker file, Here Change my Name with your registry name and Image. docker build -t saitejairrinki/wavecafe:v1 . Run Container From the Image docker run --name wavecafe -d -p 9699 :80 saitejairrinki/wavecafe:v1 Now Access From the Browser, Make sure you have to allow the port number in my case 9699 in your security group if you are using cloud VM. Public-IPaddress:9699 Docker Image You can pull my image and you can also run a container from my image without creating Dockerfile. docker pull saitejairrinki/wavecafe:v1 docker run --name wavecafe -d -p 9999 :80 saitejairrinki/wavecafe:v1 Now Access From the Browser, Make sure you have to allow the port number in my case 9999 in your security group if you are using cloud VM. Public-IPaddress:9999 Docker Compose Creating Docker Compose for local Docker File version : \"3\" services : Wavecafe : build : context : /Dockerfile_path/ ports : - \"5555:80\" container_name : wavecafe Creating Docker Compose for DockerHub Images version : '3' services : website : image : saitejairrinki/wavecafe:v1 ports : - \"8085:80\" Docker Volume Creating a Separate Directory to Store Container data mkdir mountbind Now link your Directory while running the container docker run --name db01 -e MYSQL_ROOT_PASSWORD = secret123 -p 3300 :3306 -v /root/mountbind:/var/lib/mysql -d mysql:5.7 Now do ls to the Directory there you can see the containers data ls mountbind Creating docker Volume, use the below command to see all the available options of docker volume docker volume --help Creating a new docker volume with name datadb docker volume create datadb Now run your container with that volume docker run --name db02 -e MYSQL_ROOT_PASSWORD = secret123 -p 3301 :3306 -v datadb:/var/lib/mysql -d mysql:5.7 Now check ls /var/lib/docker/volumes/datadb/_data/ Now for testing Create any file with any name of your choice, here I'm creating a file with the name Milkyway touch /var/lib/docker/volumes/datadb/_data/milkyway Now log in into the container and Verify your file. docker exec -it db02 /bin/bash ls /var/lib/mysql/ Now exit from the container exit If you want to access the MySQL database with a MySQL client then follow the below steps sudo apt update sudo apt install mysql-client Now fetch the container IP by doing Docker Inspect docker inspect db02 | grep -i ipaddress Now Connect with that IP mysql -h 172 .17.0.4 -u root -psecret123","title":"Docker Commands"},{"location":"devops/git/","text":"Git Cheat Sheet \u00b6 Git is a version control system tool. Developers follow to create software, there is going to be a lot of code that developers write or integrate into their software. all this code from different developers in the team has to be merged at a centralized place, which can keep track of all the versions of their code, maintain the code and even revert in time if anything breaks. GIT BASICS Command Usage git init <directory> Create empty Git repo in the specified directory. Run with no arguments to initialize the current directory as a git repository. git clone <repo> Clone repo located at <repo> onto the local machine. The original repo can be located on the local filesystem or on a remote machine via HTTP or SSH. git config user.name <name> Define the author name to be used for all commits in the current repo. Devs commonly use --global flag to set config options for the current user. git add <directory> Stage all changes in <directory> for the next commit. Replace <directory> with a <file> to change a specific file. git commit -m \"<message>\" Commit the staged snapshot, but instead of launching a text editor, use as the commit message. git status List which files are staged, unstaged, and untracked. git log Display the entire commit history using the default format. git diff Show unstaged changes between your index and working directory. UNDOING CHANGES Command Usage git revert <commit> Create a new commit that undoes all of the changes made in , then apply it to the current branch. git reset <file> Remove from the staging area, but leave the working directory unchanged. This unstaged a file without overwriting any changes. git clean -n Shows which files would be removed from the working directory. Use the -f flag in place of the -n flag to execute the clean. GIT BRANCHES Command Usage git branch List all of the branches in your repo. Add a argument to create a new branch with the name . git checkout -b <branch> Create and check out a new branch named . Drop the -b flag to checkout an existing branch. git merge <branch> Merge into the current branch. REMOTE REPOSITORIES Command Usage git remote add <name> <url> Create a new connection to a remote repo. After adding a remote,you can use as a shortcut for in other commands. git fetch <remote> <branch> Fetches a specific , from the repo. Leave off to fetch all remote refs. git pull Fetch the specified remote\u2019s copy of the current branch and immediately merge it into the local copy. git push <remote> <branch> Push the branch to , along with necessary commits and objects. Creates named branch in the remote repo if it doesn\u2019t exist. GIT RESET Command Usage git reset Reset the staging area to match the most recent commit but leave the working directory unchanged. git reset --hard Reset staging area and working directory to match the most recent commit and overwrites all changes in the working directory. git reset <commit> Move the current branch tip backward to , reset the staging area to match, but leave the working directory alone. git reset --hard <commit> Same as previous, but resets both the staging area & working directory to match. Deletes uncommitted changes, and all commits after . GIT PULL Command Usage git pull --rebase <remote> Fetch the remote\u2019s copy of the current branch and rebase it into the local copy. Uses git rebase instead of the merge to integrate the branches. GIT PUSH Command Usage git push <remote> --force Forces the git push even if it results in a non-fast-forward merge. Do not use the --force flag unless you\u2019re sure you know what you\u2019re doing. git push <remote> --all Push all of your local branches to the specified remote. git push <remote> --tags Tags aren\u2019t automatically pushed when you push a branch or use the --all flag. The --tags flag sends all of your local tags to the remote repo.","title":"GIT"},{"location":"devops/git/#git-cheat-sheet","text":"Git is a version control system tool. Developers follow to create software, there is going to be a lot of code that developers write or integrate into their software. all this code from different developers in the team has to be merged at a centralized place, which can keep track of all the versions of their code, maintain the code and even revert in time if anything breaks. GIT BASICS Command Usage git init <directory> Create empty Git repo in the specified directory. Run with no arguments to initialize the current directory as a git repository. git clone <repo> Clone repo located at <repo> onto the local machine. The original repo can be located on the local filesystem or on a remote machine via HTTP or SSH. git config user.name <name> Define the author name to be used for all commits in the current repo. Devs commonly use --global flag to set config options for the current user. git add <directory> Stage all changes in <directory> for the next commit. Replace <directory> with a <file> to change a specific file. git commit -m \"<message>\" Commit the staged snapshot, but instead of launching a text editor, use as the commit message. git status List which files are staged, unstaged, and untracked. git log Display the entire commit history using the default format. git diff Show unstaged changes between your index and working directory. UNDOING CHANGES Command Usage git revert <commit> Create a new commit that undoes all of the changes made in , then apply it to the current branch. git reset <file> Remove from the staging area, but leave the working directory unchanged. This unstaged a file without overwriting any changes. git clean -n Shows which files would be removed from the working directory. Use the -f flag in place of the -n flag to execute the clean. GIT BRANCHES Command Usage git branch List all of the branches in your repo. Add a argument to create a new branch with the name . git checkout -b <branch> Create and check out a new branch named . Drop the -b flag to checkout an existing branch. git merge <branch> Merge into the current branch. REMOTE REPOSITORIES Command Usage git remote add <name> <url> Create a new connection to a remote repo. After adding a remote,you can use as a shortcut for in other commands. git fetch <remote> <branch> Fetches a specific , from the repo. Leave off to fetch all remote refs. git pull Fetch the specified remote\u2019s copy of the current branch and immediately merge it into the local copy. git push <remote> <branch> Push the branch to , along with necessary commits and objects. Creates named branch in the remote repo if it doesn\u2019t exist. GIT RESET Command Usage git reset Reset the staging area to match the most recent commit but leave the working directory unchanged. git reset --hard Reset staging area and working directory to match the most recent commit and overwrites all changes in the working directory. git reset <commit> Move the current branch tip backward to , reset the staging area to match, but leave the working directory alone. git reset --hard <commit> Same as previous, but resets both the staging area & working directory to match. Deletes uncommitted changes, and all commits after . GIT PULL Command Usage git pull --rebase <remote> Fetch the remote\u2019s copy of the current branch and rebase it into the local copy. Uses git rebase instead of the merge to integrate the branches. GIT PUSH Command Usage git push <remote> --force Forces the git push even if it results in a non-fast-forward merge. Do not use the --force flag unless you\u2019re sure you know what you\u2019re doing. git push <remote> --all Push all of your local branches to the specified remote. git push <remote> --tags Tags aren\u2019t automatically pushed when you push a branch or use the --all flag. The --tags flag sends all of your local tags to the remote repo.","title":"Git Cheat Sheet"},{"location":"devops/jenkins/","text":"Jenkins \u00b6 Jenkins is a continuous integration tool. It can fetch the code from the version control system, build the code, test it and notify the developer. Jenkins can do continuous delivery also. Jenkins has so many plugins, by using these plugins we can do any task in Jenkins. Jenkins is an open sources project. It is a java based web application server so we need to set up first java on the machine to run the jenkins server. Jenkins CI CD Automation \u00b6 Jenkins Setup \u00b6 Ubuntu CentOS #!/bin/bash sudo apt update sudo apt install openjdk-8-jdk -y curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo tee \\ /usr/share/keyrings/jenkins-keyring.asc > /dev/null echo deb [ signed-by = /usr/share/keyrings/jenkins-keyring.asc ] \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list > /dev/null sudo apt-get update sudo apt-get install jenkins -y #!/bin/bash sudo wget -O /etc/yum.repos.d/jenkins.repo \\ https://pkg.jenkins.io/redhat-stable/jenkins.repo sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key sudo yum upgrade -y # Add required dependencies for the jenkins package sudo yum install fontconfig java-11-openjdk -y sudo yum install jenkins -y CICD Setup \u00b6 You can find my Jenkins CICD pipeline jobs which are saved in an HTML format, please download and extract to see the jobs To View My Jenkins CICD Jobs Click here","title":"Jenkins"},{"location":"devops/jenkins/#jenkins","text":"Jenkins is a continuous integration tool. It can fetch the code from the version control system, build the code, test it and notify the developer. Jenkins can do continuous delivery also. Jenkins has so many plugins, by using these plugins we can do any task in Jenkins. Jenkins is an open sources project. It is a java based web application server so we need to set up first java on the machine to run the jenkins server.","title":"Jenkins"},{"location":"devops/jenkins/#jenkins-ci-cd-automation","text":"","title":"Jenkins CI CD Automation"},{"location":"devops/jenkins/#jenkins-setup","text":"Ubuntu CentOS #!/bin/bash sudo apt update sudo apt install openjdk-8-jdk -y curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo tee \\ /usr/share/keyrings/jenkins-keyring.asc > /dev/null echo deb [ signed-by = /usr/share/keyrings/jenkins-keyring.asc ] \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list > /dev/null sudo apt-get update sudo apt-get install jenkins -y #!/bin/bash sudo wget -O /etc/yum.repos.d/jenkins.repo \\ https://pkg.jenkins.io/redhat-stable/jenkins.repo sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key sudo yum upgrade -y # Add required dependencies for the jenkins package sudo yum install fontconfig java-11-openjdk -y sudo yum install jenkins -y","title":"Jenkins Setup"},{"location":"devops/jenkins/#cicd-setup","text":"You can find my Jenkins CICD pipeline jobs which are saved in an HTML format, please download and extract to see the jobs To View My Jenkins CICD Jobs Click here","title":"CICD Setup"},{"location":"devops/k3s/","text":"Creating Kubernetes Cluster with k3s \u00b6 K3s is a lightweight Kubernetes distribution that Rancher Labs, which is a fully certified Kubernetes offering by CNCF. In K3s, we see that the memory footprint or binary which contains the components to run a cluster is small. It means that K3s are small in size. Setup \u00b6 curl -sfL https://get.k3s.io | sh - export KUBECONFIG = /etc/rancher/k3s/k3s.yaml sudo chmod 644 $KUBECONFIG","title":"k3s"},{"location":"devops/k3s/#creating-kubernetes-cluster-with-k3s","text":"K3s is a lightweight Kubernetes distribution that Rancher Labs, which is a fully certified Kubernetes offering by CNCF. In K3s, we see that the memory footprint or binary which contains the components to run a cluster is small. It means that K3s are small in size.","title":"Creating Kubernetes Cluster with k3s"},{"location":"devops/k3s/#setup","text":"curl -sfL https://get.k3s.io | sh - export KUBECONFIG = /etc/rancher/k3s/k3s.yaml sudo chmod 644 $KUBECONFIG","title":"Setup"},{"location":"devops/k8sdashboard/","text":"Deploy the latest Kubernetes dashboard \u00b6 Once you\u2019ve set up your Kubernetes cluster or if you already had one running, we can get started. The first thing to know about the web UI is that it can only be accessed using the localhost address on the machine it runs on. This means we need to have an SSH tunnel to the server. For most OS, you can create an SSH tunnel using this command. Replace the and with the relevant details to your Kubernetes cluster. ssh -L localhost:8001:127.0.0.1:8001 <user>@<master_public_IP> After you\u2019ve logged in, you can deploy the dashboard itself with the following single command. kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml If your cluster is working correctly, you should see an output confirming the creation of a bunch of Kubernetes components like in the example below. Output namespace / kubernetes - dashboard created serviceaccount / kubernetes - dashboard created service / kubernetes - dashboard created secret / kubernetes - dashboard - certs created secret / kubernetes - dashboard - csrf created secret / kubernetes - dashboard - key - holder created configmap / kubernetes - dashboard - settings created role . rbac . authorization . k8s . io / kubernetes - dashboard created clusterrole . rbac . authorization . k8s . io / kubernetes - dashboard created rolebinding . rbac . authorization . k8s . io / kubernetes - dashboard created clusterrolebinding . rbac . authorization . k8s . io / kubernetes - dashboard created deployment . apps / kubernetes - dashboard created service / dashboard - metrics - scraper created deployment . apps / dashboard - metrics - scraper created Afterward, you should have two new pods running on your cluster. kubectl get pods -A Output ... kubernetes - dashboard dashboard - metrics - scraper -78u jd94gf7 - ff6h8 1 / 1 Running 0 30 m kubernetes - dashboard kubernetes - dashboard - g6hujkirf7 - df65g 1 / 1 Running 0 30 m You can then continue ahead with creating the required user accounts. Creating Admin user \u00b6 The Kubernetes dashboard supports a few ways to manage access control. In this example, we\u2019ll be creating an admin user account with full privileges to modify the cluster and use tokens. Start by making a new directory for the dashboard configuration files. mkdir ~/dashboard && cd ~/dashboard Create the following configuration and save it as a dashboard-admin.yaml file. Note that indentation matters in the YAML files which should use two spaces in a regular text editor. vim dashboard-admin.yaml apiVersion : v1 kind : ServiceAccount metadata : name : admin-user namespace : kubernetes-dashboard --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : admin-user roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : admin-user namespace : kubernetes-dashboard Once set, save the file and exit the editor. Then deploy the admin user role with the next command. kubectl apply -f dashboard-admin.yaml You should see a service account and a cluster role binding created. Output serviceaccount / admin - user created clusterrolebinding . rbac . authorization . k8s . io / admin - user created Using this method doesn\u2019t require setting up or memorising passwords, instead, accessing the dashboard will require a token. Get the admin token using the command below. kubectl get secret -n kubernetes-dashboard $( kubectl get serviceaccount admin-user -n kubernetes-dashboard -o jsonpath = \"{.secrets[0].name}\" ) -o jsonpath = \"{.data.token}\" | base64 --decode You\u2019ll then see an output of a long string of seemingly random characters like in the example below. Output eyJhbGciOiJSUzI1NiIsImtpZCI6Ilk2eEd2QjJMVkhIRWNfN2xTMlA5N2RNVlR5N0o1REFET0dp dkRmel90aWMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlc y5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1Y mVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuL XEyZGJzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZ SI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb 3VudC51aWQiOiI1ODI5OTUxMS1hN2ZlLTQzZTQtODk3MC0yMjllOTM1YmExNDkiLCJzdWIiOiJze XN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.GcUs MMx4GnSV1hxQv01zX1nxXMZdKO7tU2OCu0TbJpPhJ9NhEidttOw5ENRosx7EqiffD3zdLDptS22F gnDqRDW8OIpVZH2oQbR153EyP_l7ct9_kQVv1vFCL3fAmdrUwY5p1-YMC41OUYORy1JPo5wkpXrW OytnsfWUbZBF475Wd3Gq3WdBHMTY4w3FarlJsvk76WgalnCtec4AVsEGxM0hS0LgQ-cGug7iGbmf cY7odZDaz5lmxAflpE5S4m-AwsTvT42ENh_bq8PS7FsMd8mK9nELyQu_a-yocYUggju_m-BxLjgc 2cLh5WzVbTH_ztW7COlKWvSVg7yhjikrew The token is created each time the dashboard is deployed and is required to log into the dashboard. Note that the token will change if the dashboard is stopped and redeployed. Creating Read-Only user \u00b6 If you wish to provide access to your Kubernetes dashboard, for example, for demonstrative purposes, you can create a read-only view for the cluster. Similarly to the admin account, save the following configuration in dashboard-read-only.yaml vim dashboard-read-only.yaml apiVersion : v1 kind : ServiceAccount metadata : name : read-only-user namespace : kubernetes-dashboard --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : annotations : rbac.authorization.kubernetes.io/autoupdate : \"true\" labels : name : read-only-clusterrole namespace : default rules : - apiGroups : - \"\" resources : [ \"*\" ] verbs : - get - list - watch - apiGroups : - extensions resources : [ \"*\" ] verbs : - get - list - watch - apiGroups : - apps resources : [ \"*\" ] verbs : - get - list - watch --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : read-only-binding roleRef : kind : ClusterRole name : read-only-clusterrole apiGroup : rbac.authorization.k8s.io subjects : - kind : ServiceAccount name : read-only-user namespace : kubernetes-dashboard Once set, save the file and exit the editor. Then deploy the read-only user account with the command below. kubectl apply -f dashboard-read-only.yaml To allow users to log in via the read-only account, you\u2019ll need to provide a token that can be fetched using the next command. kubectl get secret -n kubernetes-dashboard $( kubectl get serviceaccount read-only-user -n kubernetes-dashboard -o jsonpath = \"{.secrets[0].name}\" ) -o jsonpath = \"{.data.token}\" | base64 --decode The toke will be a long series of characters and unique to the dashboard currently running. Accessing the dashboard \u00b6 We\u2019ve now deployed the dashboard and created user accounts for it. Next, we can get started managing the Kubernetes cluster itself. However, before we can log in to the dashboard, it needs to be made available by creating a proxy service on the localhost. Run the next command on your Kubernetes cluster. kubectl proxy This will start the server at 127.0.0.1:8001 as shown by the output. Output Starting to serve on 127.0.0.1:8001 Now, assuming that we have already established an SSH tunnel binding to the localhost port 8001 at both ends, open a browser to the link below. Link http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ If everything is running correctly, you should see the dashboard login window. Select the token authentication method and copy your admin token into the field below. Then click the Sign in button. You will then be greeted by the overview of your Kubernetes cluster. While signed in as an admin, you can deploy new pods and services quickly and easily by clicking the plus icon at the top right corner of the dashboard. Then either copy in any configuration file you wish, select the file directly from your machine, or create a new configuration from a form.","title":"Kubernetes Dashboard"},{"location":"devops/k8sdashboard/#deploy-the-latest-kubernetes-dashboard","text":"Once you\u2019ve set up your Kubernetes cluster or if you already had one running, we can get started. The first thing to know about the web UI is that it can only be accessed using the localhost address on the machine it runs on. This means we need to have an SSH tunnel to the server. For most OS, you can create an SSH tunnel using this command. Replace the and with the relevant details to your Kubernetes cluster. ssh -L localhost:8001:127.0.0.1:8001 <user>@<master_public_IP> After you\u2019ve logged in, you can deploy the dashboard itself with the following single command. kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml If your cluster is working correctly, you should see an output confirming the creation of a bunch of Kubernetes components like in the example below. Output namespace / kubernetes - dashboard created serviceaccount / kubernetes - dashboard created service / kubernetes - dashboard created secret / kubernetes - dashboard - certs created secret / kubernetes - dashboard - csrf created secret / kubernetes - dashboard - key - holder created configmap / kubernetes - dashboard - settings created role . rbac . authorization . k8s . io / kubernetes - dashboard created clusterrole . rbac . authorization . k8s . io / kubernetes - dashboard created rolebinding . rbac . authorization . k8s . io / kubernetes - dashboard created clusterrolebinding . rbac . authorization . k8s . io / kubernetes - dashboard created deployment . apps / kubernetes - dashboard created service / dashboard - metrics - scraper created deployment . apps / dashboard - metrics - scraper created Afterward, you should have two new pods running on your cluster. kubectl get pods -A Output ... kubernetes - dashboard dashboard - metrics - scraper -78u jd94gf7 - ff6h8 1 / 1 Running 0 30 m kubernetes - dashboard kubernetes - dashboard - g6hujkirf7 - df65g 1 / 1 Running 0 30 m You can then continue ahead with creating the required user accounts.","title":"Deploy the latest Kubernetes dashboard"},{"location":"devops/k8sdashboard/#creating-admin-user","text":"The Kubernetes dashboard supports a few ways to manage access control. In this example, we\u2019ll be creating an admin user account with full privileges to modify the cluster and use tokens. Start by making a new directory for the dashboard configuration files. mkdir ~/dashboard && cd ~/dashboard Create the following configuration and save it as a dashboard-admin.yaml file. Note that indentation matters in the YAML files which should use two spaces in a regular text editor. vim dashboard-admin.yaml apiVersion : v1 kind : ServiceAccount metadata : name : admin-user namespace : kubernetes-dashboard --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : admin-user roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : admin-user namespace : kubernetes-dashboard Once set, save the file and exit the editor. Then deploy the admin user role with the next command. kubectl apply -f dashboard-admin.yaml You should see a service account and a cluster role binding created. Output serviceaccount / admin - user created clusterrolebinding . rbac . authorization . k8s . io / admin - user created Using this method doesn\u2019t require setting up or memorising passwords, instead, accessing the dashboard will require a token. Get the admin token using the command below. kubectl get secret -n kubernetes-dashboard $( kubectl get serviceaccount admin-user -n kubernetes-dashboard -o jsonpath = \"{.secrets[0].name}\" ) -o jsonpath = \"{.data.token}\" | base64 --decode You\u2019ll then see an output of a long string of seemingly random characters like in the example below. Output eyJhbGciOiJSUzI1NiIsImtpZCI6Ilk2eEd2QjJMVkhIRWNfN2xTMlA5N2RNVlR5N0o1REFET0dp dkRmel90aWMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlc y5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1Y mVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuL XEyZGJzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZ SI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb 3VudC51aWQiOiI1ODI5OTUxMS1hN2ZlLTQzZTQtODk3MC0yMjllOTM1YmExNDkiLCJzdWIiOiJze XN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.GcUs MMx4GnSV1hxQv01zX1nxXMZdKO7tU2OCu0TbJpPhJ9NhEidttOw5ENRosx7EqiffD3zdLDptS22F gnDqRDW8OIpVZH2oQbR153EyP_l7ct9_kQVv1vFCL3fAmdrUwY5p1-YMC41OUYORy1JPo5wkpXrW OytnsfWUbZBF475Wd3Gq3WdBHMTY4w3FarlJsvk76WgalnCtec4AVsEGxM0hS0LgQ-cGug7iGbmf cY7odZDaz5lmxAflpE5S4m-AwsTvT42ENh_bq8PS7FsMd8mK9nELyQu_a-yocYUggju_m-BxLjgc 2cLh5WzVbTH_ztW7COlKWvSVg7yhjikrew The token is created each time the dashboard is deployed and is required to log into the dashboard. Note that the token will change if the dashboard is stopped and redeployed.","title":"Creating Admin user"},{"location":"devops/k8sdashboard/#creating-read-only-user","text":"If you wish to provide access to your Kubernetes dashboard, for example, for demonstrative purposes, you can create a read-only view for the cluster. Similarly to the admin account, save the following configuration in dashboard-read-only.yaml vim dashboard-read-only.yaml apiVersion : v1 kind : ServiceAccount metadata : name : read-only-user namespace : kubernetes-dashboard --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : annotations : rbac.authorization.kubernetes.io/autoupdate : \"true\" labels : name : read-only-clusterrole namespace : default rules : - apiGroups : - \"\" resources : [ \"*\" ] verbs : - get - list - watch - apiGroups : - extensions resources : [ \"*\" ] verbs : - get - list - watch - apiGroups : - apps resources : [ \"*\" ] verbs : - get - list - watch --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : read-only-binding roleRef : kind : ClusterRole name : read-only-clusterrole apiGroup : rbac.authorization.k8s.io subjects : - kind : ServiceAccount name : read-only-user namespace : kubernetes-dashboard Once set, save the file and exit the editor. Then deploy the read-only user account with the command below. kubectl apply -f dashboard-read-only.yaml To allow users to log in via the read-only account, you\u2019ll need to provide a token that can be fetched using the next command. kubectl get secret -n kubernetes-dashboard $( kubectl get serviceaccount read-only-user -n kubernetes-dashboard -o jsonpath = \"{.secrets[0].name}\" ) -o jsonpath = \"{.data.token}\" | base64 --decode The toke will be a long series of characters and unique to the dashboard currently running.","title":"Creating Read-Only user"},{"location":"devops/k8sdashboard/#accessing-the-dashboard","text":"We\u2019ve now deployed the dashboard and created user accounts for it. Next, we can get started managing the Kubernetes cluster itself. However, before we can log in to the dashboard, it needs to be made available by creating a proxy service on the localhost. Run the next command on your Kubernetes cluster. kubectl proxy This will start the server at 127.0.0.1:8001 as shown by the output. Output Starting to serve on 127.0.0.1:8001 Now, assuming that we have already established an SSH tunnel binding to the localhost port 8001 at both ends, open a browser to the link below. Link http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ If everything is running correctly, you should see the dashboard login window. Select the token authentication method and copy your admin token into the field below. Then click the Sign in button. You will then be greeted by the overview of your Kubernetes cluster. While signed in as an admin, you can deploy new pods and services quickly and easily by clicking the plus icon at the top right corner of the dashboard. Then either copy in any configuration file you wish, select the file directly from your machine, or create a new configuration from a form.","title":"Accessing the dashboard"},{"location":"devops/kops/","text":"Creating Kubernetes Cluster with KOPS \u00b6 Install AWS CLI apt update && apt install awscli -y Configure AWS CLI with IAM user Credentials with a specific Region aws configure Note If you are using AWS Instance better to use IAM Role than Creating User with Access-key Check Whether AWS CLI Commands Working or not aws s3 ls Generate SSH Keys ssh-keygen Install kubectl binary with curl on Linux \u00b6 Download the latest release with the command: curl -LO \"https://dl.k8s.io/release/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl\" Install kubectl sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl kubectl version --client Installing Kubernetes with kops \u00b6 Download the latest release with the command: curl -LO https://github.com/kubernetes/kops/releases/download/ $( curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4 ) /kops-linux-amd64 curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-linux-amd64 Make the kops binary executable chmod +x kops-linux-amd64 Move the kops binary into your PATH. sudo mv kops-linux-amd64 /usr/local/bin/kops kops Creating K8s Cluster with KOPS \u00b6 Kops commands to setup k8s cluster:- \u00b6 Creating S3 Bucket for Kubernetes Cluster aws s3 mb s3:// <bucket name> kops create cluster --name = saiteja.irrinki.xyz --state = s3://<s3 bucket> --zones = ap-south-1a,ap-south-1b --node-count = 2 --node-size = t3.medium --master-size = t3.medium --dns-zone = saiteja.irrinki.xyz --node-volume-size = 8 --master-volume-size = 8 It will create a configuration of kops kops update cluster --name = saiteja.irrinki.xyz --state = s3://<s3 bucket> --yes --admin It will create kops data in the S3 bucket. It starts creating a cluster & it takes 10 mins kops validate cluster -- name = saiteja . irrinki . xyz -- state = s3 : //<s3 bucket> It shows ur cluster is ready To Delete Cluster kops delete cluster -- name = saiteja . irrinki . xyz -- state = s3 : //<s3 bucket> --yes","title":"KOPS"},{"location":"devops/kops/#creating-kubernetes-cluster-with-kops","text":"Install AWS CLI apt update && apt install awscli -y Configure AWS CLI with IAM user Credentials with a specific Region aws configure Note If you are using AWS Instance better to use IAM Role than Creating User with Access-key Check Whether AWS CLI Commands Working or not aws s3 ls Generate SSH Keys ssh-keygen","title":"Creating Kubernetes Cluster with KOPS"},{"location":"devops/kops/#install-kubectl-binary-with-curl-on-linux","text":"Download the latest release with the command: curl -LO \"https://dl.k8s.io/release/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl\" Install kubectl sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl kubectl version --client","title":"Install kubectl binary with curl on Linux"},{"location":"devops/kops/#installing-kubernetes-with-kops","text":"Download the latest release with the command: curl -LO https://github.com/kubernetes/kops/releases/download/ $( curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4 ) /kops-linux-amd64 curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-linux-amd64 Make the kops binary executable chmod +x kops-linux-amd64 Move the kops binary into your PATH. sudo mv kops-linux-amd64 /usr/local/bin/kops kops","title":"Installing Kubernetes with kops"},{"location":"devops/kops/#creating-k8s-cluster-with-kops","text":"","title":"Creating K8s Cluster with KOPS"},{"location":"devops/kops/#kops-commands-to-setup-k8s-cluster-","text":"Creating S3 Bucket for Kubernetes Cluster aws s3 mb s3:// <bucket name> kops create cluster --name = saiteja.irrinki.xyz --state = s3://<s3 bucket> --zones = ap-south-1a,ap-south-1b --node-count = 2 --node-size = t3.medium --master-size = t3.medium --dns-zone = saiteja.irrinki.xyz --node-volume-size = 8 --master-volume-size = 8 It will create a configuration of kops kops update cluster --name = saiteja.irrinki.xyz --state = s3://<s3 bucket> --yes --admin It will create kops data in the S3 bucket. It starts creating a cluster & it takes 10 mins kops validate cluster -- name = saiteja . irrinki . xyz -- state = s3 : //<s3 bucket> It shows ur cluster is ready To Delete Cluster kops delete cluster -- name = saiteja . irrinki . xyz -- state = s3 : //<s3 bucket> --yes","title":"Kops commands to setup k8s cluster:-"},{"location":"devops/kubeadm/","text":"Creating Kubernetes Cluster with kubeadm \u00b6 For this activity, deploy minimum 2 AWS instances with the Security groups All traffic is enabled between the instances Node Name Instance Details Resources Master Node -t2.medium 4GB Ram , 2 CPU Worker Node -t2.micro 1GB Ram , 1 CPU Preparing the Master and Worker nodes for kubeadm \u00b6 Execute the below Commands on both Master & Worker Nodes Change the hostname for nodes as master & worker Installing Docker: \u00b6 Add the Docker repository key and Docker repository. curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - add-apt-repository \\ \"deb https://download.docker.com/linux/ $( . /etc/os-release ; echo \" $ID \" ) \\ $( lsb_release -cs ) \\ stable\" Update the list of packages and install the docker apt-get update apt-get install -y docker-ce echo '{\"exec-opts\": [\"native.cgroupdriver=systemd\"]}' | sudo tee /etc/docker/daemon.json systemctl daemon-reload systemctl restart docker systemctl enable docker Installing kubeadm, kublet, and kubectl: \u00b6 Add the Google repository key and Google repository curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - vim /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io kubernetes-xenial main Update the list of packages. And install kubelet, kubeadm, and kubectl. apt-get update apt-get install kubelet kubeadm kubectl -y Disable the swap. swapoff -a Setup Master Node to Connect with Worker Nodes sed -i '/ swap / s/^/#/' /etc/fstab echo NODENAME = $( hostname -s ) kubeadm init --apiserver-advertise-address = 172 .31.85.246 --apiserver-cert-extra-sans = 172 .31.85.246 --pod-network-cidr = 10 .0.0.0/16 Now Create a Directory Name .kube in the master node home directory mkdir .kube Copy the Default conf file to the .kube directory cp /etc/kubernetes/admin.conf .kube/config Replace the Ip address with your Instance Private Ip Address and set pod Network After Executing kubeadm init command you will get one command as output that need to execute on worker nodes kubeadm join 172 .31.85.246:6443 --token n6v08z.53b057pfwnj8mq10 --discovery-token-ca-cert-hash sha256:9e8a38ddfc46c41ecb3317db9fc2145f70bddbdd2c6b8091fbdbab18e1dbcb19 Configuring the Calico in Master Node kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml Now check Kubectl commands kubectl get node Now test the cluster kubectl run pod --image = nginx kubectl get pod kubectl get pod -o wide","title":"Kubeadm"},{"location":"devops/kubeadm/#creating-kubernetes-cluster-with-kubeadm","text":"For this activity, deploy minimum 2 AWS instances with the Security groups All traffic is enabled between the instances Node Name Instance Details Resources Master Node -t2.medium 4GB Ram , 2 CPU Worker Node -t2.micro 1GB Ram , 1 CPU","title":"Creating Kubernetes Cluster with kubeadm"},{"location":"devops/kubeadm/#preparing-the-master-and-worker-nodes-for-kubeadm","text":"Execute the below Commands on both Master & Worker Nodes Change the hostname for nodes as master & worker","title":"Preparing the Master and Worker nodes for kubeadm"},{"location":"devops/kubeadm/#installing-docker","text":"Add the Docker repository key and Docker repository. curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - add-apt-repository \\ \"deb https://download.docker.com/linux/ $( . /etc/os-release ; echo \" $ID \" ) \\ $( lsb_release -cs ) \\ stable\" Update the list of packages and install the docker apt-get update apt-get install -y docker-ce echo '{\"exec-opts\": [\"native.cgroupdriver=systemd\"]}' | sudo tee /etc/docker/daemon.json systemctl daemon-reload systemctl restart docker systemctl enable docker","title":"Installing Docker:"},{"location":"devops/kubeadm/#installing-kubeadm-kublet-and-kubectl","text":"Add the Google repository key and Google repository curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - vim /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io kubernetes-xenial main Update the list of packages. And install kubelet, kubeadm, and kubectl. apt-get update apt-get install kubelet kubeadm kubectl -y Disable the swap. swapoff -a Setup Master Node to Connect with Worker Nodes sed -i '/ swap / s/^/#/' /etc/fstab echo NODENAME = $( hostname -s ) kubeadm init --apiserver-advertise-address = 172 .31.85.246 --apiserver-cert-extra-sans = 172 .31.85.246 --pod-network-cidr = 10 .0.0.0/16 Now Create a Directory Name .kube in the master node home directory mkdir .kube Copy the Default conf file to the .kube directory cp /etc/kubernetes/admin.conf .kube/config Replace the Ip address with your Instance Private Ip Address and set pod Network After Executing kubeadm init command you will get one command as output that need to execute on worker nodes kubeadm join 172 .31.85.246:6443 --token n6v08z.53b057pfwnj8mq10 --discovery-token-ca-cert-hash sha256:9e8a38ddfc46c41ecb3317db9fc2145f70bddbdd2c6b8091fbdbab18e1dbcb19 Configuring the Calico in Master Node kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml Now check Kubectl commands kubectl get node Now test the cluster kubectl run pod --image = nginx kubectl get pod kubectl get pod -o wide","title":"Installing kubeadm, kublet, and kubectl:"},{"location":"devops/kubernetes/","text":"Kubernetes \u00b6 Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery. Kubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. The Top 10 Reasons why Kubernetes is so popular are as follows Largest Open Source project in the world Great Community Support Robust Container deployment Effective Persistent storage Multi-Cloud Support(Hybrid Cloud) Container health monitoring Compute resource management Auto-scaling Feature Support Real-world Use cases Available High availability by cluster federation","title":"Intro"},{"location":"devops/kubernetes/#kubernetes","text":"Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery. Kubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. The Top 10 Reasons why Kubernetes is so popular are as follows Largest Open Source project in the world Great Community Support Robust Container deployment Effective Persistent storage Multi-Cloud Support(Hybrid Cloud) Container health monitoring Compute resource management Auto-scaling Feature Support Real-world Use cases Available High availability by cluster federation","title":"Kubernetes"},{"location":"devops/shellscripting/","text":"Shell Scripting \u00b6 If Condition \u00b6 If Condition If Else Condition Elif Condition Example #!/bin/bash < Command > if [ < condition > ] then < Commands to Execute > < print Messages with echo > fi #!/bin/bash < Command > if [ < condition > ] then < Commands to Execute > < print Messages with echo > else < Commands to Execute > < print Messages with echo > fi #!/bin/bash < Command > if [ < condition > ] then < Commands to Execute > < print Messages with echo > elif [ < condition > ] then < Commands to Execute > < print Messages with echo > else < Commands to Execute > < print Messages with echo > fi #!/bin/bash apt -- help >> / dev / null if [ $ ? - eq 0 ] then echo \" This is Ubuntu Operating System\" else echo \" This is CentOS Operating System\" fi Reference Link How to program with Bash https://opensource.com/article/19/10/programming-bash-logical-operators-shell-expansions For Loop \u00b6 For Loop Example #!/bin/bash for < variable > in < list > do < command > done #For loop example for adding users with name alpha beta gamma #!/bin/bash for USER in alpha beta gamma do echo \"adding user $USER to system\" sudo useradd $USER done While Loop \u00b6 While Loop While Loop Example #!/bin/bash while [ < condition > ] do < command > done #!/bin/bash #Here variable \"a\" is speed a = 0 echo \"Starting the Engine\" while [ $a - le 100 ] do sleep 1 echo \"Current Speed $a\" a = $ (( $a + 10 )) done Shell Script For Setting Up Website \u00b6 Ubuntu Centos Using Variables Using If Else Condition #!/bin/bash sudo apt update sudo apt install wget net - tools unzip figlet apache2 - y sudo systemctl start apache2 sudo systemctl enable apache2 mkdir - p webfiles cd webfiles sudo wget https : //www.tooplate.com/zip-templates/2118_chilling_cafe.zip sudo unzip - o 2118 _chilling_cafe . zip sudo cp - r 2118 _chilling_cafe /* /var/www/html/ cd .. sudo rm -rf webfiles sudo systemctl restart apache2 figlet done #!/bin/bash sudo yum install wget net - tools unzip httpd - y sudo systemctl start httpd sudo systemctl enable httpd mkdir - p webfiles cd webfiles sudo wget https : //www.tooplate.com/zip-templates/2118_chilling_cafe.zip sudo unzip - o 2118 _chilling_cafe . zip sudo cp - r 2118 _chilling_cafe /* /var/www/html/ cd .. sudo rm -rf webfiles sudo systemctl restart httpd #!/bin/bash #Website setup #Adding variables :-) URL = https : //www.tooplate.com/zip-templates/2118_chilling_cafe.zip SRV = httpd PKG = yum FILE = 2118 _chilling_cafe echo \" Installing the Services & Extractors\" echo sudo $PKG install $SRV wget unzip - y >> / dev / null echo \"Start & Enabling the Services\" echo sudo systemctl start $SRV sudo systemctl enable $SRV echo \"Downloading the zip file from tooplate.com\" echo mkdir - p webfiles cd webfiles echo sudo wget $URL >> / dev / null echo \"extracting the files \" echo sudo unzip - o $FILE . zip >> / dev / null echo \"copying the extracted file into html\" echo sudo cp - r $FILE /* /var/www/html >> /dev/null echo \"Restarting the Services\" sudo systemctl restart $SRV cd .. sudo rm -rf webfiles sudo systemctl status $SRV | grep Active date #!/bin/bash apt -- help >> / dev / null if [ $ ? - eq 0 ] then sudo apt update sudo apt install wget figlet net - tools unzip apache2 - y sudo systemctl start apache2 sudo systemctl enable apache2 mkdir - p webfiles cd webfiles sudo wget https : //www.tooplate.com/zip-templates/2118_chilling_cafe.zip sudo unzip - o 2118 _chilling_cafe . zip sudo cp - r 2118 _chilling_cafe /* /var/www/html/ cd .. sudo rm -rf webfiles sudo systemctl restart apache2 sudo systemctl status apache2 | grep Active figlet done else sudo yum install wget net-tools unzip httpd -y sudo systemctl start httpd sudo systemctl enable httpd sudo mkdir -p webfiles cd webfiles sudo wget https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip sudo unzip -o 2118_chilling_cafe.zip sudo cp -r 2118_chilling_cafe/* /var/www/html/ cd .. sudo rm -rf webfiles sudo systemctl restart httpd sudo systemctl status httpd | grep Active fi","title":"Shell Scripting"},{"location":"devops/shellscripting/#shell-scripting","text":"","title":"Shell Scripting"},{"location":"devops/shellscripting/#if-condition","text":"If Condition If Else Condition Elif Condition Example #!/bin/bash < Command > if [ < condition > ] then < Commands to Execute > < print Messages with echo > fi #!/bin/bash < Command > if [ < condition > ] then < Commands to Execute > < print Messages with echo > else < Commands to Execute > < print Messages with echo > fi #!/bin/bash < Command > if [ < condition > ] then < Commands to Execute > < print Messages with echo > elif [ < condition > ] then < Commands to Execute > < print Messages with echo > else < Commands to Execute > < print Messages with echo > fi #!/bin/bash apt -- help >> / dev / null if [ $ ? - eq 0 ] then echo \" This is Ubuntu Operating System\" else echo \" This is CentOS Operating System\" fi Reference Link How to program with Bash https://opensource.com/article/19/10/programming-bash-logical-operators-shell-expansions","title":"If Condition"},{"location":"devops/shellscripting/#for-loop","text":"For Loop Example #!/bin/bash for < variable > in < list > do < command > done #For loop example for adding users with name alpha beta gamma #!/bin/bash for USER in alpha beta gamma do echo \"adding user $USER to system\" sudo useradd $USER done","title":"For Loop"},{"location":"devops/shellscripting/#while-loop","text":"While Loop While Loop Example #!/bin/bash while [ < condition > ] do < command > done #!/bin/bash #Here variable \"a\" is speed a = 0 echo \"Starting the Engine\" while [ $a - le 100 ] do sleep 1 echo \"Current Speed $a\" a = $ (( $a + 10 )) done","title":"While Loop"},{"location":"devops/shellscripting/#shell-script-for-setting-up-website","text":"Ubuntu Centos Using Variables Using If Else Condition #!/bin/bash sudo apt update sudo apt install wget net - tools unzip figlet apache2 - y sudo systemctl start apache2 sudo systemctl enable apache2 mkdir - p webfiles cd webfiles sudo wget https : //www.tooplate.com/zip-templates/2118_chilling_cafe.zip sudo unzip - o 2118 _chilling_cafe . zip sudo cp - r 2118 _chilling_cafe /* /var/www/html/ cd .. sudo rm -rf webfiles sudo systemctl restart apache2 figlet done #!/bin/bash sudo yum install wget net - tools unzip httpd - y sudo systemctl start httpd sudo systemctl enable httpd mkdir - p webfiles cd webfiles sudo wget https : //www.tooplate.com/zip-templates/2118_chilling_cafe.zip sudo unzip - o 2118 _chilling_cafe . zip sudo cp - r 2118 _chilling_cafe /* /var/www/html/ cd .. sudo rm -rf webfiles sudo systemctl restart httpd #!/bin/bash #Website setup #Adding variables :-) URL = https : //www.tooplate.com/zip-templates/2118_chilling_cafe.zip SRV = httpd PKG = yum FILE = 2118 _chilling_cafe echo \" Installing the Services & Extractors\" echo sudo $PKG install $SRV wget unzip - y >> / dev / null echo \"Start & Enabling the Services\" echo sudo systemctl start $SRV sudo systemctl enable $SRV echo \"Downloading the zip file from tooplate.com\" echo mkdir - p webfiles cd webfiles echo sudo wget $URL >> / dev / null echo \"extracting the files \" echo sudo unzip - o $FILE . zip >> / dev / null echo \"copying the extracted file into html\" echo sudo cp - r $FILE /* /var/www/html >> /dev/null echo \"Restarting the Services\" sudo systemctl restart $SRV cd .. sudo rm -rf webfiles sudo systemctl status $SRV | grep Active date #!/bin/bash apt -- help >> / dev / null if [ $ ? - eq 0 ] then sudo apt update sudo apt install wget figlet net - tools unzip apache2 - y sudo systemctl start apache2 sudo systemctl enable apache2 mkdir - p webfiles cd webfiles sudo wget https : //www.tooplate.com/zip-templates/2118_chilling_cafe.zip sudo unzip - o 2118 _chilling_cafe . zip sudo cp - r 2118 _chilling_cafe /* /var/www/html/ cd .. sudo rm -rf webfiles sudo systemctl restart apache2 sudo systemctl status apache2 | grep Active figlet done else sudo yum install wget net-tools unzip httpd -y sudo systemctl start httpd sudo systemctl enable httpd sudo mkdir -p webfiles cd webfiles sudo wget https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip sudo unzip -o 2118_chilling_cafe.zip sudo cp -r 2118_chilling_cafe/* /var/www/html/ cd .. sudo rm -rf webfiles sudo systemctl restart httpd sudo systemctl status httpd | grep Active fi","title":"Shell Script For Setting Up Website"},{"location":"devops/terraform/","text":"Terraform \u00b6 Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can help with multi-cloud by having one workflow for all clouds. The infrastructure Terraform manages can be hosted on public clouds like Amazon Web Services, Microsoft Azure, and Google Cloud Platform, or on-prem in private clouds such as VMWare vSphere. Terraform Installation \u00b6 Ubuntu CentOs curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $( lsb_release -cs ) main\" sudo apt-get update && sudo apt-get install terraform sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo sudo yum -y install terraform Reference \u00b6 Terraform Registry https://registry.terraform.io/ Terraform Commands Cheat sheet Reference Link https://acloudguru.com/blog/engineering/the-ultimate-terraform-cheatsheet Terraform Sample Scripts \u00b6 Launching Ec2 Instance provider \"aws\" { regio n = \"us-east-2\" } resource \"aws_instance\" \"Instance\" { ami = \"ami-0fb653ca2d3203ac1\" i nstan ce_ t ype = \"t2.micro\" key_ na me = \"terraform\" vpc_securi t y_group_ids = [ \"sg-0138c7796472ac9a9\" ] ta gs = { Name = \"IAAC\" Team = \"DevOps\" } } Terraform Vars \u00b6 Using Variables Create a File vars.tf vim vars.tf variable \"REGION\" { de fault = \"us-east-2\" } variable \"ZONE1\" { de fault = \"us-east-2a\" } variable \"AMIS\" { t ype = map(a n y) de fault = { us -e as t -2 = \"ami-0fb653ca2d3203ac1\" us -e as t -1 = \"ami-0e1d30f2c40c4c701\" } } Terraform Provider \u00b6 Example for AWS vim provider.tf provider \"aws\" { regio n = var.REGION } Launching Instance with Vars File vim Instance.tf resource \"aws_instance\" \"Instance\" { ami = var.AMIS [ var.REGION ] i nstan ce_ t ype = \"t2.micro\" key_ na me = \"terraform\" vpc_securi t y_group_ids = [ \"sg-0138c7796472ac9a9\" ] ta gs = { Name = \"IAAC\" Team = \"DevOps\" } } Terraform Provisioning \u00b6 Launching AWS Resources with Terraform Provisioning vim instance_prov.tf resource \"aws_key_pair\" \"testing007\" { key_ na me = \"testing007\" public_key = f ile( \"testing007.pub\" ) } resource \"aws_instance\" \"Instance\" { ami = var.AMIS [ var.REGION ] i nstan ce_ t ype = \"t2.micro\" key_ na me = aws_key_pair. test i n g 007. key_ na me vpc_securi t y_group_ids = [ \"sg-0138c7796472ac9a9\" ] ta gs = { Name = \"IAAC\" Team = \"DevOps\" } provisio ner \"file\" { source = \"./web.sh\" des t i nat io n = \"/tmp/web.sh\" } provisio ner \"remote-exec\" { i nl i ne = [ \"chmod u+x /tmp/web.sh\" , \"sudo /tmp/web.sh\" ] } co nne c t io n { user = var.USER priva te _key = f ile( \"testing007\" ) hos t = sel f .public_ip } } ou t pu t \"PublicIP\" { value = aws_i nstan ce.I nstan ce.public_ip } Variables file - vars.tf variable \"REGION\" { de fault = \"us-east-2\" } variable \"ZONE1\" { de fault = \"us-east-2a\" } variable \"USER\" { de fault = \"ubuntu\" } variable \"AMIS\" { t ype = map(a n y) de fault = { us -e as t -2 = \"ami-0fb653ca2d3203ac1\" us -e as t -1 = \"ami-0e1d30f2c40c4c701\" } } To Store State Remotely in S3 Bucket \u00b6 Create an S3 Bucket terraf orm { backe n d \"s3\" { bucke t = \"terraform-state-009\" key = \"terraform/remote\" regio n = \"us-east-1\" } }","title":"Terraform"},{"location":"devops/terraform/#terraform","text":"Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can help with multi-cloud by having one workflow for all clouds. The infrastructure Terraform manages can be hosted on public clouds like Amazon Web Services, Microsoft Azure, and Google Cloud Platform, or on-prem in private clouds such as VMWare vSphere.","title":"Terraform"},{"location":"devops/terraform/#terraform-installation","text":"Ubuntu CentOs curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $( lsb_release -cs ) main\" sudo apt-get update && sudo apt-get install terraform sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo sudo yum -y install terraform","title":"Terraform Installation"},{"location":"devops/terraform/#reference","text":"Terraform Registry https://registry.terraform.io/ Terraform Commands Cheat sheet Reference Link https://acloudguru.com/blog/engineering/the-ultimate-terraform-cheatsheet","title":"Reference"},{"location":"devops/terraform/#terraform-sample-scripts","text":"Launching Ec2 Instance provider \"aws\" { regio n = \"us-east-2\" } resource \"aws_instance\" \"Instance\" { ami = \"ami-0fb653ca2d3203ac1\" i nstan ce_ t ype = \"t2.micro\" key_ na me = \"terraform\" vpc_securi t y_group_ids = [ \"sg-0138c7796472ac9a9\" ] ta gs = { Name = \"IAAC\" Team = \"DevOps\" } }","title":"Terraform Sample Scripts"},{"location":"devops/terraform/#terraform-vars","text":"Using Variables Create a File vars.tf vim vars.tf variable \"REGION\" { de fault = \"us-east-2\" } variable \"ZONE1\" { de fault = \"us-east-2a\" } variable \"AMIS\" { t ype = map(a n y) de fault = { us -e as t -2 = \"ami-0fb653ca2d3203ac1\" us -e as t -1 = \"ami-0e1d30f2c40c4c701\" } }","title":"Terraform Vars"},{"location":"devops/terraform/#terraform-provider","text":"Example for AWS vim provider.tf provider \"aws\" { regio n = var.REGION } Launching Instance with Vars File vim Instance.tf resource \"aws_instance\" \"Instance\" { ami = var.AMIS [ var.REGION ] i nstan ce_ t ype = \"t2.micro\" key_ na me = \"terraform\" vpc_securi t y_group_ids = [ \"sg-0138c7796472ac9a9\" ] ta gs = { Name = \"IAAC\" Team = \"DevOps\" } }","title":"Terraform Provider"},{"location":"devops/terraform/#terraform-provisioning","text":"Launching AWS Resources with Terraform Provisioning vim instance_prov.tf resource \"aws_key_pair\" \"testing007\" { key_ na me = \"testing007\" public_key = f ile( \"testing007.pub\" ) } resource \"aws_instance\" \"Instance\" { ami = var.AMIS [ var.REGION ] i nstan ce_ t ype = \"t2.micro\" key_ na me = aws_key_pair. test i n g 007. key_ na me vpc_securi t y_group_ids = [ \"sg-0138c7796472ac9a9\" ] ta gs = { Name = \"IAAC\" Team = \"DevOps\" } provisio ner \"file\" { source = \"./web.sh\" des t i nat io n = \"/tmp/web.sh\" } provisio ner \"remote-exec\" { i nl i ne = [ \"chmod u+x /tmp/web.sh\" , \"sudo /tmp/web.sh\" ] } co nne c t io n { user = var.USER priva te _key = f ile( \"testing007\" ) hos t = sel f .public_ip } } ou t pu t \"PublicIP\" { value = aws_i nstan ce.I nstan ce.public_ip } Variables file - vars.tf variable \"REGION\" { de fault = \"us-east-2\" } variable \"ZONE1\" { de fault = \"us-east-2a\" } variable \"USER\" { de fault = \"ubuntu\" } variable \"AMIS\" { t ype = map(a n y) de fault = { us -e as t -2 = \"ami-0fb653ca2d3203ac1\" us -e as t -1 = \"ami-0e1d30f2c40c4c701\" } }","title":"Terraform Provisioning"},{"location":"devops/terraform/#to-store-state-remotely-in-s3-bucket","text":"Create an S3 Bucket terraf orm { backe n d \"s3\" { bucke t = \"terraform-state-009\" key = \"terraform/remote\" regio n = \"us-east-1\" } }","title":"To Store State Remotely in S3 Bucket"},{"location":"devops/vagrant/","text":"Vagrant \u00b6 Vagrant is an open-source tool that helps us to automate the creation and management of Virtual Machines. In a nutshell, we can specify the configuration of a virtual machine in a simple configuration file, and Vagrant creates the same Virtual machine using just one simple command. It provides command-line interfaces to automate such tasks. Requirements Virtualbox Vagrant Installing Virtualbox in the Host Machine \u00b6 Windows Ubuntu Desktop Download link https : //download.virtualbox.org/virtualbox/6.1.30/VirtualBox-6.1.30-148432-Win.exe Through Command line sudo apt update sudo apt install virtualbox Note To Run Virtual Machine You should need to Disable the Secure-Boot in BIOS Options of your Machine To Verify that your Virtual Box is Working fine or not in Linux, Please hit the below command. sudo systemctl status virtualbox.service Installing Vagrant in the Host Machine \u00b6 Windows Ubuntu Desktop Download link https : //www.vagrantup.com/downloads Through Command line sudo apt update sudo apt install vagrant Note You Should Need to Restart Your Machine After Installation of Vagrant Creating a Common Directory for all VMs \u00b6 Create a Directory with name vagrantvms mkdir vagrantvms Change Directory to vagrantvms cd vagrantvms To Bring up VM \u00b6 Ubuntu CentOS Create a Directory name ubuntu mkdir ubuntu Change Directory to ubuntu cd ubuntu Initialize Ubuntu VM vagrant init ubuntu/bionic64 Now Bring up your Ubuntu VM vagrant up Once your VM is up then login to your vm vagrant ssh Now you can see the prompt of the ubuntu machine, If you wanna exit type exit Create a Directory name centos mkdir centos Change Directory to centos cd centos Initialize centos VM vagrant init geerlingguy/centos7 Now Bring up your centos VM vagrant up Once your VM is up then login to your VM vagrant ssh Now you can see the prompt of the centos machine, If you wanna exit type exit To Init Specific Vagrant box https://app.vagrantup.com/boxes/search Multi VMS Vagrantfile Vagrant . configure ( \"2\" ) do | config | config . hostmanager . enabled = true config . hostmanager . manage_host = true ### Nginx VM ### config . vm . define \"web01\" do | web01 | web01 . vm . box = \"ubuntu/bionic64\" web01 . vm . hostname = \"web01\" web01 . vm . network \"private_network\" , ip : \"192.168.33.11\" web01 . vm . synced_folder \"../vprofile-code\" , \"/vprofile-vm-data\" end ### tomcat vm ### config . vm . define \"app01\" do | app01 | app01 . vm . box = \"geerlingguy/centos7\" app01 . vm . hostname = \"app01\" app01 . vm . network \"private_network\" , ip : \"192.168.33.12\" app01 . vm . synced_folder \"../vprofile-code\" , \"/vprofile-vm-data\" app01 . vm . provider \"virtualbox\" do | vb | vb . memory = \"1024\" end app01 . vm . provision \"shell\" , inline : <<- SHELL # yum update -y yum install epel-release -y SHELL end ### RabbitMQ vm #### config . vm . define \"rmq01\" do | rmq01 | rmq01 . vm . box = \"geerlingguy/centos7\" rmq01 . vm . hostname = \"rmq01\" rmq01 . vm . network \"private_network\" , ip : \"192.168.33.16\" rmq01 . vm . synced_folder \"../vprofile-code\" , \"/vprofile-vm-data\" rmq01 . vm . provision \"shell\" , inline : <<- SHELL # yum update -y #yum install epel-release -y # yum install wget -y SHELL end ; f . , v / ### Memcache vm #### config . vm . define \"mc01\" do | mc01 | mc01 . vm . box = \"geerlingguy/centos7\" mc01 . vm . hostname = \"mc01\" mc01 . vm . network \"private_network\" , ip : \"192.168.33.14\" mc01 . vm . synced_folder \"../vprofile-code\" , \"/vprofile-vm-data\" mc01 . vm . provision \"shell\" , inline : <<- SHELL yum install epel-release -y SHELL end ### DB vm #### config . vm . define \"db01\" do | db01 | db01 . vm . box = \"geerlingguy/centos7\" db01 . vm . hostname = \"db01\" db01 . vm . network \"private_network\" , ip : \"192.168.33.15\" db01 . vm . synced_folder \"../vprofile-code\" , \"/vprofile-vm-data\" db01 . vm . provision \"shell\" , inline : <<- SHELL # yum update -y yum install epel-release -y SHELL end end","title":"Vagrant"},{"location":"devops/vagrant/#vagrant","text":"Vagrant is an open-source tool that helps us to automate the creation and management of Virtual Machines. In a nutshell, we can specify the configuration of a virtual machine in a simple configuration file, and Vagrant creates the same Virtual machine using just one simple command. It provides command-line interfaces to automate such tasks. Requirements Virtualbox Vagrant","title":"Vagrant"},{"location":"devops/vagrant/#installing-virtualbox-in-the-host-machine","text":"Windows Ubuntu Desktop Download link https : //download.virtualbox.org/virtualbox/6.1.30/VirtualBox-6.1.30-148432-Win.exe Through Command line sudo apt update sudo apt install virtualbox Note To Run Virtual Machine You should need to Disable the Secure-Boot in BIOS Options of your Machine To Verify that your Virtual Box is Working fine or not in Linux, Please hit the below command. sudo systemctl status virtualbox.service","title":"Installing Virtualbox in the Host Machine"},{"location":"devops/vagrant/#installing-vagrant-in-the-host-machine","text":"Windows Ubuntu Desktop Download link https : //www.vagrantup.com/downloads Through Command line sudo apt update sudo apt install vagrant Note You Should Need to Restart Your Machine After Installation of Vagrant","title":"Installing Vagrant in the Host Machine"},{"location":"devops/vagrant/#creating-a-common-directory-for-all-vms","text":"Create a Directory with name vagrantvms mkdir vagrantvms Change Directory to vagrantvms cd vagrantvms","title":"Creating a Common Directory for all VMs"},{"location":"devops/vagrant/#to-bring-up-vm","text":"Ubuntu CentOS Create a Directory name ubuntu mkdir ubuntu Change Directory to ubuntu cd ubuntu Initialize Ubuntu VM vagrant init ubuntu/bionic64 Now Bring up your Ubuntu VM vagrant up Once your VM is up then login to your vm vagrant ssh Now you can see the prompt of the ubuntu machine, If you wanna exit type exit Create a Directory name centos mkdir centos Change Directory to centos cd centos Initialize centos VM vagrant init geerlingguy/centos7 Now Bring up your centos VM vagrant up Once your VM is up then login to your VM vagrant ssh Now you can see the prompt of the centos machine, If you wanna exit type exit To Init Specific Vagrant box https://app.vagrantup.com/boxes/search Multi VMS Vagrantfile Vagrant . configure ( \"2\" ) do | config | config . hostmanager . enabled = true config . hostmanager . manage_host = true ### Nginx VM ### config . vm . define \"web01\" do | web01 | web01 . vm . box = \"ubuntu/bionic64\" web01 . vm . hostname = \"web01\" web01 . vm . network \"private_network\" , ip : \"192.168.33.11\" web01 . vm . synced_folder \"../vprofile-code\" , \"/vprofile-vm-data\" end ### tomcat vm ### config . vm . define \"app01\" do | app01 | app01 . vm . box = \"geerlingguy/centos7\" app01 . vm . hostname = \"app01\" app01 . vm . network \"private_network\" , ip : \"192.168.33.12\" app01 . vm . synced_folder \"../vprofile-code\" , \"/vprofile-vm-data\" app01 . vm . provider \"virtualbox\" do | vb | vb . memory = \"1024\" end app01 . vm . provision \"shell\" , inline : <<- SHELL # yum update -y yum install epel-release -y SHELL end ### RabbitMQ vm #### config . vm . define \"rmq01\" do | rmq01 | rmq01 . vm . box = \"geerlingguy/centos7\" rmq01 . vm . hostname = \"rmq01\" rmq01 . vm . network \"private_network\" , ip : \"192.168.33.16\" rmq01 . vm . synced_folder \"../vprofile-code\" , \"/vprofile-vm-data\" rmq01 . vm . provision \"shell\" , inline : <<- SHELL # yum update -y #yum install epel-release -y # yum install wget -y SHELL end ; f . , v / ### Memcache vm #### config . vm . define \"mc01\" do | mc01 | mc01 . vm . box = \"geerlingguy/centos7\" mc01 . vm . hostname = \"mc01\" mc01 . vm . network \"private_network\" , ip : \"192.168.33.14\" mc01 . vm . synced_folder \"../vprofile-code\" , \"/vprofile-vm-data\" mc01 . vm . provision \"shell\" , inline : <<- SHELL yum install epel-release -y SHELL end ### DB vm #### config . vm . define \"db01\" do | db01 | db01 . vm . box = \"geerlingguy/centos7\" db01 . vm . hostname = \"db01\" db01 . vm . network \"private_network\" , ip : \"192.168.33.15\" db01 . vm . synced_folder \"../vprofile-code\" , \"/vprofile-vm-data\" db01 . vm . provision \"shell\" , inline : <<- SHELL # yum update -y yum install epel-release -y SHELL end end","title":"To Bring up VM"},{"location":"miscellaneous/activedir/","text":"MISCELLANEOUS \u00b6 To Setup Active Directory Domain Service in Windows Server Click here","title":"Active Directory"},{"location":"miscellaneous/activedir/#miscellaneous","text":"To Setup Active Directory Domain Service in Windows Server Click here","title":"MISCELLANEOUS"},{"location":"miscellaneous/ldap/","text":"Install and Configure OpenLDAP Server on Ubuntu \u00b6 Set hostname for the Ubuntu server \u00b6 sudo hostnamectl set-hostname ldap.k8sengineers.com Add the IP and FQDN to file /etc/hosts vim /etc/hosts 192 .168.0.151 ldap.k8sengineers.com Note Replace ldap.k8sengineers.com with your correct hostname/valid domain name. Install OpenLDAP Server on Ubuntu \u00b6 sudo apt update sudo apt -y install slapd ldap-utils During the installation, you\u2019ll be prompted to set LDAP admin password, provide your desired password, then press Confirm the password and continue the installation by selecting You can confirm that your installation was successful using the commandslapcat to output SLAPD database contents. sudo slapcat dn: dc = k8sengineers,dc = com objectClass: top objectClass: dcObject objectClass: organization o: k8sengineers.com dc: k8sengineers structuralObjectClass: organization entryUUID: 0139eef2-f01c-103b-8cf7-cb31a244bcdd creatorsName: cn = admin,dc = k8sengineers,dc = com createTimestamp: 20211213045057Z entryCSN: 20211213045057 .125264Z#000000#000#000000 modifiersName: cn = admin,dc = k8sengineers,dc = com modifyTimestamp: 20211213045057Z dn: cn = admin,dc = k8sengineers,dc = com objectClass: simpleSecurityObject objectClass: organizationalRole cn: admin description: LDAP administrator userPassword:: e1NTSEF9aXpJOXpJT2tTUUNIUzkrVzJpUVQ5L1M4WVRzZjMvUU4 = structuralObjectClass: organizationalRole entryUUID: 013a5ebe-f01c-103b-8cf8-cb31a244bcdd creatorsName: cn = admin,dc = k8sengineers,dc = com createTimestamp: 20211213045057Z entryCSN: 20211213045057 .128175Z#000000#000#000000 modifiersName: cn = admin,dc = k8sengineers,dc = com modifyTimestamp: 20211213045057Z Add base dn for Users and Groups \u00b6 The next step is adding a base DN for users and groups. Create a file named basedn.ldif with the below contents: vim basedn.ldif dn: ou = people,dc = k8sengineers,dc = com objectClass: organizationalUnit ou: people dn: ou = groups,dc = k8sengineers,dc = com objectClass: organizationalUnit ou: groups Replace k8sengineers and com with your correct domain components. Now add the file by running the command: ldapadd -x -D cn = admin,dc = k8sengineers,dc = com -W -f basedn.ldif Enter LDAP Password: Output adding new entry \"ou=people,dc=k8sengineers,dc=com\" adding new entry \"ou=groups,dc=k8sengineers,dc=com\" Add User Accounts and Groups \u00b6 Generate a password for the user account to add. sudo slappasswd Set the Password Output {SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx Create a ldif file for adding users. vim ldapusers.ldif dn: uid = admin,ou = people,dc = k8sengineers,dc = com objectClass: inetOrgPerson objectClass: posixAccount objectClass: shadowAccount cn: admin sn: Wiz userPassword: { SSHA } ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx loginShell: /bin/bash uidNumber: 2000 gidNumber: 2000 homeDirectory: /home/admin Replace admin with the username to add dc=k8sengineers,dc=com with your correct domain values. cn & sn with your Username Values {SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx with your hashed password When done with the edit, add an account by running. ldapadd -x -D cn = admin,dc = k8sengineers,dc = com -W -f ldapusers.ldif Output adding new entry \"uid=admin,ou=people,dc=k8sengineers,dc=com\" Do the same for the group. Create a ldif file: vim ldapgroups.ldif dn: cn = admin,ou = groups,dc = k8sengineers,dc = com objectClass: posixGroup cn: admin gidNumber: 2000 memberUid: admin Add group: ldapadd -x -D cn = admin,dc = k8sengineers,dc = com -W -f ldapgroups.ldif Output adding new entry \"cn=admin,ou=groups,dc=k8sengineers,dc=com\" You can combine the two into single file. Install LDAP Account Manager on Ubuntu \u00b6 Install Apache Web server & PHP sudo apt -y install apache2 php php-cgi libapache2-mod-php php-mbstring php-common php-pear For Ubuntu 22.04:sudo a2enconf php8.0-cgi For Ubuntu 20.04:sudo a2enconf php7.4-cgi For Ubuntu 18.04: sudo a2enconf php7.2-cgi Here I'm using Ubuntu 20.04: sudo a2enconf php7.4-cgi sudo systemctl reload apache2 Install LDAP Account Manager sudo apt -y install ldap-account-manager Configure LDAP Account Manager http://< IP address >/lam The LDAP Account Manager Login form will be shown. We need to set our LDAP server profile by clicking on[LAM configuration] at the upper right corner. Then click on, Edit server profiles This will ask you for the LAM Profile name Password Note The default password is lam The first thing to change is Profile Password, this is at the end of the General Settings page. Next is to set the LDAP Server address and Tree suffix. Mine looks like below, you need to use your Domain components as set in the server hostname. Set Dashboard login by specifying the admin user account and domain components under the \u201cSecurity settings\u201d section Switch to the \u201cAccount types\u201d page and set Active account types LDAP suffix and List attributes. Add user accounts and groups with LDAP Account Manager \u00b6 Log in with the account admin to the LAM dashboard to start managing user accounts and groups. Add User Group Give the group a name, optional group ID, and description. Add User Accounts Once you have the groups for user accounts to be added, click on Users > New user to add a new user account to your LDAP server. You have three sections for user management: Personal \u2013 This contains the user\u2019s personal information like the first name, last name, email, phone, department, address e.t.c Configure your Ubuntu 22.04|20.04|18.04 as LDAP Client \u00b6 The last step is to configure the systems in your network to authenticate against the LDAP server we\u2019ve just configured: \u00b6 Add LDAP server address to /etc/hosts file if you don\u2019t have an active DNS server in your network. sudo vim /etc/hosts 192 .168.18.50 ldap.k8sengineers.com Install LDAP client utilities on your Ubuntu system: sudo apt -y install libnss-ldap libpam-ldap ldap-utils Begin configuring the settings to look like below * Set LDAP URI- This can be IP address or hostname Set a Distinguished name for the search base Select LDAP version 3 Select Yes for Make local root Database admin Answer No for Does the LDAP database require login? Set LDAP account for root, something like cn=admin,cd=k8sengineers,cn=com Provide LDAP root account Password After the installation, edit /etc/nsswitch.conf and add ldap authentication to passwd and group lines. vim /etc/nsswitch.conf passwd: compat systemd ldap group: compat systemd ldap shadow: compat Modify the file /etc/pam.d/common-password Remove use_authtok on line 26 to look like below. vim /etc/pam.d/common-password password [ success = 1 user_unknown = ignore default = die ] pam_ldap.so try_first_pass Enable creation of home directory on the first login by adding the following line to the end of file /etc/pam.d/common-session session optional pam_mkhomedir.so skel = /etc/skel umask = 077 See the below screenshot: Test by switching to a user account on LDAP sudo su - <username>","title":"LDAP"},{"location":"miscellaneous/ldap/#install-and-configure-openldap-server-on-ubuntu","text":"","title":"Install and Configure OpenLDAP Server on Ubuntu"},{"location":"miscellaneous/ldap/#set-hostname-for-the-ubuntu-server","text":"sudo hostnamectl set-hostname ldap.k8sengineers.com Add the IP and FQDN to file /etc/hosts vim /etc/hosts 192 .168.0.151 ldap.k8sengineers.com Note Replace ldap.k8sengineers.com with your correct hostname/valid domain name.","title":"Set hostname for the Ubuntu server"},{"location":"miscellaneous/ldap/#install-openldap-server-on-ubuntu","text":"sudo apt update sudo apt -y install slapd ldap-utils During the installation, you\u2019ll be prompted to set LDAP admin password, provide your desired password, then press Confirm the password and continue the installation by selecting You can confirm that your installation was successful using the commandslapcat to output SLAPD database contents. sudo slapcat dn: dc = k8sengineers,dc = com objectClass: top objectClass: dcObject objectClass: organization o: k8sengineers.com dc: k8sengineers structuralObjectClass: organization entryUUID: 0139eef2-f01c-103b-8cf7-cb31a244bcdd creatorsName: cn = admin,dc = k8sengineers,dc = com createTimestamp: 20211213045057Z entryCSN: 20211213045057 .125264Z#000000#000#000000 modifiersName: cn = admin,dc = k8sengineers,dc = com modifyTimestamp: 20211213045057Z dn: cn = admin,dc = k8sengineers,dc = com objectClass: simpleSecurityObject objectClass: organizationalRole cn: admin description: LDAP administrator userPassword:: e1NTSEF9aXpJOXpJT2tTUUNIUzkrVzJpUVQ5L1M4WVRzZjMvUU4 = structuralObjectClass: organizationalRole entryUUID: 013a5ebe-f01c-103b-8cf8-cb31a244bcdd creatorsName: cn = admin,dc = k8sengineers,dc = com createTimestamp: 20211213045057Z entryCSN: 20211213045057 .128175Z#000000#000#000000 modifiersName: cn = admin,dc = k8sengineers,dc = com modifyTimestamp: 20211213045057Z","title":"Install OpenLDAP Server on Ubuntu"},{"location":"miscellaneous/ldap/#add-base-dn-for-users-and-groups","text":"The next step is adding a base DN for users and groups. Create a file named basedn.ldif with the below contents: vim basedn.ldif dn: ou = people,dc = k8sengineers,dc = com objectClass: organizationalUnit ou: people dn: ou = groups,dc = k8sengineers,dc = com objectClass: organizationalUnit ou: groups Replace k8sengineers and com with your correct domain components. Now add the file by running the command: ldapadd -x -D cn = admin,dc = k8sengineers,dc = com -W -f basedn.ldif Enter LDAP Password: Output adding new entry \"ou=people,dc=k8sengineers,dc=com\" adding new entry \"ou=groups,dc=k8sengineers,dc=com\"","title":"Add base dn for Users and Groups"},{"location":"miscellaneous/ldap/#add-user-accounts-and-groups","text":"Generate a password for the user account to add. sudo slappasswd Set the Password Output {SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx Create a ldif file for adding users. vim ldapusers.ldif dn: uid = admin,ou = people,dc = k8sengineers,dc = com objectClass: inetOrgPerson objectClass: posixAccount objectClass: shadowAccount cn: admin sn: Wiz userPassword: { SSHA } ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx loginShell: /bin/bash uidNumber: 2000 gidNumber: 2000 homeDirectory: /home/admin Replace admin with the username to add dc=k8sengineers,dc=com with your correct domain values. cn & sn with your Username Values {SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx with your hashed password When done with the edit, add an account by running. ldapadd -x -D cn = admin,dc = k8sengineers,dc = com -W -f ldapusers.ldif Output adding new entry \"uid=admin,ou=people,dc=k8sengineers,dc=com\" Do the same for the group. Create a ldif file: vim ldapgroups.ldif dn: cn = admin,ou = groups,dc = k8sengineers,dc = com objectClass: posixGroup cn: admin gidNumber: 2000 memberUid: admin Add group: ldapadd -x -D cn = admin,dc = k8sengineers,dc = com -W -f ldapgroups.ldif Output adding new entry \"cn=admin,ou=groups,dc=k8sengineers,dc=com\" You can combine the two into single file.","title":"Add User Accounts and Groups"},{"location":"miscellaneous/ldap/#install-ldap-account-manager-on-ubuntu","text":"Install Apache Web server & PHP sudo apt -y install apache2 php php-cgi libapache2-mod-php php-mbstring php-common php-pear For Ubuntu 22.04:sudo a2enconf php8.0-cgi For Ubuntu 20.04:sudo a2enconf php7.4-cgi For Ubuntu 18.04: sudo a2enconf php7.2-cgi Here I'm using Ubuntu 20.04: sudo a2enconf php7.4-cgi sudo systemctl reload apache2 Install LDAP Account Manager sudo apt -y install ldap-account-manager Configure LDAP Account Manager http://< IP address >/lam The LDAP Account Manager Login form will be shown. We need to set our LDAP server profile by clicking on[LAM configuration] at the upper right corner. Then click on, Edit server profiles This will ask you for the LAM Profile name Password Note The default password is lam The first thing to change is Profile Password, this is at the end of the General Settings page. Next is to set the LDAP Server address and Tree suffix. Mine looks like below, you need to use your Domain components as set in the server hostname. Set Dashboard login by specifying the admin user account and domain components under the \u201cSecurity settings\u201d section Switch to the \u201cAccount types\u201d page and set Active account types LDAP suffix and List attributes.","title":"Install LDAP Account Manager on Ubuntu"},{"location":"miscellaneous/ldap/#add-user-accounts-and-groups-with-ldap-account-manager","text":"Log in with the account admin to the LAM dashboard to start managing user accounts and groups. Add User Group Give the group a name, optional group ID, and description. Add User Accounts Once you have the groups for user accounts to be added, click on Users > New user to add a new user account to your LDAP server. You have three sections for user management: Personal \u2013 This contains the user\u2019s personal information like the first name, last name, email, phone, department, address e.t.c","title":"Add user accounts and groups with LDAP Account Manager"},{"location":"miscellaneous/ldap/#configure-your-ubuntu-220420041804-as-ldap-client","text":"","title":"Configure your Ubuntu 22.04|20.04|18.04 as LDAP Client"},{"location":"miscellaneous/ldap/#the-last-step-is-to-configure-the-systems-in-your-network-to-authenticate-against-the-ldap-server-weve-just-configured","text":"Add LDAP server address to /etc/hosts file if you don\u2019t have an active DNS server in your network. sudo vim /etc/hosts 192 .168.18.50 ldap.k8sengineers.com Install LDAP client utilities on your Ubuntu system: sudo apt -y install libnss-ldap libpam-ldap ldap-utils Begin configuring the settings to look like below * Set LDAP URI- This can be IP address or hostname Set a Distinguished name for the search base Select LDAP version 3 Select Yes for Make local root Database admin Answer No for Does the LDAP database require login? Set LDAP account for root, something like cn=admin,cd=k8sengineers,cn=com Provide LDAP root account Password After the installation, edit /etc/nsswitch.conf and add ldap authentication to passwd and group lines. vim /etc/nsswitch.conf passwd: compat systemd ldap group: compat systemd ldap shadow: compat Modify the file /etc/pam.d/common-password Remove use_authtok on line 26 to look like below. vim /etc/pam.d/common-password password [ success = 1 user_unknown = ignore default = die ] pam_ldap.so try_first_pass Enable creation of home directory on the first login by adding the following line to the end of file /etc/pam.d/common-session session optional pam_mkhomedir.so skel = /etc/skel umask = 077 See the below screenshot: Test by switching to a user account on LDAP sudo su - <username>","title":"The last step is to configure the systems in your network to authenticate against the LDAP server we\u2019ve just configured:"},{"location":"miscellaneous/nexus/","text":"Nexus Installation Setup \u00b6 Instance Details Resources CentOS - t2.medium 4GB Ram , 2 CPU Nexus Script #!/bin/bash yum install java-1.8.0-openjdk.x86_64 wget -y mkdir -p /opt/nexus/ mkdir -p /tmp/nexus/ cd /tmp/nexus NEXUSURL = \"https://download.sonatype.com/nexus/3/latest-unix.tar.gz\" wget $NEXUSURL -O nexus.tar.gz EXTOUT = ` tar xzvf nexus.tar.gz ` NEXUSDIR = ` echo $EXTOUT | cut -d '/' -f1 ` rm -rf /tmp/nexus/nexus.tar.gz rsync -avzh /tmp/nexus/ /opt/nexus/ useradd nexus chown -R nexus.nexus /opt/nexus cat <<EOT>> /etc/systemd/system/nexus.service [Unit] Description=nexus service After=network.target [Service] Type=forking LimitNOFILE=65536 ExecStart=/opt/nexus/$NEXUSDIR/bin/nexus start ExecStop=/opt/nexus/$NEXUSDIR/bin/nexus stop User=nexus Restart=on-abort [Install] WantedBy=multi-user.target EOT echo 'run_as_user=\"nexus\"' > /opt/nexus/ $NEXUSDIR /bin/nexus.rc systemctl daemon-reload systemctl start nexus systemctl enable nexus","title":"Nexus"},{"location":"miscellaneous/nexus/#nexus-installation-setup","text":"Instance Details Resources CentOS - t2.medium 4GB Ram , 2 CPU Nexus Script #!/bin/bash yum install java-1.8.0-openjdk.x86_64 wget -y mkdir -p /opt/nexus/ mkdir -p /tmp/nexus/ cd /tmp/nexus NEXUSURL = \"https://download.sonatype.com/nexus/3/latest-unix.tar.gz\" wget $NEXUSURL -O nexus.tar.gz EXTOUT = ` tar xzvf nexus.tar.gz ` NEXUSDIR = ` echo $EXTOUT | cut -d '/' -f1 ` rm -rf /tmp/nexus/nexus.tar.gz rsync -avzh /tmp/nexus/ /opt/nexus/ useradd nexus chown -R nexus.nexus /opt/nexus cat <<EOT>> /etc/systemd/system/nexus.service [Unit] Description=nexus service After=network.target [Service] Type=forking LimitNOFILE=65536 ExecStart=/opt/nexus/$NEXUSDIR/bin/nexus start ExecStop=/opt/nexus/$NEXUSDIR/bin/nexus stop User=nexus Restart=on-abort [Install] WantedBy=multi-user.target EOT echo 'run_as_user=\"nexus\"' > /opt/nexus/ $NEXUSDIR /bin/nexus.rc systemctl daemon-reload systemctl start nexus systemctl enable nexus","title":"Nexus Installation Setup"},{"location":"miscellaneous/sonarqube/","text":"SonarQube Installation Script \u00b6 Instance Details Resources Ubuntu - t2.medium 4GB Ram , 2 CPU SonarQube Script #!/bin/bash cp /etc/sysctl.conf /root/sysctl.conf_backup cat <<EOT> /etc/sysctl.conf vm.max_map_count=262144 fs.file-max=65536 ulimit -n 65536 ulimit -u 4096 EOT cp /etc/security/limits.conf /root/sec_limit.conf_backup cat <<EOT> /etc/security/limits.conf sonarqube - nofile 65536 sonarqube - nproc 409 EOT sudo apt-get update -y sudo apt-get install openjdk-11-jdk -y sudo update-alternatives --config java java -version sudo apt update wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add - sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list' sudo apt install postgresql postgresql-contrib -y #sudo -u postgres psql -c \"SELECT version();\" sudo systemctl enable postgresql.service sudo systemctl start postgresql.service sudo echo \"postgres:admin123\" | chpasswd runuser -l postgres -c \"createuser sonar\" sudo -i -u postgres psql -c \"ALTER USER sonar WITH ENCRYPTED PASSWORD 'admin123';\" sudo -i -u postgres psql -c \"CREATE DATABASE sonarqube OWNER sonar;\" sudo -i -u postgres psql -c \"GRANT ALL PRIVILEGES ON DATABASE sonarqube to sonar;\" systemctl restart postgresql #systemctl status -l postgresql netstat -tulpena | grep postgres sudo mkdir -p /sonarqube/ cd /sonarqube/ sudo curl -O https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.3.0.34182.zip sudo apt-get install zip -y sudo unzip -o sonarqube-8.3.0.34182.zip -d /opt/ sudo mv /opt/sonarqube-8.3.0.34182/ /opt/sonarqube sudo groupadd sonar sudo useradd -c \"SonarQube - User\" -d /opt/sonarqube/ -g sonar sonar sudo chown sonar:sonar /opt/sonarqube/ -R cp /opt/sonarqube/conf/sonar.properties /root/sonar.properties_backup cat <<EOT> /opt/sonarqube/conf/sonar.properties sonar.jdbc.username=sonar sonar.jdbc.password=admin123 sonar.jdbc.url=jdbc:postgresql://localhost/sonarqube sonar.web.host=0.0.0.0 sonar.web.port=9000 sonar.web.javaAdditionalOpts=-server sonar.search.javaOpts=-Xmx512m -Xms512m -XX:+HeapDumpOnOutOfMemoryError sonar.log.level=INFO sonar.path.logs=logs EOT cat <<EOT> /etc/systemd/system/sonarqube.service [Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop User=sonar Group=sonar Restart=always LimitNOFILE=65536 LimitNPROC=4096 [Install] WantedBy=multi-user.target EOT systemctl daemon-reload systemctl enable sonarqube.service #systemctl start sonarqube.service #systemctl status -l sonarqube.service apt-get install nginx -y rm -rf /etc/nginx/sites-enabled/default rm -rf /etc/nginx/sites-available/default cat <<EOT> /etc/nginx/sites-available/sonarqube server{ listen 80; server_name sonarqube.groophy.in; access_log /var/log/nginx/sonar.access.log; error_log /var/log/nginx/sonar.error.log; proxy_buffers 16 64k; proxy_buffer_size 128k; location / { proxy_pass http://127.0.0.1:9000; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_redirect off; proxy_set_header Host \\$host; proxy_set_header X-Real-IP \\$remote_addr; proxy_set_header X-Forwarded-For \\$proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto http; } } EOT ln -s /etc/nginx/sites-available/sonarqube /etc/nginx/sites-enabled/sonarqube systemctl enable nginx.service #systemctl restart nginx.service sudo ufw allow 80 ,9000,9001/tcp echo \"System reboot in 30 sec\" sleep 30 reboot SonarQube-Analysis-Properties sonar . projectKey = vprofile sonar . projectName = vprofile - repo sonar . projectVersion = 1.0 sonar . sources = src / sonar . java . binaries = target / test - classes / com / visualpathit / account / controllerTest / sonar . junit . reportsPath = target / surefire - reports / sonar . jacoco . reportsPath = target / jacoco . exec sonar . java . checkstyle . reportPaths = target / checkstyle - result . xml","title":"SonarQube"},{"location":"miscellaneous/sonarqube/#sonarqube-installation-script","text":"Instance Details Resources Ubuntu - t2.medium 4GB Ram , 2 CPU SonarQube Script #!/bin/bash cp /etc/sysctl.conf /root/sysctl.conf_backup cat <<EOT> /etc/sysctl.conf vm.max_map_count=262144 fs.file-max=65536 ulimit -n 65536 ulimit -u 4096 EOT cp /etc/security/limits.conf /root/sec_limit.conf_backup cat <<EOT> /etc/security/limits.conf sonarqube - nofile 65536 sonarqube - nproc 409 EOT sudo apt-get update -y sudo apt-get install openjdk-11-jdk -y sudo update-alternatives --config java java -version sudo apt update wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add - sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list' sudo apt install postgresql postgresql-contrib -y #sudo -u postgres psql -c \"SELECT version();\" sudo systemctl enable postgresql.service sudo systemctl start postgresql.service sudo echo \"postgres:admin123\" | chpasswd runuser -l postgres -c \"createuser sonar\" sudo -i -u postgres psql -c \"ALTER USER sonar WITH ENCRYPTED PASSWORD 'admin123';\" sudo -i -u postgres psql -c \"CREATE DATABASE sonarqube OWNER sonar;\" sudo -i -u postgres psql -c \"GRANT ALL PRIVILEGES ON DATABASE sonarqube to sonar;\" systemctl restart postgresql #systemctl status -l postgresql netstat -tulpena | grep postgres sudo mkdir -p /sonarqube/ cd /sonarqube/ sudo curl -O https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.3.0.34182.zip sudo apt-get install zip -y sudo unzip -o sonarqube-8.3.0.34182.zip -d /opt/ sudo mv /opt/sonarqube-8.3.0.34182/ /opt/sonarqube sudo groupadd sonar sudo useradd -c \"SonarQube - User\" -d /opt/sonarqube/ -g sonar sonar sudo chown sonar:sonar /opt/sonarqube/ -R cp /opt/sonarqube/conf/sonar.properties /root/sonar.properties_backup cat <<EOT> /opt/sonarqube/conf/sonar.properties sonar.jdbc.username=sonar sonar.jdbc.password=admin123 sonar.jdbc.url=jdbc:postgresql://localhost/sonarqube sonar.web.host=0.0.0.0 sonar.web.port=9000 sonar.web.javaAdditionalOpts=-server sonar.search.javaOpts=-Xmx512m -Xms512m -XX:+HeapDumpOnOutOfMemoryError sonar.log.level=INFO sonar.path.logs=logs EOT cat <<EOT> /etc/systemd/system/sonarqube.service [Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop User=sonar Group=sonar Restart=always LimitNOFILE=65536 LimitNPROC=4096 [Install] WantedBy=multi-user.target EOT systemctl daemon-reload systemctl enable sonarqube.service #systemctl start sonarqube.service #systemctl status -l sonarqube.service apt-get install nginx -y rm -rf /etc/nginx/sites-enabled/default rm -rf /etc/nginx/sites-available/default cat <<EOT> /etc/nginx/sites-available/sonarqube server{ listen 80; server_name sonarqube.groophy.in; access_log /var/log/nginx/sonar.access.log; error_log /var/log/nginx/sonar.error.log; proxy_buffers 16 64k; proxy_buffer_size 128k; location / { proxy_pass http://127.0.0.1:9000; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_redirect off; proxy_set_header Host \\$host; proxy_set_header X-Real-IP \\$remote_addr; proxy_set_header X-Forwarded-For \\$proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto http; } } EOT ln -s /etc/nginx/sites-available/sonarqube /etc/nginx/sites-enabled/sonarqube systemctl enable nginx.service #systemctl restart nginx.service sudo ufw allow 80 ,9000,9001/tcp echo \"System reboot in 30 sec\" sleep 30 reboot SonarQube-Analysis-Properties sonar . projectKey = vprofile sonar . projectName = vprofile - repo sonar . projectVersion = 1.0 sonar . sources = src / sonar . java . binaries = target / test - classes / com / visualpathit / account / controllerTest / sonar . junit . reportsPath = target / surefire - reports / sonar . jacoco . reportsPath = target / jacoco . exec sonar . java . checkstyle . reportPaths = target / checkstyle - result . xml","title":"SonarQube Installation Script"}]}