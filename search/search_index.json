{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Blog","text":"<p> <p>About</p> <p></p>"},{"location":"about/","title":"About","text":"<p>Hello, my name is SAITEJA IRRINKI, and I work as a Senior DevOps Engineer in Build &amp; Release. I'm experienced in Infrastructure Provisioning, Configuration Management, Version Control, Code Quality, Environment Administration, Defect Tracking, Release Management, Continuous Integration, and Continuous Delivery, Cloud Computing, Scripting for Automation, Static Web Development, Orchestration-Technologies, and Operating Systems- Linux and Windows.</p> <p> +91 9493322788</p> <p> saitejairrinki91@gmail.com</p> <p> https://www.instagram.com/saitejairrinki/ </p> <p> https://saitejairrinki.medium.com/</p>"},{"location":"awsdevops/","title":"DevOps","text":"<p>DevOps is the combination of cultural philosophies, practices, and tools that increases an organization\u2019s ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.</p> <p></p>"},{"location":"blog/","title":"My DevOps Blog","text":"<p>Welcome to my DevOps Blog. Here you will find a collection of articles and tutorials on various DevOps topics. </p> <p>Whether you are just getting started with DevOps or a pro, I hope you find something useful here. </p> <p>This blog is all about my thoughts, experiences, Challanges and lessons learned in the world of DevOps.</p> <p>I have been working in the IT industry and have been involved in a variety of roles including InfraStructure Provisioning, system administration and operations. In recent years I have become very interested in the DevOps movement and how it can help organisations to improve the way they deliver software.</p> <p>I hope you enjoy reading my blog and please feel free to leave any comments or questions</p> <p> saitejairrinki91@gmail.com</p> <p> +91 9493322788</p> <p>The majority of the content in this blog is created by me through research and conducting POC, while only a small number of items are sourced from the internet, such as definitions,icons,pictures and etc used for reference.</p> <p>Thanks for reading!</p>"},{"location":"job_roles/","title":"DevOps ENGINEER ROLES","text":""},{"location":"job_roles/#aws-tasks","title":"AWS Tasks:","text":"<p>Managing Ec2 Instances, EIP, Network Interfaces, Security Groups &amp; Key Pairs  Managing EBS Volumes, AMI &amp; Snapshots (Backup &amp; Restore, Migration, etc.) Setup &amp; Managing Elastic Load Balancers, ACM &amp; Autoscaling Groups Setting &amp; Managing Cloudwatch Alarms on metrics from Ec2, ELB &amp; RDS  Creating &amp; Managing RDS Instances, RDS Snapshots, Updating Parameters Groups  AWS CLI for any AWS Tasks</p>"},{"location":"job_roles/#cloud-migration-using-lift-shift-strategy-on-aws-services-using","title":"Cloud Migration using Lift &amp; Shift Strategy on AWS, Services using","text":"<ul> <li> <p>VPC, Ec2, S3, Application Load Balancer, Route53 </p> </li> <li> <p>IAM to give secure access to AWS account using MFA</p> </li> <li> <p>Tightly controlled Security Group for firewall rules of EC2</p> </li> <li> <p>EBS volume for storage on Ec2, Snapshot for Backups of EBS</p> </li> <li> <p>Autoscaling for Automatic scaling of Ec2 instance based on CPU usage</p> </li> <li> <p>Modernization on AWS Cloud, Services used</p> </li> <li> <p>Beanstalk for PAAS for Tomcat Web App</p> </li> <li> <p>RDS for MySQL Database</p> </li> <li> <p>Object storage S3 to store and retrieve files</p> </li> <li> <p>Route53 for Private &amp; Public Hosted zones/Records</p> </li> <li> <p>Amazon MQ for fully managed RabbitMQ</p> </li> <li> <p>ElastiCache for in-memory datastore in cloud</p> </li> <li> <p>Monitoring with CloudWatch, Grafana</p> </li> <li> <p>Notification using SNS</p> </li> </ul>"},{"location":"job_roles/#aws-cloud-automation-using","title":"AWS Cloud Automation using","text":"<ul> <li>Ansible</li> <li>CloudFormation (Stacks)</li> </ul>"},{"location":"job_roles/#aws-securities","title":"AWS Securities","text":"<ul> <li>Inspector &amp; Best practices</li> <li>IAM management</li> </ul>"},{"location":"job_roles/#azure-azure-devops","title":"Azure &amp; Azure DevOps","text":"<ul> <li>Automated the provisioning of Azure resources (App Registration, Blob, Back-up Center, Cosmos DB, Key Vault, Azure VMs) using PowerShell and Templates.</li> <li>Created a Build/Release pipeline with automated build and Continuous-Integration using Azure DevOps</li> </ul>"},{"location":"job_roles/#continuous-delivery-of-java-web-application","title":"Continuous Delivery of Java Web Application","text":"<ul> <li> <p>CICD Pipeline using Jenkins, Git, Maven, Nexus, S3 &amp; SonarQube</p> </li> <li> <p>Deploying Artifact to Beanstack</p> </li> <li> <p>Jenkins Pipeline As A Code for CICD</p> </li> <li> <p>Website Automation through Jenkins whenever there is a GIT Commit</p> </li> </ul>"},{"location":"job_roles/#configuration-management-using-ansible","title":"Configuration Management using Ansible","text":"<ul> <li>Ansible AdHoc commands to execute remote tasks</li> <li>Ansible playbook for Service/Server Deployments</li> <li>Ansible playbook to setup VPC &amp; Bastion Host on AWS</li> <li>Writing our own configuration file (ansible.cfg)</li> <li>Ansible Roles for modular &amp; reusable automation framework</li> </ul>"},{"location":"job_roles/#docker-containers","title":"Docker Containers","text":"<ul> <li>Building customized docker images using Dockerfile</li> <li>Docker-compose to define &amp; run MultiContainer Docker Application</li> </ul>"},{"location":"job_roles/#kubernetes","title":"Kubernetes","text":"<ul> <li>Creating Production grade K8s cluster using Kops &amp; Kubeadm</li> <li>Hosting Containerized Application on K8s cluster using Pod, Service, Replication Controller, Deployment, Secrets &amp; ConfigMap</li> </ul>"},{"location":"job_roles/#mkdocs","title":"MkDocs","text":"<ul> <li>Designing and Building a Static Website using MkDocs (The Current Website you're watching) </li> </ul>"},{"location":"profile_summary/","title":"PROFILESUMMARY","text":"<ul> <li>Experience in IT area comprising the configuration management, Deploy, CI/CD pipeline, AWS, and DevOps methodologies.</li> <li>Expertise in troubleshooting the problems generated while building and deploying.</li> <li>Working Experience on Git, GitHub, and Jenkins. Debugging issues if there is any failure in broken Jenkins builds and maintaining Jenkins build pipeline.</li> <li>Expertise Knowledge in Source Code Management (version control system) tools using GIT. </li> <li>Experienced in Branching, Merging, and Tagging concepts in Version Control tools like GIT.</li> <li>Proficient in developing Continuous Integration / Continuous Delivery pipelines.</li> <li>Experience with containerization tools like Docker and Kubernetes. </li> <li>Implemented Docker-based Continues Integration and Deployment framework.</li> <li>Strong experience in building tools and packaging the source code using Maven. </li> <li>Scheduled builds overnight to support development needs using Jenkins, Git, and Maven. </li> <li>Experience in integrating Unit Tests and Code Quality Analysis Tools like SonarQube. </li> <li>Experience in using Nexus Repository Manager and S3 Bucket for Maven builds.</li> <li>Experience in orchestration tools like Ansible and Kubernetes. </li> <li>Experience with services IAM, Compute Engine, Kubernetes Engine, Storage Services, S3 Bucket, and VPC Network.</li> <li>Experience with Amazon Web services Creating, configuring, and Managing EC2, Storage, IAM, S3, VPC, ELB, EFS, SNS, Route53, and some more services in AWS.</li> <li>Experience using Apache Tomcat &amp; Red Hat Server application servers for deployments.</li> <li>Working experience with operating systems like Linux and Windows.</li> <li>Performed continuous Build and Deployments to multiple DEV, QA, PRE-Prod, and PROD environments.</li> <li>Working Experience in Azure Cloud and Azure DevOps</li> <li>Ensuring security and compliance across pipelines and repositories using Azure DevOps provides security features like RBAC.</li> <li>Performed continuous Build and Deployments to multiple DEV, QA, Validation and PROD Environments</li> <li>Know about Google Cloud Platform.</li> <li>Have good Knowledge of Terraform.</li> <li>Have good Knowledge of Shell Scripting.</li> <li>Have Knowledge of Kubernetes Core Concepts.</li> <li>Experience with Styra Declarative Authorization Service (DAS).</li> <li>Known about Open Policy Agent (OPA).</li> </ul>"},{"location":"profile_summary/#academic-details","title":"ACADEMIC DETAILS","text":"<ul> <li>B.Tech - 2020</li> </ul>"},{"location":"profile_summary/#professional-experience","title":"PROFESSIONAL EXPERIENCE","text":"<ul> <li>Working as a DevOps Engineer from March-2021 to till-date.</li> </ul>"},{"location":"profile_summary/#strengths","title":"STRENGTHS","text":"<ul> <li>Flexibility and adaptability to work in any environment.</li> <li>Good troubleshooting skills.</li> <li>Willingness to accept any challenge irrespective of its complexity.</li> <li>Good team player with a positive attitude.</li> </ul>"},{"location":"project/","title":"Projects","text":""},{"location":"project/#project-1","title":"PROJECT 1","text":""},{"location":"project/#role-devops-engineer","title":"Role: DevOps Engineer","text":""},{"location":"project/#environment-git-github-maven-apache-tomcat-jenkins-linux-sonarqube-nexus-aws-ansible","title":"Environment: Git, GitHub, Maven, Apache Tomcat, Jenkins, Linux, SonarQube, Nexus, AWS, Ansible.","text":""},{"location":"project/#roles-and-responsibilities","title":"Roles and Responsibilities :","text":"<ul> <li>Configured Git with Jenkins and scheduled jobs using the POLL SCM option</li> <li>Installed and configured GIT and communicated with the repositories in GitHUB. Collaborate with different teams to deploy application code into Dev, QA, and Staging.</li> <li>Installing and updating the Jenkins plugins to achieve CI/CD.</li> <li>Responsible for installing Jenkins master and slave nodes.</li> <li>Created Jenkins CICD pipelines for continuous build &amp; deployment and integrated Junit and SonarQube plugins in Jenkins for automated testing and code quality check.</li> <li>Integrated SonarQube with Jenkins for continuous inspection of code quality and analysis with SonarQube scanner for Maven.</li> <li>Managed Sonatype Nexus repositories to download the artifacts (jar, war &amp; ear) during the build.</li> <li>Wrote playbook manifests for deploying, configuring, and managing components.</li> <li>Managing the working environments through configuration management tools ansible.</li> <li>Working with developers and Testers to test the source code and applications through Jenkins plugins.</li> <li>Installation of apache, tomcat, and troubleshooting web server issues.</li> <li>Administration and maintenance of servers using Red Hat Linux/CentOS-7,8.</li> <li>Installing and configuration of ansible server. * Implemented AWS solutions using EC2, S3, EBS, ELB, Route53, Auto scaling groups.</li> <li>Built servers using AWS, importing volumes, launching EC2, creating Security groups, Auto-scaling, Load balancers (ELBs) using Cloud formation templates &amp; AMIs using Infrastructure as a Service (IaaS Including EC2 and S3), focusing on high availability, fault tolerance, and auto-scaling.</li> <li>Configured ELB with different launch configurations using AMI and EC2 Autoscaling groups.</li> <li>Creating S3 buckets and S3 life cycle policies and bucket policies (Read/Write).</li> <li>Creating EBS Volumes and snapshots and attaching them to the EC2 instances.</li> </ul>"},{"location":"project/#project-2","title":"PROJECT 2","text":""},{"location":"project/#role-devops-engineer_1","title":"Role: DevOps Engineer","text":""},{"location":"project/#environment-git-github-maven-nexus-sonarqube-jenkins-docker-kubernetes-aws-linux","title":"Environment: Git, GitHub, Maven, Nexus, SonarQube, Jenkins, Docker, Kubernetes, AWS, Linux.","text":""},{"location":"project/#roles-and-responsibilities_1","title":"Roles and Responsibilities :","text":"<ul> <li>Involved in CI/CD process and integrated GIT, Nexus, SonarQube, and Maven artifacts build with and and and and Jenkins and creating Docker image and using the Docker image to deploy over Kubernetes. </li> <li>Building and deploying various microservices in EKS. </li> <li>Creating and maintaining namespaces, config maps, secrets, service, ingress, RBAC in Kubernetes. </li> <li>Implemented and maintained the Branching and build/ release strategies utilizing GIT. </li> <li>Experience with container-based deployments using Docker, working with Docker Images, Docker Hub and Docker-registries and Kubernetes. </li> <li>Building/Maintaining Docker container clusters managed by Kubernetes Linux, Bash, GIT, Docker. </li> <li>Implemented docker-maven-plugin in maven pom to build docker images for all microservices and later used Docker file to build the docker images from the java jar files. </li> <li>Utilized Kubernetes for the runtime environment of the CI/CD system to build, and test deploy. </li> <li>Experience in working on AWS and its services like AWS IAM, VPC, EC2, EKS, EBS, S3, ELB, Auto Scaling, Route 53, Cloud Front, Cloud Watch, Cloud Trail, and SNS. </li> <li>Experienced in Cloud automation using AWS Cloud Formation templates to create customized VPC, subnets, NAT, EC2 instances, ELB, and Security groups. </li> <li>Experienced in creating complex IAM policies, Roles, and user management for delegated acceaccess withAWS.</li> <li>Identify, troubleshoot and resolve issues related to the build and deploy process. </li> <li>Deploying Docker images in Kubernetes cluster using Yaml files and exposing the application to the internet using service object.</li> </ul>"},{"location":"project/#project-3","title":"PROJECT 3","text":""},{"location":"project/#role-devops-teamlead","title":"Role: DevOps - Teamlead","text":""},{"location":"project/#environment-azure-cloud-services-azure-devops-powershell-power-apps-windows-os","title":"Environment: Azure Cloud Services, Azure DevOps, PowerShell, Power Apps, Windows OS.","text":""},{"location":"project/#roles-and-responsibilities_2","title":"Roles and Responsibilities:","text":"<ul> <li>Worked with team to Develop Application Requirements.</li> <li>Created a Workflow &amp; Branching Strategy with the Requirements.</li> <li>Setup/managed Azure Repos and Branching Strategy.</li> <li>Established a Build/Release pipeline with automated build and Continuous-Integration.</li> <li>Automated the provisioning of Powerapps Environments (Dev, Staging, Pre-Prod, Prod) using Azure DevOps Tasks and Azure resources (App Registration, Blob, Back-up Center, Cosmos DB, Key Vault, Azure VMs) using PowerShell and Templates.</li> <li>Validated Azure Resources during provisioning &amp; pre-configuration by writing a PowerShell Script.</li> <li>Monitored Automated build &amp; Continuous-Integration process to drive Build/Release Failure Resolution.</li> <li>Automated and Implemented system backup and recovery procedures.</li> <li>Worked with Software Development and Testing-Team members to design and develop robust solutions to meet client requirements for functionality, scalability, and performance.</li> <li>Documented project design for reference and future use cases</li> <li>Involved in CI/CD process and integrated SonarQube with Azure DevOps for QualityAnalysis.</li> <li>Have hands-on experience in writing PowerShell when needed to automate jobs and tasks in the pipeline.</li> <li>Experience deploying infrastructure resources utilizing ARM templates.</li> <li>Maintained Source code Repositories</li> <li>Implemented high availability with Azure classic and Azure Resource Manager (ARM) deployment models.</li> <li>Experience delivering applications in Azure.</li> <li>Ability to Document deployment processes and transfer knowledge to other operational team members.</li> <li>Ensure all cloud deployments follow established workflows.</li> <li>Supports Development and production systems and deployments.</li> </ul>"},{"location":"resume/","title":"Resume","text":"<p>To View My Resume please click here  Resume</p>"},{"location":"technical_skills/","title":"TECHNICAL SKILLS","text":"Category Tools Technologies &amp; Softwares Operating Systems - Linux   Windows  Virtualization - Vagrant Cloud Platform - AWS    AZURE   GCP  Scripting Languages - Shell Scripting  PowerShell  Configuration Management Tool - Ansible  Infrastructure as Code - Terraform   Cloud Formation  Version Control Tool - GIT  Build Software - Maven Build &amp; Release/CICD - Azure DevOps  Jenkins  Containerization Tools - Docker   Kubernetes  Web/App Server - Apache Tomcat  Ticketing Tool - Freshdesk  Static Web Development - MkDocs Repository Manager - Nexus Repository Monitoring - CloudWatch  Grafana Code Quality Analysis - SonarQube/Cloud"},{"location":"azure/branchingstrategy/","title":"Branching strategies","text":"<p>Azure DevOps supports several branching strategies that teams can use to manage source code and collaborate on development. Here are some of the most common branching strategies:</p> <p>Mainline/Branching: In this strategy, all developers work on a single branch, usually called \"master\" or \"main\". Developers make changes directly to this branch, and all changes are integrated and tested continuously. This strategy is best for small teams or small projects where the development cycle is short.</p> <p>Feature Branching: In this strategy, each new feature is developed on a separate branch, which is later merged into the main branch when the feature is complete. This strategy allows multiple features to be developed simultaneously without impacting the main branch. It is a common strategy for large projects with multiple teams working on different features.</p> <p>Release Branching: In this strategy, a separate branch is created for each release. Development work is done on the main branch, and when it's time for a release, a new branch is created from the main branch. Any bug fixes or changes are made on this branch and then merged back into the main branch after the release is completed. This strategy is best for projects with frequent releases.</p> <p>Gitflow: This is a variation of the feature branching strategy. It involves two long-lived branches: \"master\" and \"develop\", and multiple short-lived branches for feature development and hotfixes. Features are developed on separate branches and then merged into the \"develop\" branch for integration testing. Once the feature is complete, it is merged into the \"master\" branch for release.</p> <p></p> <p>Note</p> <p>These are just a few of the branching strategies that can be used with Azure DevOps. The best strategy for a team depends on the project requirements, team size, and development cycle. It is essential to choose a strategy that aligns with the team's workflow and ensures smooth collaboration and code management.</p>"},{"location":"azure/comparison/","title":"Azure DevOps vs Jenkins","text":"<p>Azure DevOps and Jenkins are two popular tools for continuous integration and delivery (CI/CD) that help teams to automate the software development and delivery process. </p>"},{"location":"azure/comparison/#here-are-some-key-differences-between-azure-devops-and-jenkins","title":"Here are some key differences between Azure DevOps and Jenkins:","text":"<p>Hosted vs. Self-hosted: Azure DevOps is a cloud-based service, while Jenkins is a self-hosted solution that can be installed on-premises or on a virtual machine. This means that Azure DevOps offers the convenience of not having to worry about maintaining servers, while Jenkins offers more control over the environment.</p> <p>Integration with Microsoft Tools: Azure DevOps is tightly integrated with Microsoft's development tools, including Visual Studio and Azure. This integration allows for seamless collaboration and provides a more cohesive solution for teams that use Microsoft's tools. Jenkins, on the other hand, has a wider range of integrations with various tools and platforms.</p> <p>Ease of use: Azure DevOps provides a user-friendly interface that is easy to navigate and use, making it an excellent option for small to medium-sized teams. Jenkins, however, can be more complex and requires more technical knowledge to set up and configure.</p> <p>Security: Azure DevOps provides enterprise-grade security and compliance features, such as multi-factor authentication, encryption, and compliance with industry standards. Jenkins, on the other hand, requires additional plugins and configurations to provide the same level of security.</p> <p>Cost: Azure DevOps offers a range of pricing options, including a free tier for small teams. Jenkins, on the other hand, is open-source and free to use, but may require additional costs for hosting and maintenance.</p> <p>Conclusion</p> <p>Overall, Azure DevOps is a more integrated and user-friendly solution that is better suited for small to medium-sized teams or those using Microsoft tools. Jenkins, on the other hand, offers more flexibility and control for larger teams and organizations that require a more customized CI/CD solution.</p>"},{"location":"azure/intro/","title":"Introduction","text":"<p>Azure DevOps is a comprehensive set of development tools that provides an integrated and collaborative environment for software development teams. It offers a suite of services that includes version control, agile project management, continuous integration and delivery, testing and release management capabilities.</p> <p>Azure DevOps was previously known as Visual Studio Team Services (VSTS) and provides cloud-hosted services that can be accessed from anywhere with an internet connection. It is a cloud-based solution that allows teams to collaborate on projects, track progress, manage code repositories, build and test applications, and deploy applications to production.</p>"},{"location":"azure/intro/#some-of-the-key-features-of-azure-devops-include","title":"Some of the key features of Azure DevOps include:","text":"<p>Azure Repos: This is a cloud-based version control system that allows teams to manage and share code securely across the organization. It supports both Git and Team Foundation Version Control (TFVC) repositories.</p> <p>Azure Boards: This is a project management tool that supports agile methodologies such as Scrum and Kanban. It allows teams to plan and track work, create backlogs, and manage sprints and iterations.</p> <p>Azure Pipelines: This is a continuous integration and delivery (CI/CD) tool that allows teams to build, test, and deploy applications to multiple platforms and environments, including cloud platforms like Azure and AWS.</p> <p>Azure Test Plans: This is a testing tool that enables teams to test applications across different platforms and devices. It includes features like exploratory testing, test case management, and automated testing.</p> <p>Azure Artifacts: This is a package management tool that allows teams to manage and share packages, such as code libraries and dependencies, across the organization.</p>"},{"location":"azure/intro/#azure-devops-basic-workflow-digaram","title":"Azure DevOps Basic Workflow Digaram","text":"<p>Conclusion</p> <p>Overall, Azure DevOps provides a comprehensive suite of tools that enables software development teams to collaborate effectively and deliver high-quality applications at a faster pace</p>"},{"location":"azure/playwright/","title":"Automation Testing using Playwright","text":"<p>Playwright is a popular open-source testing framework for web applications that allows developers to write end-to-end tests in a simple and concise manner. With Azure DevOps, you can automate your Playwright tests and integrate them into your CI/CD pipeline for continuous testing and deployment.</p> <p>Here are the steps to set up Playwright automation with Azure DevOps:</p> <ul> <li> <p>Create a new project in Azure DevOps: Navigate to the Azure DevOps portal and create a new project.</p> </li> <li> <p>Set up a build pipeline: In the project, create a new build pipeline and configure it to use the appropriate build agent. Then, add a task to install the necessary dependencies for running Playwright tests, such as Node.js and Playwright.</p> </li> </ul> <pre><code>npm init playwright@latest\n</code></pre> <ul> <li>Configure the Playwright test task: Add a new task to the build pipeline for running Playwright tests. This task should run the command to execute your Playwright tests, such as \"npx playwright test\" or \"npm test\". You can also configure this task to specify the browser and environment in which the tests should run.</li> </ul> <pre><code>npx playwright test\n\nnpm test\n</code></pre> <ul> <li> <p>Set up test reporting: To view the results of your Playwright tests in Azure DevOps, you can add a test reporting task to the build pipeline. This task should publish the test results to Azure DevOps so that you can view them in the test summary.</p> </li> <li> <p>Set up continuous testing: Once your build pipeline is set up, you can configure it to trigger automatically whenever changes are made to your code repository. This will enable continuous testing and allow you to catch any issues early in the development process.</p> </li> </ul> <p>By following these steps, you can set up Playwright automation with Azure DevOps and integrate it into your development workflow for continuous testing and deployment.</p> <p>Info</p> <p>You can automate some DevOps operations that lack automation or API support using PlayWright by recording and playing, however this is not a recommended procedure as it may fail if there are any changes to the UI or platform side.</p> Official Document <p>Please see the official document for further details.</p> <p>https://playwright.dev/docs/intro</p>"},{"location":"azure/serviceprinciple/","title":"How to Create Service Principles in\u00a0Azure","text":"<p>To use service principles while running Azure commands in Azure DevOps pipelines, you will need to first create a service principle and grant it the necessary permissions in Azure. Then, you can use the service principle's credentials to authenticate and authorize the Azure commands that you want to run in your Azure DevOps pipelines.</p> <p></p>"},{"location":"azure/serviceprinciple/#here-is-an-example-of-how-you-can-create-a-service-principle-and-use-it-in-an-azure-devops-pipeline","title":"Here is an example of how you can create a service principle and use it in an Azure DevOps pipeline:","text":"<ul> <li> <p>Sign in to the Azure portal and navigate to the Azure Active Directory page.</p> </li> <li> <p>Click on \"App registrations\" and then click on the \"New registration\" button.</p> </li> <li> <p>Give your service principle a name and select \"Accounts in this organizational directory only\" as the supported account type. Click on the \"Register\" button to create the service principle.</p> </li> <li> <p>Click on the service principle that you just created, and then click on the \"Certificates &amp; secrets\" tab.</p> </li> <li> <p>Click on the \"New client secret\" button, give the secret a description, and select an expiration time. Click on the \"Add\" button to create the secret.</p> </li> <li> <p>Copy the secret value, as you will need it later or Store it in a Key-Vault</p> </li> <li> <p>Navigate to the resource or resources that you want to grant the service principle access to.</p> </li> <li> <p>Click on the \"Access control (IAM)\" tab, and then click on the \"Add role assignment\" button.</p> </li> <li> <p>Select the role that you want to assign to the service principle, and then type in the name of the service principle in the \"Select\" field. Click on the \"Save\" button to assign the role to the service principle.</p> </li> <li> <p>To use the service principle in your Azure DevOps pipeline, you will need to pass the service principle's client ID and client secret as environment variables. Here is an example of how you can do this:</p> </li> <li> <p>In your Azure DevOps project, navigate to the \"Pipelines\" page and click on the pipeline that you want to edit.</p> </li> <li> <p>Click on the \"Variables\" tab, and then click on the \"Add\" button.</p> </li> <li> <p>Add two new variables with the names \"AZURE_CLIENT_ID\" and \"AZURE_CLIENT_SECRET\", and set the values to the service principle's client ID and client secret, respectively.</p> </li> <li> <p>In your pipeline tasks, use the service principle's client ID and client secret to authenticate and authorize the Azure commands that you want to run. For example, you can use the following command to authenticate using the service principle:</p> </li> </ul> <pre><code>az login\u200a-\u200aservice-principal -u $AZURE_CLIENT_ID -p $AZURE_CLIENT_SECRET\u200a-\u200atenant $TENANT_ID\n</code></pre> <p>Note</p> <ul> <li> <p>Replace TENANT_ID with the ID of your Azure Active Directory tenant.</p> </li> <li> <p>Don't forget to store AZURE_CLIENT_ID &amp; AZURE_CLIENT_SECRET in Key-Vault for Future Use.</p> </li> </ul>"},{"location":"azure/techchallenges/","title":"Azure DevOps Technical Challanges","text":"<p>Here are some common technical challenges that can arise when using Azure DevOps:</p> ChallengesTips to Overcome <p>Integration challenges: Integrating Azure DevOps with other tools in your tech stack can be challenging. This includes integrating with other Azure services, as well as non-Azure tools like GitHub or Jira.</p> <p>Pipeline complexity: Azure DevOps provides powerful tools for building and deploying pipelines, but creating complex pipelines can be challenging. For example, setting up continuous delivery with multiple environments can require configuring many moving parts.</p> <p>Security and compliance: Ensuring security and compliance across your pipelines and repositories can be challenging. Azure DevOps provides security features like RBAC, but implementing and enforcing security policies can require careful planning and configuration.</p> <p>Release management: Managing the release of software to multiple environments can be challenging. This includes configuring release gates and approvals, rolling back releases, and tracking deployments across environments.</p> <p>Performance and scalability: As your pipeline and repository size grows, you may encounter performance and scalability issues. This can include slow build times, long queue times, and issues with concurrent builds.</p> <p>Version control: Using Git for version control in Azure DevOps can be challenging for teams not familiar with Git. It requires a different workflow than centralized version control systems, and can be difficult to manage if multiple teams are working on the same codebase.</p> <p>Continuous testing: Setting up and maintaining continuous testing with Azure DevOps can be challenging. This includes configuring test plans, integrating with test automation tools, and ensuring that tests run reliably and accurately. </p> <p>Integration challenges: To overcome integration challenges, it's important to understand the capabilities of Azure DevOps and the tools you want to integrate with. Azure DevOps provides a range of APIs and extensions that can help with integration. It's also important to plan integration in advance and test it thoroughly before deployment.</p> <p>Pipeline complexity: To simplify complex pipelines, it's important to break them down into smaller, more manageable pieces. Use templates and variables to reduce duplication and improve consistency. It's also important to test pipelines regularly and optimize for speed and reliability.</p> <p>Security and compliance: To ensure security and compliance, it's important to understand the security features of Azure DevOps and how to configure them properly. You should also establish clear security policies and ensure that all users are aware of them. Regular audits and monitoring can also help ensure compliance.</p> <p>Release management: To manage releases effectively, it's important to establish clear release policies and automate as much of the release process as possible. Use release gates and approvals to control the release process and ensure that releases are tested thoroughly before deployment. Monitor releases closely and be prepared to roll back if necessary.</p> <p>Performance and scalability: To improve performance and scalability, it's important to optimize pipelines for speed and reduce queue times. This can involve using parallelism, caching, and distributed builds. It's also important to monitor performance regularly and scale resources as necessary.</p> <p>Version control: To overcome version control challenges, it's important to establish clear version control policies and train all users in the use of Git. Use branching and merging effectively to manage code changes, and ensure that code reviews are conducted regularly to maintain code quality.</p> <p>Continuous testing: To set up and maintain continuous testing, it's important to establish clear testing policies and automate as much of the testing process as possible. Use test plans and test suites to organize and manage tests, and integrate with test automation tools for better efficiency. Monitor test results closely and be prepared to adjust your testing strategy as necessary. </p>"},{"location":"azure/techchallenges/#skipping-ci-for-git-push","title":"Skipping CI for Git Push","text":"<p>Moreover, you may instruct Azure Pipelines to forego starting a pipeline that a push would typically start. To prevent Azure Pipelines from performing CI for this push, just put [skip ci] in the message or description of any of the commits that are a part of a push. Any of the following modifications are also acceptable.</p> <ul> <li> <p>[skip ci] or [ci skip] </p> </li> <li> <p><code>***NO_CI***</code></p> </li> </ul> <p>Example</p> <p>git commit -m \" Commit Message ***No_CI*** \"</p>"},{"location":"azure/techchallenges/#syncs-changes-back-to-the-same-branch-in-azure-devops","title":"syncs changes back to the same branch in Azure DevOps:","text":"<p>The script provided below is a PowerShell script that is used in Azure DevOps to sync changes back to the same branch in the code repository. This script assumes that you have already cloned the repository in the pipeline and are working in the same branch.</p> Pre-Requsites <p>Enable OAuth Token in Agent: The agent running the script should have an OAuth token enabled to access the code repository. This can be done by following the steps in the Azure DevOps documentation.</p> <p>Now let's take a closer look at the script and what each line does:</p> <p>git pull origin $(Build.SourceBranch):  This command pulls the latest changes from the source branch into the local repository.</p> <p>git checkout $(Build.SourceBranch):  This command switches to the source branch.</p> <p>git commit -m \"Syncing updates with the code repository NO_CI\":  This  command commits the changes made to the repository and adds a commit message. The NO_CI flag in the commit message disables the continuous integration (CI) trigger for subsequent commits. This means that if there are further changes made to the code repository, the pipeline won't trigger a build, preventing the pipeline from running in a loop.</p> <p>git push origin HEAD:$(Build.SourceBranch):  This command pushes the changes to the same branch in the remote repository.</p> <p>The next block of code is an if statement that checks if the previous command executed successfully:</p> <p><pre><code>if ($?) {Write-Host \"Syncing Changes Back to Repo\"}\nelse {Write-Host \"Pushing Changes Anyway\"\ngit push -f origin HEAD:$(Build.SourceBranch)}\n</code></pre> If the previous command was successful, it displays \"Syncing Changes Back to Repo\". If the command failed, it displays \"Pushing Changes Anyway\" and then forces a push to the same branch in the remote repository.</p> <p>Overall, this script automates the process of syncing changes back to the same branch in a code repository using Azure DevOps.</p> PowerShellShell <pre><code>git pull origin $(Build.SourceBranch)\ngit checkout $(Build.SourceBranch)\ngit merge %sourceBranch% -m \"Merge to $(Build.SourceBranch)\"\ngit config --global user.email \"$(Build.RequestedForEmail)\"\ngit config --global user.name \"$(Build.RequestedFor)\"\ngit add .\ngit status\ngit commit -m \"Syncing updates with the code repository ***NO_CI***\"\ngit push origin HEAD:$(Build.SourceBranch)\n\nif ($?) {\nWrite-Host \"Syncing Changes Back to Repo\"\n} else {\nWrite-Host \"Pushing Changes Anyway\"\ngit push -f origin HEAD:$(Build.SourceBranch)\n}\n</code></pre> <pre><code>git pull origin $(Build.SourceBranch)\ngit checkout $(Build.SourceBranch)\ngit merge %sourceBranch% -m \"Merge to $(Build.SourceBranch)\"\ngit config --global user.email \"$(Build.RequestedForEmail)\"\ngit config --global user.name \"$(Build.RequestedFor)\"\ngit add .\ngit status\ngit commit -m \"Syncing updates with the code repository ***NO_CI***\"\ngit push origin HEAD:$(Build.SourceBranch)\n\nif [ $? -eq 0 ]\nthen\necho \"Syncing Changes Back to Repo\"\nelse\necho \"Pulling before pushing Changes Back to Repo\"\ngit pull origin $BUILD_SOURCEBRANCH\ngit push origin HEAD:$BUILD_SOURCEBRANCH\nfi\n</code></pre>"},{"location":"azure/techchallenges/#pre-merge-validation-from-variable-source-branches-to-fixed-target-branch","title":"Pre-Merge Validation from Variable Source Branches to Fixed Target Branch","text":"<p>When there is a pull request from a variable feature branch, such as <code>Feature-A</code>, <code>Feature-B</code>, or <code>Feature-C</code> to the <code>Development</code> branch, we must determine whether the code in the feature branch is buildable or not. When the feature branch is good, we must only merge it with the development branch. However, in Azure Classic pipelines, we are unable to automatically change the source branch name. The script below will get the variable feature branch name whenever there is a PR and\u00a0 it will run the Build Validation pipeline by changing the branch name.</p> Pre-Merge Validation from Feature-Dev Branch <pre><code>echo \"Login to Azure \"\n\naz login -u $(username) -p $(password)\n\n\necho \"Fetching Current Feature Branch \"\n\n$PR = az repos pr list --repository $(Repo) --target-branch $(targetbranch) --organization $(URL) --project $(project) | grep sourceRefName | awk '{print $3}'\n$Branchname = $PR.Split(\"/\")[2] | sed 's/\"\"/\\ /g' | sed 's/,/\\ /g' | sed -e 's/^[ \\t]*//' | Foreach {$_.TrimEnd()}\n\n\necho \"The Current PR Received from Branch: $Branchname \"\ngit switch $Branchname\n\necho \"Commands to Execute\"\n\nnpm i #Example Commands\nnpm run build #Example Commands\n</code></pre>"},{"location":"devops/ansible/","title":"Ansible","text":"<p>Ansible is an open-source IT Configuration Management, Deployment &amp; Orchestration tool. It aims to provide large productivity gains to a wide variety of automation challenges. This tool is very simple to use yet powerful enough to automate complex multi-tier IT application environments.</p>"},{"location":"devops/ansible/#installing-ansible","title":"Installing Ansible","text":"UbuntuCentOS <p><pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt install software-properties-common -y\n</code></pre> <pre><code>sudo add-apt-repository --yes --update ppa:ansible/ansible\n</code></pre> <pre><code>sudo apt install ansible -y\n</code></pre> <pre><code>ansible --version\n</code></pre></p> <p><pre><code>sudo yum install epel-release -y\n</code></pre> <pre><code>sudo yum install ansible -y\n</code></pre> <pre><code>ansible --version\n</code></pre></p>"},{"location":"devops/ansible/#scp-commands","title":"SCP Commands","text":"<p>To Copy files From Localmachine to Remote machine <pre><code>scp -i &lt;private-key-file-path&gt; &lt;files to copy&gt; &lt;user&gt;@&lt;IP&gt;:&lt;Remote-machine-path&gt;\n</code></pre></p> <p>Example</p> <p>scp -i Downloads/control.pem sample.txt ubuntu@54.165.128.104:/home/ubuntu/</p> <p>To Copy Files from Remotemachine to Local machine <pre><code>scp -i &lt;private-key-file-path&gt; &lt;user&gt;@&lt;IP&gt;:&lt;remote-files-path&gt; &lt;Localmachine-path&gt;\n</code></pre></p> <p>Example</p> <p>scp -i Downloads/control.pem ubuntu@54.165.128.104:/home/ubuntu/sample.txt Desktop/</p>"},{"location":"devops/ansible/#sample-inventory-file","title":"Sample Inventory File","text":"Inventory <pre><code>##Host Level\n\nwebserver01    ansible_host=&lt;Private IP&gt;\nwebserver02    ansible_host=&lt;Private IP&gt;\nwebserver03    ansible_host=&lt;Private IP&gt;      dbserver01     ansible_host=&lt;Private IP&gt;\ndbserver02     ansible_host=&lt;Private IP&gt;      ansible_user=ubuntu\n\n##Group Level \n\n[Group1]\nwebserverserver01\nwebserverserver02\nwebserverserver03\n\n[Group2]\ndbserver01\ndbserver02\n\n##Parent Level\n\n[dc_mumbai:children] webservergrp\ndbsrvgrp\n\n##Variables\n\n[dc_mumbai:vars]\nansible_user=&lt;user&gt;\nansible_ssh_private_key_file=&lt;key-path&gt;\n</code></pre> <p>Info</p> <p>Host level has the highest priority, If you mention anything like username or Keyfile etc. It will take only, which are mentioned at the host level.</p>"},{"location":"devops/ansible/#ansible-commands","title":"Ansible Commands","text":"<p>To test the connection of a particular Remote Machine <pre><code>ansible -i &lt;Inventoryfile path&gt; -m ping &lt;hostname&gt;\n</code></pre> To test the connection of a particular Group of Remote Machines <pre><code>ansible -i &lt;Inventoryfile path&gt; -m ping &lt;Groupname&gt;\n</code></pre> To test the connection of All Remote Machine <pre><code>ansible -i &lt;Inventoryfile path&gt; -m ping all\n</code></pre> To see details about the machine  <pre><code>ansible -i &lt;Inventoryfile path&gt; -m setup &lt;hostname&gt;\n</code></pre></p>"},{"location":"devops/ansible/#some-example-ad-hoc-commands","title":"Some Example Ad hoc Commands","text":"Ad hoc Commands <p>Copy files to Remote machines name start with web <pre><code>ansible -i &lt;Inventoryfile path&gt; -m copy -a \"src=index.html dest=/var/www/html/index.html\" 'web*' --become\n</code></pre> Installing httpd in centos Remote machine  <pre><code>ansible -i &lt;Inventoryfile path&gt; -m yum -a \"name=httpd state=present\" websrvgrp --become\n</code></pre> Start &amp; Enable httpd in centos Remote machine <pre><code>ansible -i &lt;Inventoryfile path&gt; -m service -a \"name=httpd state=started enabled=yes\" websrvgrp --become\n</code></pre></p> <p>Info</p> <p>Ansible Playbooks should be with .yml or .yaml Extension for example vim sample.yml</p>"},{"location":"devops/ansible/#playbook-for-creating-files-directories","title":"Playbook For Creating Files &amp; Directories","text":"Sample Ansible Playbooks <p><pre><code>- name: Creating Files &amp; Directories\nhosts: &lt;host&gt;\nbecome: yes\ntasks:\n- name: Creating a Directory\nfile:\npath: /tmp/welcome\nstate: directory\n\n- name: Creating a File\nfile:\npath: /tmp/sample.txt\nstate: touch\n</code></pre> To Execute the playbook <pre><code>ansible-playbook -i &lt;Inventory file path&gt; sample.yml\n</code></pre></p>"},{"location":"devops/ansible/#writing-playbook-for-installing-httpd-service-in-remote-machines-with-start-and-enable-and-copying-indexhtml-files-from-local-machine-to-the-remote-machine","title":"Writing Playbook For Installing Httpd service in remote machines with start and enable and copying index.html files from local machine to the remote machine","text":"<pre><code>- name: Install httpd and start the service\nhosts: all\ntasks:\n- name: Installing the Apache package\nyum:\nname: httpd\nstate: present\n\n\n- name: Starting service\nservice:\nname: httpd\nstate: started\nenabled: yes\n\n- name: Copy file with owner and permissions\ncopy:\nsrc: ./index.html\ndest: /var/www/html/index.html\n\n- name: Restarting service\nservice:\nname: httpd\nstate: restarted\n</code></pre>"},{"location":"devops/ansible/#writing-playbook-for-setting-up-website-in-remote-machine","title":"Writing Playbook for Setting Up Website in Remote Machine","text":"<pre><code>- name: Setting up Website\nhosts: websrv\ngather_facts: False\nbecome: True\n\ntasks:\n- name: Installing Packages in CentOS\nyum:\nname: \"{{item}}\"\nstate: present\nwhen: ansible_distribution == \"CentOS\"\nloop:\n- httpd\n- wget\n- unzip\n\n- name: Start &amp; Enable httpd\nservice:\nname: httpd\nstate: started\nenabled: yes   - name: Downloading Source code\nget_url:\nurl: https://www.tooplate.com/zip-templates/2114_pixie.zip\ndest: /opt\n\n- name: Unarchive a file that is already on the remote machine\nunarchive:\nsrc: /opt/2114_pixie.zip\ndest: /opt\nremote_src: yes\n\n- name : Deploy Website\ncopy:\nsrc: /opt/2114_pixie/\ndest: /var/www/html/\nremote_src: yes\n\n\n- name: Restarting httpd service\nservice:\nname: httpd\nstate: restarted\n</code></pre>"},{"location":"devops/ansible/#writing-playbook-for-setting-up-website-in-remote-machine-with-conditions-handlers","title":"Writing Playbook for Setting Up Website in Remote Machine with Conditions &amp; Handlers.","text":"<pre><code>- name: Writing playbook for loops and conditions\nhosts: all\ntasks:\n- name: Install packages on centos\nyum:\nname: \"{{item}}\"\nstate: present\nwhen: ansible_distribution == \"CentOS\"\nloop:\n- httpd\n- wget\n- unzip\n- zip\n- git           - name: Install packages on Ubuntu apt:\nname: \"{{item}}\"\nstate: present\nupdate_cache: yes\nwhen: ansible_distribution == \"Ubuntu\"\nloop:\n- apache2\n- wget\n- unzip\n- zip\n- git\n\n\n- name: Start &amp; enable service on CentOS\nservice:\nname: httpd\nstate: started\nenabled: yes\nwhen: ansible_distribution == \"CentOS\"\n\n- name: Start &amp; enable service on Ubuntu\nservice:\nname: apache2\nstate: started\nenabled: yes\nwhen: ansible_distribution == \"Ubuntu\"\n\n\n- name: Push index.html on centos\ncopy:\nsrc: index.html\ndest: /var/www/html/\nbackup: yes\nwhen: ansible_distribution == \"CentOS\"\nnotify:\n- Restart service on CentOS\n\n- name: Push index.html on ubuntu\ncopy:\nsrc: index.html\ndest: /var/www/html/\nbackup: yes\nwhen: ansible_distribution == \"Ubuntu\"\nnotify:\n- Restart service on Ubuntu\n\nhandlers:\n- name: Restart service on CentOS\nservice:\nname: httpd\nstate: restarted\nenabled: yes\nwhen: ansible_distribution == \"CentOS\"\n\n- name: Restart service on Ubuntu\nservice:\nname: apache2\nstate: restarted\nenabled: yes\nwhen: ansible_distribution == \"Ubuntu\"\n</code></pre>"},{"location":"devops/ansible/#writing-playbook-to-create-vpc-in-aws-cloud-and-including-variables-from-different-file","title":"Writing Playbook to Create VPC in AWS Cloud and Including Variables from different File","text":"Requirements <ul> <li> <p>python &gt;= 3.6</p> </li> <li> <p>boto3 &gt;= 1.15.0</p> </li> <li> <p>botocore &gt;= 1.18.0</p> </li> </ul> <p><pre><code>apt install python3-pip\n</code></pre> <pre><code>pip install boto\n</code></pre> <pre><code>pip install boto3\n</code></pre> <pre><code>pip install botocore\n</code></pre> Creating File to store Variables <pre><code>vim vpc_setup.txt\n</code></pre> <pre><code>vpc_name: \"Vprofile-vpc\"\n\n#Vpc-range\nvpcrange: '172.21.0.0/16'\n\n#subnet range\npubip1: '172.21.1.0/24'\npubip2: '172.21.2.0/24'\npubip3: '172.21.3.0/24'\npvtip1: '172.21.4.0/24'\npvtip2: '172.21.5.0/24'\npvtip3: '172.21.6.0/24'\n\n#region\nregion: 'us-east-2'\n\n#zone names\nzone1: us-east-2a\nzone2: us-east-2b\nzone3: us-east-2c\n\n\nstate: present\n</code></pre></p> Playbook <pre><code>- hosts: localhost\nconnection: local\ngather_facts: False\ntasks:\n- name: Import vpc variables\ninclude_vars: /path/vpc_setup\n\n- name: create vpc\nec2_vpc_net:\nname: \"{{vpc_name}}\"\ncidr_block: \"{{vpcrange}}\"\nregion: \"{{region}}\"\ndns_support: yes\ndns_hostnames: yes\ntenancy: default\nstate: \"{{state}}\"\nregister: vpcout\n\n\n- name: Create a public subnet for zone1\nec2_vpc_subnet:\nvpc_id: \"{{vpcout.vpc.id}}\"\nregion: \"{{region}}\"\naz: \"{{zone1}}\"\nstate: \"{{state}}\"\ncidr: \"{{pubip1}}\"\nmap_public: yes\ntags:\nName: vprofile_pubsub1\nregister: pubsub1_out\n\n- name: Create a public subnet for zone2\nec2_vpc_subnet:\nvpc_id: \"{{vpcout.vpc.id}}\"\nregion: \"{{region}}\"\naz: \"{{zone2}}\"\nstate: \"{{state}}\"\ncidr: \"{{pubip2}}\"\nmap_public: yes\ntags:\nName: vprofile_pubsub2\nregister: pubsub2_out\n\n- name: Create a public subnet for zone3\nec2_vpc_subnet:\nvpc_id: \"{{vpcout.vpc.id}}\"\nregion: \"{{region}}\"\naz: \"{{zone3}}\"\nstate: \"{{state}}\"\ncidr: \"{{pubip3}}\"\nmap_public: yes\ntags:\nName: vprofile_pubsub3\nregister: pubsub3_out\n\n- name: Create a private subnet for zone1\nec2_vpc_subnet:\nvpc_id: \"{{vpcout.vpc.id}}\"\nregion: \"{{region}}\"\naz: \"{{zone1}}\"\nstate: \"{{state}}\"\ncidr: \"{{pvtip1}}\"\nmap_public: yes\ntags:\nName: vprofile_pvtsub1\nregister: pvtsub1_out\n\n- name: Create a private subnet for zone2\nec2_vpc_subnet:\nvpc_id: \"{{vpcout.vpc.id}}\"\nregion: \"{{region}}\"\naz: \"{{zone2}}\"\nstate: \"{{state}}\"\ncidr: \"{{pvtip2}}\"\nmap_public: yes\ntags:\nName: vprofile_pvtsub2\nregister: pvtsub2_out\n\n- name: Create a private subnet for zone3\nec2_vpc_subnet:\nvpc_id: \"{{vpcout.vpc.id}}\"\nregion: \"{{region}}\"\naz: \"{{zone3}}\"\nstate: \"{{state}}\"\ncidr: \"{{pvtip3}}\"\nmap_public: yes\ntags:\nName: vprofile_pvtsub3\nregister: pvtsub3_out\n\n- name: Internet gateway setup\nec2_vpc_igw:\nvpc_id: \"{{vpcout.vpc.id}}\"\nregion: \"{{region}}\"\nstate: \"{{state}}\"\ntags:\nName: vprofile_IGW\nregister: igw_out\n\n\n\n- name: public subnet route table\nec2_vpc_route_table:\nvpc_id: \"{{vpcout.vpc.id}}\"\nregion: \"{{region}}\"\ntags:\nName: vprofile_Public\nsubnets:\n- \"{{pubsub1_out.subnet.id}}\"\n- \"{{pubsub2_out.subnet.id}}\"\n- \"{{pubsub3_out.subnet.id}}\"\nroutes:\n- dest: 0.0.0.0/0\ngateway_id: \"{{ igw_out.gateway_id }}\"\nregister: public_route_table\n\n\n- name: Create a new nat gateway and allocate a new EIP if a nat gateway does not yet exist in the subnet.\nec2_vpc_nat_gateway:\nstate: \"{{state}}\"\nsubnet_id: \"{{pubsub1_out.subnet.id}}\"\nwait: true\nregion: \"{{region}}\"\nif_exist_do_not_create: true\nregister: nat_out\n\n- name: private subnet route table\nec2_vpc_route_table:\nvpc_id: \"{{vpcout.vpc.id}}\"\nregion: \"{{region}}\"\ntags:\nName: vprofile_Private\nsubnets:\n- \"{{pvtsub1_out.subnet.id}}\"\n- \"{{pvtsub2_out.subnet.id}}\"\n- \"{{pvtsub3_out.subnet.id}}\"\nroutes:\n- dest: 0.0.0.0/0\ngateway_id: \"{{nat_out.nat_gateway_id}}\"\nregister: private_route_table\n\n\n- debug:\nvar: \"{{item}}\"\nloop:\n- vpcout.vpc.id\n- pubsub1_out.subnet.id\n- pubsub2_out.subnet.id\n- pubsub3_out.subnet.id\n- pvtsub1_out.subnet.id\n- pvtsub2_out.subnet.id\n- pvtsub3_out.subnet.id\n- igw_out.gateway_id\n- public_route_table.route_table.id\n- nat_out.nat_gateway_id\n- private_route_table.route_table.id\n\n- set_fact:\nvpcid: \"{{vpcout.vpc.id}}\"\npubsublid: \"{{ pubsub1_out.subnet.id }}\"\npubsub2id: \"{{ pubsub2_out.subnet.id }}\"\npubsub3id: \"{{ pubsub3_out.subnet.id }}\"\nprivsublid: \"{{ pvtsub1_out.subnet.id }}\"\nprivsub2id: \"{{ pvtsub2_out.subnet.id }}\"\nprivsub3id: \"{{ pvtsub3_out.subnet.id }}\"\nigwid: \"{{ igw_out.gateway_id }}\"\npubRTid: \"{{ public_route_table.route_table.id }}\"\nNATGWid: \"{{ nat_out.nat_gateway_id }}\"\nprivRTid: \"{{ private_route_table.route_table.id }}\"\ncacheable: yes\n\n- name: creating file for vpc output\ncopy:\ncontent: \"vpcid: {{vpcout.vpc.id}}\\n pubsublid: {{ pubsub1_out.subnet.id }}\\npubsub2id: {{ pubsub2_out.subnet.id }}\\npubsub3id: {{ pubsub3_out.subnet.id }}\\nprivsublid: {{ pvtsub1_out.subnet.id }}\\nprivsub2id: {{ pvtsub2_out.subnet.id }}\\nprivsub3id: {{ pvtsub3_out.subnet.id }}\\nigwid: {{ igw_out.gateway_id }}\\npubRTid: {{ public_route_table.route_table.id }}\\nNATGWid: {{ nat_out.nat_gateway_id }}\\nprivRTid: {{ private_route_table.route_table.id }}\"\ndest: /home/ubuntu/Vprofile/vars/output_vars\n</code></pre>"},{"location":"devops/ansible/#launching-ec2-instance-in-aws-cloud","title":"Launching Ec2 Instance in AWS Cloud","text":"Requirements <ul> <li> <p>python &gt;= 3.6</p> </li> <li> <p>boto3 &gt;= 1.15.0</p> </li> <li> <p>botocore &gt;= 1.18.0</p> </li> </ul> <p><pre><code>apt install python3-pip\n</code></pre> <pre><code>pip install boto\n</code></pre> <pre><code>pip install boto3\n</code></pre> <pre><code>pip install botocore\n</code></pre></p> Launching Instance Playbook <pre><code>- name: Launching Ec2 Instance\nhosts: localhost\nconnection: local\ntasks:\n- name: Creating Key pair\namazon.aws.ec2_key:\nname: samplekey\nregion: us-west-1\nregister: key\n\n- debug:\nvar: key\n\n- name: Storing private key into a file\ncopy:\ncontent: \"{{key.key.private_key}}\"\ndest: \"./sample.pem\"\nmode: 0600\nwhen: key.changed\n\n\n- name: Creating Security Group\namazon.aws.ec2_group:\nname: mysg\ndescription: Allowing 22 and 80\nvpc_id: vpc-0c8e70cf05b1342ac\nregion: us-west-1\nrules:\n- proto: tcp\nfrom_port: 22\nto_port: 22\ncidr_ip: 0.0.0.0/0\nrule_desc: allow all on port 80 &amp; 22\nregister: sg_out\n\n- name: Launching bastion_host\nec2:\nkey_name: \"samplekey\"\nregion: us-west-1\ninstance_type: t2.micro\nimage: ami-0573b70afecda915d\nwait: yes\nwait_timeout: 300\ninstance_tags:\nname: \"Ansible Instance\"\nproject: vprofile\nowner: devops team\nexact_count: 1\ncount_tag:\nname: \"Ansible Instance\"\nproject: vprofile\nowner: devops team\ngroup_id: \"{{sg_out.group_id}}\"\nvpc_subnet_id: subnet-0c4734c845e549cda\nregister: instance\n</code></pre>"},{"location":"devops/ansible/#ansible-configuration-file","title":"Ansible Configuration File","text":"<p>Note</p> <p>You should save the configuration file with name ansible.cfg</p> <pre><code>vim ansible.cfg\n</code></pre> Ansible Configuration File <pre><code>[defaults]\nhost_key_checking=False\ninventory=&lt;Inventory File Path&gt;\ntimeout=20\nlog_path=/var/log/ansible_world.log\nremote_port=22\nremote_user=&lt;username&gt;\n\n[privilege_escalation]\nbecome=True\nbecome_method=sudo\nbecome_user=root\nbecome_ask_pass=False\n</code></pre>"},{"location":"devops/aws-cli/","title":"AWS","text":"<p>Amazon Web Services (AWS) is the world\u2019s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally.</p>"},{"location":"devops/aws-cli/#aws-cli-command-line-interface","title":"AWS CLI (Command Line Interface)","text":""},{"location":"devops/aws-cli/#setting-up-iam-user","title":"Setting up IAM user","text":""},{"location":"devops/aws-cli/#creating-user-with-access-key","title":"Creating User with Access-key","text":""},{"location":"devops/aws-cli/#set-permissions-attaching-policies","title":"Set permissions &amp; Attaching Policies","text":""},{"location":"devops/aws-cli/#installing-aws-cli","title":"Installing AWS-CLI","text":"<pre><code>sudo -i\n</code></pre> <p><pre><code>sudo apt update\n</code></pre> <pre><code>apt install awscli -y\n</code></pre> </p>"},{"location":"devops/aws-cli/#configure-aws-cli-with-iam-user-credentials-with-a-specific-region","title":"Configure AWS CLI with IAM user Credentials with a specific Region","text":"<p><pre><code>aws configure\n</code></pre>  Once it is done try some aws cli commands like aws s3 ls If u have any buckets in your s3 it will list</p>"},{"location":"devops/aws-cli/#ec2-elastic-compute-cloud","title":"EC2 \u2013 Elastic Compute Cloud","text":""},{"location":"devops/aws-cli/#create-a-key-pair","title":"Create a key pair","text":"<p><pre><code>aws ec2 create-key-pair --key-name &lt;keypair-Name&gt; --query 'KeyMaterial' --output text &gt; &lt;keypair-Name.pem&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-a-key-pair","title":"Delete a key pair","text":"<p>To delete a key pair, run the aws ec2 delete-key-pair command, substituting MyKeyPair with the name of the pair to delete. <pre><code>aws ec2 delete-key-pair --key-name &lt;keypair-Name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-a-security-group-adding-inbound-rules","title":"Create a Security Group &amp; Adding Inbound rules","text":"<p><pre><code>aws ec2 create-security-group --group-name &lt;security grp Name&gt; --description \"&lt;Description&gt;\"\n</code></pre> <pre><code>curl https://checkip.amazonaws.com\n</code></pre> <pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;security group Id&gt; --protocol tcp --port &lt;port Number&gt; --cidr &lt;ip address&gt;\n</code></pre> <pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;security grp Id&gt;--protocol tcp --port 22-8000 --cidr 0.0.0.0/0 </code></pre>  To view the initial information for my-sg, run the aws ec2 describe-security-groups command. For an EC2-Classic security group, you can reference it by its name. <pre><code>aws ec2 describe-security-groups --group-names &lt;security grp Name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-your-security-group","title":"Delete your security group","text":"<p>The following command example deletes the EC2-Classic security group named. <pre><code>aws ec2 delete-security-group --group-name &lt;security grp Name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#launch-instance","title":"Launch Instance","text":"<p>You can use the following command to launch a t2.micro instance in EC2-Classic. Replace the italicized parameter values with your own. You can get the AMI IDs from documentation or console for your required Instance. <pre><code> aws ec2 run-instances --image-id &lt;ami-Id&gt; --count 1 --instance-type &lt;type&gt; --key-name &lt;keypair-Name&gt; --security-groups &lt;security grp Name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#add-a-tag-to-your-instance","title":"Add a tag to your Instance","text":"<p><pre><code>aws ec2 create-tags --resources &lt;Instance-Id&gt;--tags Key=Name,Value=&lt;value&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#terminate-your-instance","title":"Terminate your Instance","text":"<p>To delete an instance, you use the command aws ec2 terminate-instances to delete it. <pre><code>aws ec2 terminate-instances --instance-ids &lt;Instance-Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-launch-template","title":"Create Launch Template","text":"<p><pre><code>aws ec2 create-launch-template --launch-template-name &lt;Name&gt;\":[{\"AssociatePublicIpAddress\":true,\"DeviceIndex\":0,\"Ipv6AddressCount\":1,\"SubnetId\":\"pe\":\"&lt;Instance type\",\"TagSpecifications\":[{\"ResourceType\":\"instance\",\" Tags\":[{\"Key\":\"Name\",\"Value\":\"&lt;value&gt;\"}]}]}'\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-launch-template","title":"Delete Launch Template","text":"<p><pre><code>aws ec2 delete-launch-template --launch-template-id &lt; template id&gt;  --region &lt;region&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#creating-auto-scaling-group","title":"Creating Auto-Scaling group","text":"<p><pre><code>aws autoscaling create-auto-scaling-group --auto-scaling-group-name &lt;Name&gt;  --launch-LaunchTemplateId=&lt;template \u2013 id &gt; --min-size 2 --max-size 5 --vpc-zone-identifier \"subnet1-id,subnet2-id,subnet3-id\"\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-your-auto-scaling-group","title":"Delete your Auto-Scaling Group","text":"<pre><code>aws autoscaling delete-auto-scaling-group --auto-scaling-group-name &lt; Auto -Scaling group Name &gt;\n</code></pre>"},{"location":"devops/aws-cli/#ebs-elastic-block-storage","title":"EBS \u2013 Elastic Block Storage","text":""},{"location":"devops/aws-cli/#create-ebs-volume","title":"Create EBS Volume","text":"<p>To create an empty General Purpose SSD (gp2) volume <pre><code>aws ec2 create-volume --volume-type &lt;volume type&gt; --size &lt;size in number&gt; --availability-zone &lt;zone&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-create-an-encrypted-volume","title":"To create an encrypted volume","text":"<p><pre><code>aws ec2 create-volume --volume-type &lt;volume type&gt; --size &lt;size in number&gt;  --encrypted --availability-zone &lt;zone&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-create-a-volume-with-tags","title":"To create a volume with tags","text":"<p><pre><code>aws ec2 create-tags --resources &lt;volume-id&gt; --tags Key=Name,Value=&lt;value&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-delete-a-volume","title":"To Delete a Volume","text":"<pre><code>aws ec2 delete-volume --volume-id &lt;volume Id&gt;\n</code></pre> <p>Output</p> <p>Output: None</p>"},{"location":"devops/aws-cli/#to-create-a-snapshot","title":"To create a snapshot","text":"<p>This example command creates a snapshot of the volume with a volume ID of  and a short description to identify the snapshot. <pre><code>aws ec2 create-snapshot --volume-id &lt;volume Id&gt; --description \"&lt;Description&gt;\"\n</code></pre>"},{"location":"devops/aws-cli/#to-create-a-snapshot-with-tags","title":"To create a snapshot with tags","text":"<p><pre><code>aws ec2 create-snapshot --volume-id &lt;volume Id&gt; --description 'Prod backup' --tag-specifications 'ResourceType=snapshot,Tags=[{Key=Name,Value=&lt;value&gt;},{Key=Database,Value=Mysql}]'\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-allocate-an-elastic-ip-address-for-ec2-classic","title":"To allocate an Elastic IP address for EC2-Classic","text":"<p>The following allocate-address example allocates an Elastic IP address to use with an instance in EC2-Classic. <pre><code>aws ec2 allocate-address\n</code></pre> </p>"},{"location":"devops/aws-cli/#elb-elastic-load-balancer","title":"ELB \u2013 Elastic Load Balancer","text":""},{"location":"devops/aws-cli/#create-load-balancer","title":"Create-load-balancer","text":""},{"location":"devops/aws-cli/#to-create-an-application-load-balancer","title":"To create an Application load balancer","text":"<p>The below commands to find subnet id &amp; Instance Id  <pre><code>aws ec2 describe-subnets\n</code></pre> <pre><code>aws ec2 describe-instances\n</code></pre> <pre><code>aws elbv2 create-load-balancer --name &lt;Load balancer Name&gt;--type &lt;type&gt; --subnets &lt;subnet-Id&gt; &lt;subnet-Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-create-a-network-load-balancer","title":"To create a Network load balancer","text":"<p><pre><code>aws elbv2 create-load-balancer --name &lt;Load balancer Name&gt;--type &lt;type&gt; --subnets &lt;subnet-Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-register-instances-with-a-load-balancer","title":"To register instances with a load balancer","text":"<pre><code>aws elb register-instances-with-load-balancer --load-balancer-name &lt;Load balancer Name&gt; --instances &lt;Instance-Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#to-delete-a-specific-load-balancer","title":"To Delete a Specific Load balancer","text":"<p><pre><code>aws elbv2 delete-load-balancer --load-balancer-arn &lt;arn end point&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#rds-relational-database-service","title":"RDS - Relational Database Service","text":""},{"location":"devops/aws-cli/#create-db-instance","title":"Create-db-Instance","text":"<p><pre><code> aws rds create-db-instance --db-instance-identifier &lt;db - Name&gt; --db-instance-class &lt;db.type&gt; --engine &lt;Database Engine&gt;  --master-username &lt;username&gt; --master-user-password &lt;password&gt; --allocated-storage &lt;storage in numbers&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-delete-your-db-instance","title":"To delete your db-Instance","text":"<p><pre><code>aws rds delete-db-instance --db-instance-identifier &lt;db - Name&gt; --final-db-snapshot-identifier &lt;db - Name&gt;-final-snap\n</code></pre> </p>"},{"location":"devops/aws-cli/#s3-simple-storage-service","title":"S3 \u2013 Simple Storage Service","text":""},{"location":"devops/aws-cli/#list-buckets-objects","title":"List Buckets &amp; Objects","text":"<p>To list your buckets, folders, or objects, use the s3 ls command. Using the command without a target or options lists all buckets. <pre><code>aws s3 ls\n</code></pre> <pre><code>aws s3 ls s3://&lt;bucket name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-a-bucket","title":"Create a bucket","text":"<p>Use the s3 mb command to make a bucket. Bucket names must be globally unique (unique across all of Amazon S3) and should be DNS compliant. <pre><code>aws s3 mb s3:// &lt;bucket name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#copy-objects","title":"Copy objects","text":"<p>Use the s3 cp command to copy objects from a bucket or a local directory <pre><code>aws s3 cp &lt;file&gt; s3:// &lt;bucket name&gt;\n</code></pre> <pre><code>aws s3 cp s3://&lt;source bucket/file&gt; s3://&lt;destination-bucket&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#move-objects","title":"Move objects","text":"<p>Use the s3 mv command to move objects from a bucket or a local directory. <pre><code>aws s3 mv &lt;local file&gt; s3:// &lt;bucket name&gt; </code></pre> <pre><code>aws s3 mv s3:// &lt;source bucket/file&gt; s3://&lt;destination-bucket&gt;\n</code></pre></p>"},{"location":"devops/aws-cli/#sync-objects","title":"Sync Objects","text":"<p><pre><code>aws s3 sync . s3://&lt;bucket name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-objects","title":"Delete Objects","text":"<p><pre><code>aws s3 rm s3://&lt;bucket name/file&gt; --recursive\n</code></pre> </p>"},{"location":"devops/aws-cli/#empty-bucket","title":"Empty Bucket","text":"<pre><code>aws s3 rm s3://&lt;bucket name&gt; --recursive\n</code></pre>"},{"location":"devops/aws-cli/#delete-bucket","title":"Delete Bucket","text":"<pre><code>aws s3 rb s3://&lt;bucket name&gt;\n</code></pre>"},{"location":"devops/aws-cli/#vpc-virtual-private-cloud","title":"VPC \u2013 Virtual Private Cloud","text":""},{"location":"devops/aws-cli/#to-create-a-vpc-and-subnets-using-the-aws-cli","title":"To create a VPC and subnets using the AWS CLI","text":""},{"location":"devops/aws-cli/#create-a-vpc-with-a-1000016-cidr-block-using-the-following-create-vpc-command","title":"Create a VPC with a 10.0.0.0/16 CIDR block using the following create-vpc command.","text":"<p><pre><code>aws ec2 create-vpc --cidr-block &lt;Ip address&gt; --query Vpc.VpcId --output text\n</code></pre> </p>"},{"location":"devops/aws-cli/#using-the-vpc-id-from-the-previous-step-create-a-subnet-with-a-1001024-cidr-block-using-the-following-create-subnet-command","title":"Using the VPC ID from the previous step, create a subnet with a 10.0.1.0/24 CIDR block using the following create-subnet command.","text":"<p><pre><code>aws ec2 create-subnet --vpc-id &lt;vpc - Id&gt;--cidr-block &lt;Ip address&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-a-second-subnet-in-your-vpc-with-a-1002024-cidr-block","title":"Create a second subnet in your VPC with a 10.0.2.0/24 CIDR block.","text":"<p><pre><code>aws ec2 create-subnet --vpc-id &lt;vpc - Id&gt;--cidr-block &lt;Ip address&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-an-internet-gateway-using-the-following-create-internet-gateway-command","title":"Create an internet gateway using the following create-internet-gateway command.","text":"<p><pre><code>aws ec2 create-internet-gateway --query InternetGateway.InternetGatewayId --output text\n</code></pre> </p> <p></p>"},{"location":"devops/aws-cli/#using-the-id-from-the-previous-step-attach-the-internet-gateway-to-your-vpc-using-the-following-attach-internet-gateway-command","title":"Using the ID from the previous step, attach the internet gateway to your VPC using the following attach-internet-gateway command.","text":"<pre><code>aws ec2 attach-internet-gateway --vpc-id &lt;vpc - Id&gt;--internet-gateway-id &lt;IGW - Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#create-a-custom-route-table-for-your-vpc-using-the-following-create-route-table-command","title":"Create a custom route table for your VPC using the following create-route-table command.","text":"<p><pre><code>aws ec2 create-route-table --vpc-id &lt;vpc - Id&gt;--query RouteTable.RouteTableId --output text\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-a-route-in-the-route-table-that-points-all-traffic-00000-to-the-internet-gateway-using-the-following-create-route-command","title":"Create a route in the route table that points all traffic (0.0.0.0/0) to the internet gateway using the following create-route command.","text":"<p><pre><code>aws ec2 create-route --route-table-id &lt;route table - Id&gt;--destination-cidr-block 0.0.0.0/0 --gateway-id &lt;Igw - Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#you-can-describe-the-route-table-using-the-following-describe-route-tables-command","title":"You can describe the route table using the following describe-route-tables command.","text":"<p><pre><code>aws ec2 describe-route-tables --route-table-id &lt;route table - Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#the-route-table-is-currently-not-associated-with-any-subnet-you-need-to-associate-it-with-a-subnet-in-your-vpc-so-that-traffic-from-that-subnet-is-routed-to-the-internet-gateway","title":"The route table is currently not associated with any subnet. You need to associate it with a subnet in your VPC so that traffic from that subnet is routed to the internet gateway.","text":"<p><pre><code>aws ec2 describe-subnets --filters \"Name=vpc-id,Values=&lt;vpc \u2013Id&gt;  --query \"Subnets[*].{ID:SubnetId,CIDR:CidrBlock}\"\n</code></pre> </p>"},{"location":"devops/aws-cli/#you-can-choose-which-subnet-to-associate-with-the-custom-route-table-for-example-subnet-0c312202b3f26703a-and-associate-it-using-the-associate-route-table-command-this-subnet-is-your-public-subnet","title":"You can choose which subnet to associate with the custom route table, for example, subnet-0c312202b3f26703a, and associate it using the associate-route-table command. This subnet is your public subnet.","text":"<pre><code>aws ec2 associate-route-table  --subnet-id &lt;subnet-Id&gt; --route-table-id &lt;route table - Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#clean-up","title":"CLEAN UP","text":""},{"location":"devops/aws-cli/#delete-your-custom-route-table","title":"Delete your custom route table:","text":"<pre><code>aws ec2 delete-route-table --route-table-id &lt;route table - Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#delete-your-subnets","title":"Delete your subnets:","text":"<pre><code>aws ec2 delete-subnet --subnet-id &lt;subnet-Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#detach-your-internet-gateway-from-your-vpc","title":"Detach your internet gateway from your VPC:","text":"<pre><code>aws ec2 detach-internet-gateway --internet-gateway-id &lt;Igw -Id&gt; --vpc-id &lt;vpc- Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#delete-your-internet-gateway","title":"Delete your internet gateway:","text":"<pre><code>aws ec2 delete-internet-gateway --internet-gateway-id &lt;Igw - Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#delete-your-vpc","title":"Delete your VPC:","text":"<pre><code>aws ec2 delete-vpc --vpc-id &lt;vpc- Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#cloud-watch","title":"Cloud Watch","text":""},{"location":"devops/aws-cli/#creating-alarm","title":"Creating Alarm","text":"<p><pre><code>aws cloudwatch put-metric-alarm --alarm-name &lt;Alarm name&gt; --alarm-description \"&lt;Description&gt;\" --metric-name &lt;Metric&gt; --namespace AWS/EC2 --statistic Average --period 300 --threshold &lt;70&gt; --comparison-operator &lt;GreaterThanThreshold&gt;  --dimensions \"Name=InstanceId,Value=&lt;Id&gt;\" --evaluation-periods 2 --alarm-actions &lt;SNS \u2013 arn &gt; --unit Percent </code></pre> </p>"},{"location":"devops/aws-cli/#delete-your-alarm","title":"Delete Your Alarm","text":"<pre><code>aws cloudwatch delete-alarms --alarm-names &lt;Alarm name&gt; </code></pre>"},{"location":"devops/aws-cli/#disable-your-alarm","title":"Disable your Alarm","text":"<pre><code>aws cloudwatch disable-alarm-actions --alarm-names &lt;Alarm name&gt;\n</code></pre>"},{"location":"devops/aws-cli/#enable-your-alarm","title":"Enable your Alarm","text":"<pre><code>aws cloudwatch enable-alarm-actions --alarm-names &lt;Alarm name&gt;\n</code></pre>"},{"location":"devops/awsdevops/","title":"DevOps","text":"<p>DevOps is the combination of cultural philosophies, practices, and tools that increase an organization\u2019s ability to deliver applications and services at high velocity: Evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.</p>"},{"location":"devops/awsdevops/#learning-paths-with-practical-workshops","title":"Learning Paths with Practical Workshops","text":"<p>Decoding DevOps:This course is designed to provide a comprehensive understanding of the principles and practices of DevOps. It covers topics such as continuous integration and delivery, infrastructure as code, configuration management, and monitoring and logging. The course is suitable for software developers, system administrators, and IT professionals who want to learn about DevOps and how it can be implemented in their organizations. By the end of the course, you should have a good understanding of the DevOps culture, tools, and practices.</p> <p>Real Time DevOps Projects:This course is focused on providing hands-on experience with DevOps tools and practices through real-world projects. You'll work on projects such as setting up a CI/CD pipeline, automating infrastructure deployment, and monitoring and logging applications. The course is designed for learners who have some experience with DevOps and want to gain practical experience working on real-world projects. By the end of the course, you should have a portfolio of completed projects that you can showcase to potential employers. </p> <p></p>"},{"location":"devops/azure/","title":"Azure","text":"<p>Updating...</p>"},{"location":"devops/docker-setup/","title":"Docker","text":"<p>Docker is a containerized tool designed to make it easier to create, deploy and run applications using containers. Containers allow a developer to package up an application with libraries and other dependencies and deploy it as one package. Containers are OS virtualization. We don't need an OS in the container to install our application. It depends on the Host OS kernel.</p>"},{"location":"devops/docker-setup/#install-docker-engine-docker-compose-on-ubuntu-centos","title":"Install Docker Engine &amp; Docker-Compose on Ubuntu &amp; CentOS","text":"Docker Setup <p>Create a file name docker_setup.sh and copy the below script <pre><code>vim docker_setup.sh\n</code></pre> <pre><code>#!/bin/bash\napt --help &gt;&gt;/dev/null\nif [ $? -eq 0 ]\nthen echo \" INSTALLING DOCKER IN UBUNTU\"\necho\nsudo apt update\nsudo apt-get remove docker docker-engine docker.io containerd runc\nsudo apt-get update\nsudo apt-get -y install \\\napt-transport-https \\\nca-certificates \\\ncurl \\\ngnupg \\\nlsb-release\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\necho \\\n\"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\\n$(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io -y  \nsudo docker run hello-world\n    else\necho \" INSTALLING DOCKER IN CENTOS\"\necho\nsudo yum remove docker \\\ndocker-client \\\ndocker-client-latest \\\ndocker-common \\\ndocker-latest \\\ndocker-latest-logrotate \\\ndocker-logrotate \\\ndocker-engine\nsudo yum install -y yum-utils\nsudo yum-config-manager \\\n--add-repo \\\nhttps://download.docker.com/linux/centos/docker-ce.repo\nsudo yum install docker-ce docker-ce-cli containerd.io -y   \nsudo systemctl start docker\nsudo docker run hello-world\nfi\necho \" Installing Docker Compose\"\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\ndocker-compose --version\n</code></pre> Give Execute permission for script <pre><code>sudo chmod +x docker_setup.sh\n</code></pre> Now run the Script <pre><code>./docker_setup.sh\n</code></pre></p>"},{"location":"devops/docker-setup/#docker-commands","title":"Docker Commands","text":"Docker Images <p>List all images that are locally stored with the Docker Engine <pre><code>docker image ls\n</code></pre> Build an image from the Dockerfile in the current directory and tag the image <pre><code>docker build -t &lt;imagename&gt;:&lt;tag&gt; </code></pre> Delete an image from the local image store <pre><code>docker image rm &lt;imagename&gt;:&lt;tag&gt;\n</code></pre></p> Containers <p>Run a container in an interactive mode: <pre><code>docker run -it &lt;imagename&gt;:&lt;tag&gt;\n</code></pre> Run a container from the Image nginx:latest, name the running container \u201cweb\u201d and expose port 5000 externally, mapped to port 80 inside the container in detached mode.  </p> <p><pre><code>docker run --name web -d -p 5000:80 nginx:latest\n</code></pre> Run a detached container in a previously created container network: <pre><code>docker network create &lt;mynetwork&gt;\n</code></pre> <pre><code>docker run --name web -d --net mynetwork -p 5000:80 nginx:latest\n</code></pre> Follow the logs of a specific container: <pre><code>docker logs -f &lt;container name or container container-id&gt;\n</code></pre> List only active containers <pre><code>docker ps\n</code></pre> List all containers <pre><code>docker ps -a\n</code></pre> Stop a container <pre><code>docker stop &lt;container name or container container-id&gt;\n</code></pre> Stop a container (timeout = 1 second) <pre><code>docker stop -t1\n</code></pre> Remove a stopped container <pre><code>docker rm &lt;container name or container container-id&gt;\n</code></pre> Force stop and remove a container <pre><code>docker rm -f &lt;container name or container container-id&gt;\n</code></pre> Remove all containers <pre><code>docker rm -f $(docker ps-aq)\n</code></pre> Remove all stopped containers <pre><code>docker rm $(docker ps -q -f \u201cstatus=exited\u201d)\n</code></pre> Execute a new process in an existing container: Execute and access bash inside a container <pre><code>docker exec -it &lt;container name or container-id&gt; bash\n</code></pre> To inspect the container <pre><code>docker inspect &lt;container name or container container-id&gt;\n</code></pre></p> Share <p>To Establish Connections from Local to Remote. log in with your Dockerhub Credentials. <pre><code>docker login\n</code></pre> Pull an image from a registry</p> <p><pre><code>docker pull &lt;imagename&gt;:&lt;tag&gt;\n</code></pre> Retag a local image with a new image name and tag <pre><code>docker tag myimage:1.0 myrepo/myimage:2.0\n</code></pre> Push an image to a registry. <pre><code>docker push myrepo/myimage:2.0\n</code></pre></p> Dockerfile <p>Sample Dockerfile for Deploying a Static website.</p> <p><pre><code>vim Dockerfile\n</code></pre> <pre><code>FROM centos:7\nLABEL \"Author\"=\"saiteja Irrinki\"\nLABEL \"Project\"=\"Wave\"\nRUN yum install httpd wget unzip -y\nRUN wget https://www.tooplate.com/zip-templates/2121_wave_cafe.zip\nRUN unzip 2121_wave_cafe.zip\nRUN cp -r 2121_wave_cafe/* /var/www/html/\nCMD [\"/usr/sbin/httpd\", \"-D\", \"FOREGROUND\"]\nEXPOSE 80\nWORKDIR /var/www/html\nVOLUME /var/log/httpd\n</code></pre> Build the image from the Docker file, Here Change my Name with your registry name and Image. <pre><code>docker build -t saitejairrinki/wavecafe:v1 . </code></pre> Run Container From the Image <pre><code>docker run --name wavecafe -d -p 9699:80 saitejairrinki/wavecafe:v1\n</code></pre> Now Access From the Browser, Make sure you have to allow the port number in my case 9699 in your security group if you are using cloud VM. <pre><code>Public-IPaddress:9699\n</code></pre></p> <p>Docker Image</p> <p>You can pull my image and you can also run a container from my image without creating Dockerfile.</p> <pre><code>docker pull saitejairrinki/wavecafe:v1\n</code></pre> <p><pre><code>docker run --name wavecafe -d -p 9999:80 saitejairrinki/wavecafe:v1\n</code></pre> Now Access From the Browser, Make sure you have to allow the port number in my case 9999 in your security group if you are using cloud VM. <pre><code>Public-IPaddress:9999\n</code></pre></p> Docker Compose <p>Creating Docker Compose for local Docker File</p> <pre><code>version: \"3\"\nservices:\nWavecafe:\nbuild:\ncontext: /Dockerfile_path/\nports:\n- \"5555:80\"\ncontainer_name: wavecafe\n</code></pre> <p>Creating Docker Compose for DockerHub Images <pre><code>version: '3'\nservices:\nwebsite:\nimage: saitejairrinki/wavecafe:v1\nports:\n- \"8085:80\"\n</code></pre></p> Docker Volume <p>Creating a Separate Directory to Store Container data</p> <p><pre><code>mkdir mountbind\n</code></pre> Now link your Directory while running the container <pre><code>docker run --name db01 -e MYSQL_ROOT_PASSWORD=secret123 -p 3300:3306 -v /root/mountbind:/var/lib/mysql -d mysql:5.7\n</code></pre> Now do ls to the Directory there you can see the containers data <pre><code>ls mountbind\n</code></pre> Creating docker Volume, use the below command to see all the available options of docker volume <pre><code>docker volume --help\n</code></pre> Creating a new docker volume with name datadb <pre><code>docker volume create datadb\n</code></pre> Now run your container with that volume <pre><code>docker run --name db02 -e MYSQL_ROOT_PASSWORD=secret123 -p 3301:3306 -v datadb:/var/lib/mysql -d mysql:5.7\n</code></pre> Now check  <pre><code>ls /var/lib/docker/volumes/datadb/_data/\n</code></pre> Now for testing Create any file with any name of your choice, here I'm creating a file with the name Milkyway <pre><code>touch /var/lib/docker/volumes/datadb/_data/milkyway\n</code></pre> Now log in into the container and Verify your file. <pre><code>docker exec -it db02 /bin/bash\n</code></pre> <pre><code>ls /var/lib/mysql/\n</code></pre> Now exit from the container <pre><code>exit\n</code></pre> If you want to access the MySQL database with a MySQL client then follow the below steps</p> <p><pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt install mysql-client\n</code></pre> Now fetch the container IP by doing Docker Inspect <pre><code>docker inspect db02 |grep -i ipaddress\n</code></pre> Now Connect with that IP <pre><code>mysql -h 172.17.0.4 -u root -psecret123\n</code></pre></p>"},{"location":"devops/git/","title":"Git Cheat Sheet","text":"<p>Git is a version control system tool. Developers follow to create software, there is going to be a lot of code that developers write or integrate into their software. all this code from different developers in the team has to be merged at a centralized place, which can keep track of all the versions of their code, maintain the code and even revert in time if anything breaks.</p> <p>GIT BASICS</p> Command Usage git init <code>&lt;directory&gt;</code> Create empty Git repo in the specified directory. Run with no arguments to initialize the current directory as a git repository. git clone <code>&lt;repo&gt;</code> Clone repo located at <code>&lt;repo&gt;</code> onto the local machine. The original repo can be located on the local filesystem or on a remote machine via HTTP or SSH. git config user.name <code>&lt;name&gt;</code> Define the author name to be used for all commits in the current repo. Devs commonly use --global flag to set config options for the current user. git add <code>&lt;directory&gt;</code> Stage all changes in <code>&lt;directory&gt;</code> for the next commit. Replace <code>&lt;directory&gt;</code> with a <code>&lt;file&gt;</code> to change a specific file. git commit -m <code>\"&lt;message&gt;\"</code> Commit the staged snapshot, but instead of launching a text editor, use  as the commit message. git status List which files are staged, unstaged, and untracked. git log Display the entire commit history using the default format. git diff Show unstaged changes between your index and working directory. <p>UNDOING CHANGES</p> Command Usage git revert <code>&lt;commit&gt;</code> Create a new commit that undoes all of the changes made in , then apply it to the current branch. git reset <code>&lt;file&gt;</code> Remove  from the staging area, but leave the working directory unchanged. This unstaged a file without overwriting any changes. git clean -n Shows which files would be removed from the working directory. Use the -f flag in place of the -n flag to execute the clean. <p>GIT BRANCHES</p> Command Usage git branch List all of the branches in your repo. Add a  argument to create a new branch with the name . git checkout -b <code>&lt;branch&gt;</code> Create and check out a new branch named . Drop the -b flag to checkout an existing branch. git merge <code>&lt;branch&gt;</code> Merge  into the current branch. <p>REMOTE REPOSITORIES</p> Command Usage git remote add <code>&lt;name&gt;</code> <code>&lt;url&gt;</code> Create a new connection to a remote repo. After adding a remote,you can use  as a shortcut for  in other commands. git fetch <code>&lt;remote&gt;</code> <code>&lt;branch&gt;</code> Fetches a specific , from the repo. Leave off  to fetch all remote refs. git pull  Fetch the specified remote\u2019s copy of the current branch and immediately merge it into the local copy. git push <code>&lt;remote&gt;</code> <code>&lt;branch&gt;</code> Push the branch to , along with necessary commits and objects. Creates named branch in the remote repo if it doesn\u2019t exist. <p>GIT RESET</p> Command Usage git reset Reset the staging area to match the most recent commit but leave the working directory unchanged. git reset --hard Reset staging area and working directory to match the most recent commit and overwrites all changes in the working directory. git reset <code>&lt;commit&gt;</code> Move the current branch tip backward to , reset the staging area to match, but leave the working directory alone. git reset --hard <code>&lt;commit&gt;</code> Same as previous, but resets both the staging area &amp; working directory to match. Deletes uncommitted changes, and all commits after . <p>GIT PULL</p> Command Usage git pull --rebase <code>&lt;remote&gt;</code> Fetch the remote\u2019s copy of the current branch and rebase it into the local copy. Uses git rebase instead of the merge to integrate the branches. <p>GIT PUSH</p> Command Usage git push <code>&lt;remote&gt;</code> --force Forces the git push even if it results in a non-fast-forward merge. Do not use the --force flag unless you\u2019re sure you know what you\u2019re doing. git push <code>&lt;remote&gt;</code> --all Push all of your local branches to the specified remote. git push <code>&lt;remote&gt;</code> --tags Tags aren\u2019t automatically pushed when you push a branch or use the --all flag. The --tags flag sends all of your local tags to the remote repo."},{"location":"devops/jenkins/","title":"Jenkins","text":"<p>Jenkins is a continuous integration tool. It can fetch the code from the version control system, build the code, test it and notify the developer. Jenkins can do continuous delivery also. Jenkins has so many plugins, by using these plugins we can do any task in Jenkins. Jenkins is an open sources project. It is a java based web application server so we need to set up first java on the machine to run the jenkins server.</p>"},{"location":"devops/jenkins/#jenkins-ci-cd-automation","title":"Jenkins CI CD Automation","text":""},{"location":"devops/jenkins/#jenkins-setup","title":"Jenkins Setup","text":"UbuntuCentOS <pre><code> #!/bin/bash\nsudo apt update\nsudo apt install openjdk-11-jdk -y\ncurl -fsSL https://pkg.jenkins.io/debian/jenkins.io-2023.key | sudo tee \\\n/usr/share/keyrings/jenkins-keyring.asc &gt; /dev/null\necho deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\\nhttps://pkg.jenkins.io/debian binary/ | sudo tee \\\n/etc/apt/sources.list.d/jenkins.list &gt; /dev/null\nsudo apt-get update\nsudo apt-get install jenkins -y\n</code></pre> <pre><code>#!/bin/bash\nsudo wget -O /etc/yum.repos.d/jenkins.repo \\\nhttps://pkg.jenkins.io/redhat-stable/jenkins.repo\nsudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key\nsudo yum upgrade -y\n# Add required dependencies for the jenkins package\nsudo yum install fontconfig java-11-openjdk -y\nsudo yum install jenkins -y\n</code></pre>"},{"location":"devops/jenkins/#cicd-setup","title":"CICD Setup","text":"<p>You can find my Jenkins CICD pipeline jobs which are saved in an HTML format, please download and extract to see the jobs </p> <p>To View My Jenkins CICD Jobs Click here</p>"},{"location":"devops/k3s/","title":"Creating Kubernetes Cluster with k3s","text":"<p>K3s is a lightweight Kubernetes distribution that Rancher Labs, which is a fully certified Kubernetes offering by CNCF. In K3s, we see that the memory footprint or binary which contains the components to run a cluster is small. It means that K3s are small in size.</p>"},{"location":"devops/k3s/#setup","title":"Setup","text":"<pre><code>curl -sfL https://get.k3s.io | sh -\n</code></pre> <p><pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\n</code></pre> <pre><code>sudo chmod 644 $KUBECONFIG\n</code></pre></p>"},{"location":"devops/k8sdashboard/","title":"Deploy the latest Kubernetes dashboard","text":"<p>Once you\u2019ve set up your Kubernetes cluster or if you already had one running, we can get started.</p> <p>The first thing to know about the web UI is that it can only be accessed using the localhost address on the machine it runs on. This means we need to have an SSH tunnel to the server. For most OS, you can create an SSH tunnel using this command. Replace the  and  with the relevant details to your Kubernetes cluster. <pre><code>ssh -L localhost:8001:127.0.0.1:8001 &lt;user&gt;@&lt;master_public_IP&gt;\n</code></pre> After you\u2019ve logged in, you can deploy the dashboard itself with the following single command. <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n</code></pre> If your cluster is working correctly, you should see an output confirming the creation of a bunch of Kubernetes components like in the example below. Output <pre><code>namespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n</code></pre> <p>Afterward, you should have two new pods running on your cluster.</p> <pre><code>kubectl get pods -A\n</code></pre> Output <pre><code>...\nkubernetes-dashboard   dashboard-metrics-scraper-78ujd94gf7-ff6h8   1/1     Running   0          30m\nkubernetes-dashboard   kubernetes-dashboard-g6hujkirf7-df65g        1/1     Running   0          30m\n</code></pre> <p>You can then continue ahead with creating the required user accounts.</p>"},{"location":"devops/k8sdashboard/#creating-admin-user","title":"Creating Admin user","text":"<p>The Kubernetes dashboard supports a few ways to manage access control. In this example, we\u2019ll be creating an admin user account with full privileges to modify the cluster and use tokens.</p> <p>Start by making a new directory for the dashboard configuration files. <pre><code>mkdir ~/dashboard &amp;&amp; cd ~/dashboard\n</code></pre> Create the following configuration and save it as a dashboard-admin.yaml file. Note that indentation matters in the YAML files which should use two spaces in a regular text editor.</p> <pre><code>vim dashboard-admin.yaml\n</code></pre> <p><pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: admin-user\nnamespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\nname: admin-user\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: ClusterRole\nname: cluster-admin\nsubjects:\n- kind: ServiceAccount\nname: admin-user\nnamespace: kubernetes-dashboard\n</code></pre> Once set, save the file and exit the editor.</p> <p>Then deploy the admin user role with the next command.</p> <p><pre><code>kubectl apply -f dashboard-admin.yaml\n</code></pre> You should see a service account and a cluster role binding created.</p> Output <pre><code>serviceaccount/admin-user created\nclusterrolebinding.rbac.authorization.k8s.io/admin-user created\n</code></pre> <p>Using this method doesn\u2019t require setting up or memorising passwords, instead, accessing the dashboard will require a token.</p> <p>Get the admin token using the command below. <pre><code>kubectl get secret -n kubernetes-dashboard $(kubectl get serviceaccount admin-user -n kubernetes-dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode\n</code></pre> You\u2019ll then see an output of a long string of seemingly random characters like in the example below.</p> Output <p>eyJhbGciOiJSUzI1NiIsImtpZCI6Ilk2eEd2QjJMVkhIRWNfN2xTMlA5N2RNVlR5N0o1REFET0dp dkRmel90aWMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlc y5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1Y mVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuL XEyZGJzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZ SI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb 3VudC51aWQiOiI1ODI5OTUxMS1hN2ZlLTQzZTQtODk3MC0yMjllOTM1YmExNDkiLCJzdWIiOiJze XN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.GcUs MMx4GnSV1hxQv01zX1nxXMZdKO7tU2OCu0TbJpPhJ9NhEidttOw5ENRosx7EqiffD3zdLDptS22F gnDqRDW8OIpVZH2oQbR153EyP_l7ct9_kQVv1vFCL3fAmdrUwY5p1-YMC41OUYORy1JPo5wkpXrW OytnsfWUbZBF475Wd3Gq3WdBHMTY4w3FarlJsvk76WgalnCtec4AVsEGxM0hS0LgQ-cGug7iGbmf cY7odZDaz5lmxAflpE5S4m-AwsTvT42ENh_bq8PS7FsMd8mK9nELyQu_a-yocYUggju_m-BxLjgc 2cLh5WzVbTH_ztW7COlKWvSVg7yhjikrew</p> <p>The token is created each time the dashboard is deployed and is required to log into the dashboard. Note that the token will change if the dashboard is stopped and redeployed.</p>"},{"location":"devops/k8sdashboard/#creating-read-only-user","title":"Creating Read-Only user","text":"<p>If you wish to provide access to your Kubernetes dashboard, for example, for demonstrative purposes, you can create a read-only view for the cluster.</p> <p>Similarly to the admin account, save the following configuration in dashboard-read-only.yaml <pre><code>vim dashboard-read-only.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: read-only-user\nnamespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\nannotations:\nrbac.authorization.kubernetes.io/autoupdate: \"true\"\nlabels:\nname: read-only-clusterrole\nnamespace: default\nrules:\n- apiGroups:\n- \"\"\nresources: [\"*\"]\nverbs:\n- get\n- list\n- watch\n- apiGroups:\n- extensions\nresources: [\"*\"]\nverbs:\n- get\n- list\n- watch\n- apiGroups:\n- apps\nresources: [\"*\"]\nverbs:\n- get\n- list\n- watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\nname: read-only-binding\nroleRef:\nkind: ClusterRole\nname: read-only-clusterrole\napiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\nname: read-only-user\nnamespace: kubernetes-dashboard\n</code></pre></p> <p>Once set, save the file and exit the editor.</p> <p>Then deploy the read-only user account with the command below.</p> <p><pre><code>kubectl apply -f dashboard-read-only.yaml\n</code></pre> To allow users to log in via the read-only account, you\u2019ll need to provide a token that can be fetched using the next command. <pre><code>kubectl get secret -n kubernetes-dashboard $(kubectl get serviceaccount read-only-user -n kubernetes-dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode\n</code></pre> The toke will be a long series of characters and unique to the dashboard currently running.</p>"},{"location":"devops/k8sdashboard/#accessing-the-dashboard","title":"Accessing the dashboard","text":"<p>We\u2019ve now deployed the dashboard and created user accounts for it. Next, we can get started managing the Kubernetes cluster itself.</p> <p>However, before we can log in to the dashboard, it needs to be made available by creating a proxy service on the localhost. Run the next command on your Kubernetes cluster. <pre><code>kubectl proxy\n</code></pre> This will start the server at 127.0.0.1:8001 as shown by the output.</p> Output <p>Starting to serve on 127.0.0.1:8001</p> <p>Now, assuming that we have already established an SSH tunnel binding to the localhost port 8001 at both ends, open a browser to the link below.</p> <p>Link</p> <p>http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</p> <p>If everything is running correctly, you should see the dashboard login window.</p> <p></p> <p>Select the token authentication method and copy your admin token into the field below. Then click the Sign in button.</p> <p>You will then be greeted by the overview of your Kubernetes cluster.</p> <p></p> <p>While signed in as an admin, you can deploy new pods and services quickly and easily by clicking the plus icon at the top right corner of the dashboard.</p> <p></p> <p>Then either copy in any configuration file you wish, select the file directly from your machine, or create a new configuration from a form.</p>"},{"location":"devops/kops/","title":"Creating Kubernetes Cluster with KOPS","text":"<p>Install AWS CLI  <pre><code>apt update &amp;&amp; apt install awscli -y\n</code></pre> Configure AWS CLI with IAM user Credentials with a specific Region  <pre><code>aws configure\n</code></pre></p> <p>Note</p> <p>If you are using AWS Instance better to use IAM Role than Creating User with Access-key</p> <p>Check Whether AWS CLI Commands Working or not  <pre><code>aws s3 ls\n</code></pre> Generate SSH Keys <pre><code>ssh-keygen\n</code></pre></p>"},{"location":"devops/kops/#install-kubectl-binary-with-curl-on-linux","title":"Install kubectl binary with curl on Linux","text":"<p>Download the latest release with the command: <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n</code></pre> Install kubectl <pre><code>sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre> <pre><code>kubectl version --client\n</code></pre></p>"},{"location":"devops/kops/#installing-kubernetes-with-kops","title":"Installing Kubernetes with kops","text":"<p>Download the latest release with the command:</p> <p><pre><code>curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-linux-amd64\n</code></pre> <pre><code>curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-linux-amd64\n</code></pre></p> <p>Make the kops binary executable <pre><code>chmod +x kops-linux-amd64\n</code></pre> Move the kops binary into your PATH. <pre><code>sudo mv kops-linux-amd64 /usr/local/bin/kops\n</code></pre> <pre><code>kops\n</code></pre></p>"},{"location":"devops/kops/#creating-k8s-cluster-with-kops","title":"Creating K8s Cluster with KOPS","text":""},{"location":"devops/kops/#kops-commands-to-setup-k8s-cluster-","title":"Kops commands to setup k8s cluster:-","text":"<p>Creating S3 Bucket for Kubernetes Cluster <pre><code>aws s3 mb s3:// &lt;bucket name&gt;\n</code></pre> <pre><code>kops create cluster --name=saiteja.irrinki.xyz --state=s3://&lt;s3 bucket&gt; --zones=ap-south-1a,ap-south-1b --node-count=2 --node-size=t3.medium --master-size=t3.medium --dns-zone=saiteja.irrinki.xyz --node-volume-size=8 --master-volume-size=8\n</code></pre> It will create a configuration of kops <pre><code>kops update cluster --name=saiteja.irrinki.xyz --state=s3://&lt;s3 bucket&gt; --yes --admin\n</code></pre> It will create kops data in the S3 bucket. It starts creating a cluster &amp; it takes 10 mins <pre><code>kops validate cluster --name=saiteja.irrinki.xyz --state=s3://&lt;s3 bucket&gt;\n</code></pre> It shows ur cluster is ready</p> <p>To Delete Cluster</p> <pre><code>kops delete cluster --name=saiteja.irrinki.xyz --state=s3://&lt;s3 bucket&gt; --yes \n</code></pre>"},{"location":"devops/kubeadm/","title":"Creating Kubernetes Cluster with kubeadm","text":"For this activity, deploy minimum 2 AWS instances with the Security groups <p>All traffic is enabled between the instances </p> Node Name Instance Details Resources Master Node -t2.medium 4GB Ram , 2 CPU Worker Node -t2.micro 1GB Ram , 1 CPU"},{"location":"devops/kubeadm/#preparing-the-master-and-worker-nodes-for-kubeadm","title":"Preparing the Master and Worker nodes for kubeadm","text":"<p>Execute the below Commands on both Master &amp; Worker Nodes</p> <p>Change the hostname for nodes as master &amp; worker</p>"},{"location":"devops/kubeadm/#installing-docker","title":"Installing Docker:","text":"<p>Add the Docker repository key and Docker repository. <pre><code>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -\nadd-apt-repository \\\n\"deb https://download.docker.com/linux/$(. /etc/os-release; echo \"$ID\") \\\n$(lsb_release -cs) \\\nstable\"\n</code></pre> Update the list of packages and install the docker</p> <p><pre><code>apt-get update\n</code></pre> <pre><code>apt-get install -y docker-ce\n</code></pre> <pre><code>echo '{\"exec-opts\": [\"native.cgroupdriver=systemd\"]}' | sudo tee /etc/docker/daemon.json\n</code></pre> <pre><code>systemctl daemon-reload\n</code></pre> <pre><code>systemctl restart docker\n</code></pre> <pre><code>systemctl enable docker\n</code></pre></p>"},{"location":"devops/kubeadm/#installing-kubeadm-kublet-and-kubectl","title":"Installing kubeadm, kublet, and kubectl:","text":"<p>Add the Google repository key and Google repository</p> <p><pre><code>curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n</code></pre> <pre><code>vim /etc/apt/sources.list.d/kubernetes.list\n</code></pre> <pre><code>deb http://apt.kubernetes.io kubernetes-xenial main\n</code></pre> Update the list of packages. And install kubelet, kubeadm, and kubectl.</p> <p><pre><code>apt-get update </code></pre> <pre><code>apt-get install kubelet kubeadm kubectl -y\n</code></pre> Disable the swap.</p> <pre><code>swapoff -a\n</code></pre> <p>Setup Master Node to Connect with Worker Nodes</p> <p><pre><code>sed -i '/ swap / s/^/#/' /etc/fstab\n</code></pre> <pre><code> echo NODENAME=$(hostname -s)\n</code></pre> <pre><code>kubeadm init --apiserver-advertise-address=172.31.85.246  --apiserver-cert-extra-sans=172.31.85.246  --pod-network-cidr=10.0.0.0/16 </code></pre> Now Create a Directory Name .kube in the master node home directory <pre><code>mkdir .kube\n</code></pre> Copy the Default conf file to the .kube directory <pre><code>cp /etc/kubernetes/admin.conf .kube/config\n</code></pre> Replace the Ip address with your Instance Private Ip Address and set pod Network </p> <p>After Executing kubeadm init command you will get one command as output that need to execute on worker nodes</p> <p><pre><code>kubeadm join 172.31.85.246:6443 --token n6v08z.53b057pfwnj8mq10         --discovery-token-ca-cert-hash sha256:9e8a38ddfc46c41ecb3317db9fc2145f70bddbdd2c6b8091fbdbab18e1dbcb19 </code></pre> Configuring the Calico in Master Node <pre><code>kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml\n</code></pre> Now check Kubectl commands <pre><code>kubectl get node\n</code></pre> Now test the cluster </p> <p><pre><code>kubectl run pod --image=nginx\n</code></pre> <pre><code>kubectl get pod\n</code></pre> <pre><code>kubectl get pod -o wide\n</code></pre></p>"},{"location":"devops/kubernetes/","title":"Kubernetes","text":"<p>Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery.</p> <p>Kubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.</p> <p>The Top 10 Reasons why Kubernetes is so popular are as follows</p> <ul> <li>Largest Open Source project in the world</li> <li>Great Community Support</li> <li>Robust Container deployment</li> <li>Effective Persistent storage</li> <li>Multi-Cloud Support(Hybrid Cloud)</li> <li>Container health monitoring</li> <li>Compute resource management</li> <li>Auto-scaling Feature Support</li> <li>Real-world Use cases Available</li> <li>High availability by cluster federation</li> </ul>"},{"location":"devops/shellscripting/","title":"Shell Scripting","text":""},{"location":"devops/shellscripting/#if-condition","title":"If Condition","text":"If ConditionIf Else ConditionElif ConditionExample <pre><code>#!/bin/bash\n\n&lt;Command&gt;\n\nif [ &lt;condition&gt; ]\n\nthen\n\n&lt;Commands to Execute&gt;\n&lt;print Messages with echo&gt;\nfi\n</code></pre> <pre><code>#!/bin/bash\n\n&lt;Command&gt;\n\nif [ &lt;condition&gt; ]\n\nthen\n\n&lt;Commands to Execute&gt;\n&lt;print Messages with echo&gt;\n\nelse\n&lt;Commands to Execute&gt;\n&lt;print Messages with echo&gt;\n\nfi\n</code></pre> <pre><code>#!/bin/bash\n\n&lt;Command&gt;\n\nif [ &lt;condition&gt; ]\n\nthen\n\n&lt;Commands to Execute&gt;\n&lt;print Messages with echo&gt;\n\nelif [ &lt;condition&gt; ]\nthen\n\n&lt;Commands to Execute&gt;\n&lt;print Messages with echo&gt;\n\nelse\n&lt;Commands to Execute&gt;\n&lt;print Messages with echo&gt;\n\nfi\n</code></pre> <pre><code>#!/bin/bash\n\napt --help &gt;&gt; /dev/null\n\nif [ $? -eq 0 ]\n\nthen\necho \" This is Ubuntu Operating System\"\nelse\necho \" This is CentOS Operating System\"\nfi\n</code></pre> Reference Link <p>How to program with Bash</p> <p>https://opensource.com/article/19/10/programming-bash-logical-operators-shell-expansions</p>"},{"location":"devops/shellscripting/#for-loop","title":"For Loop","text":"For LoopExample <pre><code>#!/bin/bash\n\nfor &lt;variable&gt; in &lt;list&gt;\ndo\n&lt;command&gt;\ndone\n</code></pre> <pre><code>#For loop example for adding users with name alpha beta gamma\n\n#!/bin/bash\n\nfor USER in alpha beta gamma do\necho \"adding user $USER to system\"\n\nsudo useradd $USER\n\ndone\n</code></pre>"},{"location":"devops/shellscripting/#while-loop","title":"While Loop","text":"While LoopWhile Loop Example <pre><code>#!/bin/bash\nwhile [ &lt;condition&gt; ]\ndo\n&lt;command&gt;\ndone\n</code></pre> <pre><code>#!/bin/bash\n#Here variable \"a\" is speed\n\na=0\necho \"Starting the Engine\"\n\nwhile [ $a -le 100 ]\ndo\nsleep 1\necho \"Current Speed $a\"\n\na=$(($a+10))\n\ndone\n</code></pre>"},{"location":"devops/shellscripting/#shell-script-for-setting-up-website","title":"Shell Script For Setting Up Website","text":"UbuntuCentosUsing VariablesUsing If Else Condition <pre><code>#!/bin/bash\nsudo apt update\nsudo apt install wget net-tools unzip figlet apache2 -y\nsudo systemctl start apache2\nsudo systemctl enable apache2\nmkdir -p webfiles\ncd webfiles\nsudo wget https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip\nsudo unzip -o 2118_chilling_cafe.zip\nsudo cp -r 2118_chilling_cafe/* /var/www/html/\ncd ..\nsudo rm -rf webfiles\nsudo systemctl restart apache2\nfiglet done\n</code></pre> <pre><code>#!/bin/bash\nsudo yum install wget net-tools unzip httpd -y\nsudo systemctl start httpd\nsudo systemctl enable httpd\nmkdir -p webfiles\ncd webfiles\nsudo wget https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip\nsudo unzip -o 2118_chilling_cafe.zip\nsudo cp -r 2118_chilling_cafe/* /var/www/html/\ncd ..\nsudo rm -rf webfiles\nsudo systemctl restart httpd\n</code></pre> <pre><code>#!/bin/bash\n#Website setup\n#Adding variables :-)\nURL=https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip\nSRV=httpd\nPKG=yum\nFILE=2118_chilling_cafe\necho \"  Installing the Services &amp; Extractors\"\necho\nsudo $PKG install $SRV wget unzip -y &gt;&gt; /dev/null\necho \"Start &amp; Enabling the Services\"\necho\nsudo systemctl start $SRV\nsudo systemctl enable $SRV\necho \"Downloading the zip file from tooplate.com\"\necho\nmkdir -p webfiles\ncd webfiles\necho\nsudo wget $URL &gt;&gt; /dev/null\necho \"extracting the files \"\necho\nsudo unzip -o $FILE.zip &gt;&gt; /dev/null\necho \"copying the extracted file into html\"\necho\nsudo cp -r $FILE/* /var/www/html &gt;&gt; /dev/null\necho \"Restarting the Services\"\nsudo systemctl restart $SRV\ncd ..\nsudo rm -rf webfiles\nsudo systemctl status $SRV | grep Active\ndate\n</code></pre> <pre><code>#!/bin/bash\napt --help &gt;&gt; /dev/null\nif [ $? -eq 0 ]\nthen sudo apt update\nsudo apt install wget figlet net-tools unzip apache2 -y\nsudo systemctl start apache2\nsudo systemctl enable apache2\nmkdir -p webfiles\ncd webfiles\nsudo wget https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip\nsudo unzip -o 2118_chilling_cafe.zip\nsudo cp -r 2118_chilling_cafe/* /var/www/html/\n   cd ..\n   sudo rm -rf webfiles\n   sudo systemctl restart apache2\n   sudo systemctl status apache2 | grep Active\nfiglet done\nelse\n    sudo yum install wget net-tools unzip httpd -y\n    sudo systemctl start httpd\n    sudo systemctl enable httpd\n    sudo mkdir -p webfiles\n    cd webfiles\n    sudo wget https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip\n    sudo unzip -o 2118_chilling_cafe.zip\n    sudo cp -r 2118_chilling_cafe/* /var/www/html/\n    cd ..    \n    sudo rm -rf webfiles\n    sudo systemctl restart httpd\n    sudo systemctl status httpd | grep Active\nfi\n</code></pre>"},{"location":"devops/terraform/","title":"Terraform","text":"<p>Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can help with multi-cloud by having one workflow for all clouds. The infrastructure Terraform manages can be hosted on public clouds like Amazon Web Services, Microsoft Azure, and Google Cloud Platform, or on-prem in private clouds such as VMWare vSphere.</p>"},{"location":"devops/terraform/#terraform-installation","title":"Terraform Installation","text":"UbuntuCentOs <pre><code>curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -\n</code></pre> <pre><code>sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\"\n</code></pre> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install terraform\n</code></pre> <pre><code>sudo yum install -y yum-utils\n</code></pre> <pre><code>sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo\n</code></pre> <pre><code>sudo yum -y install terraform\n</code></pre>"},{"location":"devops/terraform/#reference","title":"Reference","text":"Terraform Registry <p>https://registry.terraform.io/</p> Terraform Commands Cheat sheet Reference Link <p>https://acloudguru.com/blog/engineering/the-ultimate-terraform-cheatsheet</p>"},{"location":"devops/terraform/#terraform-sample-scripts","title":"Terraform Sample Scripts","text":"<p>Launching Ec2 Instance</p> <pre><code>provider \"aws\" {\n\nregion = \"us-east-2\"\n\n}\n\n\nresource \"aws_instance\" \"Instance\" {\n\nami                    = \"ami-0fb653ca2d3203ac1\"\n\ninstance_type          = \"t2.micro\"\n\nkey_name               = \"terraform\"\n\nvpc_security_group_ids = [\"sg-0138c7796472ac9a9\"]\n\ntags = {\n\nName = \"IAAC\"\n\nTeam = \"DevOps\"\n\n}\n\n}\n</code></pre>"},{"location":"devops/terraform/#terraform-vars","title":"Terraform Vars","text":"<p>Using Variables</p> <p>Create a File vars.tf </p> <p><pre><code>vim vars.tf\n</code></pre> <pre><code>variable \"REGION\" {\n\ndefault = \"us-east-2\"\n\n}\n\n\n\nvariable \"ZONE1\" {\n\ndefault = \"us-east-2a\"\n\n}\n\n\nvariable \"AMIS\" {\n\ntype = map(any)\n\ndefault = {\n\nus-east-2 = \"ami-0fb653ca2d3203ac1\"\n\nus-east-1 = \"ami-0e1d30f2c40c4c701\"\n\n}\n\n}\n</code></pre></p>"},{"location":"devops/terraform/#terraform-provider","title":"Terraform Provider","text":"<p>Example for AWS <pre><code>vim provider.tf\n</code></pre></p> <pre><code>provider \"aws\" {\n\nregion = var.REGION\n\n}\n</code></pre> <p>Launching Instance with Vars File</p> <pre><code>vim Instance.tf\n</code></pre> <pre><code>resource \"aws_instance\" \"Instance\" {\n\nami                    = var.AMIS[var.REGION]\n\ninstance_type          = \"t2.micro\"\n\nkey_name               = \"terraform\"\n\nvpc_security_group_ids = [\"sg-0138c7796472ac9a9\"]\n\ntags = {\n\nName = \"IAAC\"\n\nTeam = \"DevOps\"\n\n}\n\n}\n</code></pre>"},{"location":"devops/terraform/#terraform-provisioning","title":"Terraform Provisioning","text":"<p>Launching AWS Resources with Terraform Provisioning</p> <pre><code>vim instance_prov.tf\n</code></pre> <pre><code>resource \"aws_key_pair\" \"testing007\" {\n\nkey_name   = \"testing007\"\n\npublic_key = file(\"testing007.pub\")\n\n}\n\nresource \"aws_instance\" \"Instance\" {\n\nami                    = var.AMIS[var.REGION]\n\ninstance_type          = \"t2.micro\"\n\nkey_name               = aws_key_pair.testing007.key_name\n\nvpc_security_group_ids = [\"sg-0138c7796472ac9a9\"]\n\ntags = {\n\nName = \"IAAC\"\n\nTeam = \"DevOps\"\n\n}\n\nprovisioner \"file\" {\n\nsource      = \"./web.sh\"\n\ndestination = \"/tmp/web.sh\"\n\n}\n\nprovisioner \"remote-exec\" {\n\ninline = [\n\n\"chmod u+x /tmp/web.sh\",\n\n\"sudo /tmp/web.sh\"\n\n]\n\n}\n\nconnection {\n\nuser        = var.USER\n\nprivate_key = file(\"testing007\")\n\nhost        = self.public_ip\n\n}\n\n}\n\noutput \"PublicIP\" {\n\nvalue = aws_instance.Instance.public_ip\n\n}\n</code></pre> <p>Variables file - vars.tf </p> <pre><code>variable \"REGION\" {\n\ndefault = \"us-east-2\"\n\n}\n\n\n\nvariable \"ZONE1\" {\n\ndefault = \"us-east-2a\"\n\n}\n\n\n\nvariable \"USER\" {\n\ndefault = \"ubuntu\"\n\n}\n\n\n\nvariable \"AMIS\" {\n\ntype = map(any)\n\ndefault = {\n\nus-east-2 = \"ami-0fb653ca2d3203ac1\"\n\nus-east-1 = \"ami-0e1d30f2c40c4c701\"\n\n\n\n}\n\n}\n</code></pre>"},{"location":"devops/terraform/#to-store-state-remotely-in-s3-bucket","title":"To Store State Remotely in S3 Bucket","text":"<p>Create an S3 Bucket</p> <pre><code>terraform {\n\nbackend \"s3\" {\n\nbucket = \"terraform-state-009\"\n\nkey    = \"terraform/remote\"\n\nregion = \"us-east-1\"\n\n}\n\n}\n</code></pre>"},{"location":"devops/vagrant/","title":"Vagrant","text":"<p>Vagrant is an open-source tool that helps us to automate the creation and management of Virtual Machines. In a nutshell, we can specify the configuration of a virtual machine in a simple configuration file, and Vagrant creates the same Virtual machine using just one simple command. It provides command-line interfaces to automate such tasks.</p> Requirements <p>Virtualbox</p> <p>Vagrant</p>"},{"location":"devops/vagrant/#installing-virtualbox-in-the-host-machine","title":"Installing Virtualbox in the Host Machine","text":"WindowsUbuntu Desktop <p>Download link <pre><code>https://download.virtualbox.org/virtualbox/6.1.30/VirtualBox-6.1.30-148432-Win.exe\n</code></pre></p> <p>Through Command line  <pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt install virtualbox\n</code></pre></p> Note <p>To Run Virtual Machine You should need to Disable the Secure-Boot in BIOS Options of your Machine</p> <p>To Verify that your Virtual Box is Working fine or not in Linux, Please hit the below command. <pre><code>sudo systemctl status virtualbox.service\n</code></pre></p>"},{"location":"devops/vagrant/#installing-vagrant-in-the-host-machine","title":"Installing Vagrant in the Host Machine","text":"WindowsUbuntu Desktop <p>Download link <pre><code>https://releases.hashicorp.com/vagrant/2.3.4/vagrant_2.3.4_windows_i686.msi\n</code></pre></p> <p>Through Command line  <pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt install vagrant\n</code></pre></p> Note <p>You Should Need to Restart Your Machine After Installation of Vagrant </p>"},{"location":"devops/vagrant/#creating-a-common-directory-for-all-vms","title":"Creating a Common Directory for all VMs","text":"<p>Create a Directory with name vagrantvms <pre><code>mkdir vagrantvms\n</code></pre> Change Directory to vagrantvms <pre><code>cd vagrantvms\n</code></pre></p>"},{"location":"devops/vagrant/#to-bring-up-vm","title":"To Bring up VM","text":"UbuntuCentOS <p>Create a Directory name ubuntu <pre><code>mkdir ubuntu </code></pre> Change Directory to ubuntu <pre><code>cd ubuntu\n</code></pre> Initialize Ubuntu VM <pre><code>vagrant init ubuntu/bionic64 </code></pre> Now Bring up your Ubuntu VM  <pre><code>vagrant up\n</code></pre> Once your VM is up then login to your vm  <pre><code>vagrant ssh\n</code></pre> Now you can see the prompt of the ubuntu machine, If you wanna exit type exit</p> <p>Create a Directory name centos <pre><code>mkdir centos </code></pre> Change Directory to centos <pre><code>cd centos\n</code></pre> Initialize centos VM <pre><code>vagrant init geerlingguy/centos7 </code></pre> Now Bring up your centos VM  <pre><code>vagrant up\n</code></pre> Once your VM is up then login to your VM  <pre><code>vagrant ssh\n</code></pre> Now you can see the prompt of the centos machine, If you wanna exit type exit</p> To Init Specific Vagrant box <p>https://app.vagrantup.com/boxes/search</p> Multi VMS Vagrantfile <pre><code>Vagrant.configure(\"2\") do |config|\nconfig.hostmanager.enabled = true config.hostmanager.manage_host = true\n\n### Nginx VM ###\nconfig.vm.define \"web01\" do |web01|\nweb01.vm.box = \"ubuntu/bionic64\"\nweb01.vm.hostname = \"web01\"\nweb01.vm.network \"private_network\", ip: \"192.168.33.11\"\nweb01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\nend\n\n### tomcat vm ###\nconfig.vm.define \"app01\" do |app01|\napp01.vm.box = \"geerlingguy/centos7\"\napp01.vm.hostname = \"app01\"\napp01.vm.network \"private_network\", ip: \"192.168.33.12\"\napp01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\napp01.vm.provider \"virtualbox\" do |vb|\nvb.memory = \"1024\"\nend\napp01.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n#     yum update -y\n     yum install epel-release -y\n   SHELL\nend\n\n### RabbitMQ vm  ####\nconfig.vm.define \"rmq01\" do |rmq01|\nrmq01.vm.box = \"geerlingguy/centos7\"\nrmq01.vm.hostname = \"rmq01\"\nrmq01.vm.network \"private_network\", ip: \"192.168.33.16\"\nrmq01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\nrmq01.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n   # yum update -y\n    #yum install epel-release -y\n    # yum install wget -y\n    SHELL\nend ; f\n., v/\n### Memcache vm  #### \nconfig.vm.define \"mc01\" do |mc01|\nmc01.vm.box = \"geerlingguy/centos7\"\nmc01.vm.hostname = \"mc01\"\nmc01.vm.network \"private_network\", ip: \"192.168.33.14\"\nmc01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\nmc01.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n     yum install epel-release -y\n    SHELL\nend\n\n### DB vm  ####\nconfig.vm.define \"db01\" do |db01|\ndb01.vm.box = \"geerlingguy/centos7\"\ndb01.vm.hostname = \"db01\"\ndb01.vm.network \"private_network\", ip: \"192.168.33.15\"\ndb01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\ndb01.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n#   yum update -y\n    yum install epel-release -y\n    SHELL\nend\n\n\nend\n</code></pre>"},{"location":"miscellaneous/activedir/","title":"MISCELLANEOUS","text":"<p>To Setup Active Directory Domain Service in Windows Server Click here</p>"},{"location":"miscellaneous/ldap/","title":"Install and Configure OpenLDAP Server on Ubuntu","text":""},{"location":"miscellaneous/ldap/#set-hostname-for-the-ubuntu-server","title":"Set hostname for the Ubuntu server","text":"<p><pre><code>sudo hostnamectl set-hostname ldap.k8sengineers.com\n</code></pre> Add the IP and FQDN to file /etc/hosts <pre><code>vim /etc/hosts\n</code></pre></p> <pre><code>192.168.0.151 ldap.k8sengineers.com </code></pre> <p>Note</p> <p>Replace ldap.k8sengineers.com with your correct hostname/valid domain name.</p>"},{"location":"miscellaneous/ldap/#install-openldap-server-on-ubuntu","title":"Install OpenLDAP Server on Ubuntu","text":"<p><pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt -y install slapd ldap-utils\n</code></pre> During the installation, you\u2019ll be prompted to set LDAP admin password, provide your desired password, then press  <p></p> <p>Confirm the password and continue the installation by selecting  <p>You can confirm that your installation was successful using the commandslapcat to output SLAPD database contents.</p> <p><pre><code>sudo slapcat\n</code></pre> <pre><code>dn: dc=k8sengineers,dc=com\nobjectClass: top\nobjectClass: dcObject\nobjectClass: organization\no: k8sengineers.com\ndc: k8sengineers\nstructuralObjectClass: organization\nentryUUID: 0139eef2-f01c-103b-8cf7-cb31a244bcdd\ncreatorsName: cn=admin,dc=k8sengineers,dc=com\ncreateTimestamp: 20211213045057Z\nentryCSN: 20211213045057.125264Z#000000#000#000000\nmodifiersName: cn=admin,dc=k8sengineers,dc=com\nmodifyTimestamp: 20211213045057Z\n\ndn: cn=admin,dc=k8sengineers,dc=com\nobjectClass: simpleSecurityObject\nobjectClass: organizationalRole\ncn: admin\ndescription: LDAP administrator\nuserPassword:: e1NTSEF9aXpJOXpJT2tTUUNIUzkrVzJpUVQ5L1M4WVRzZjMvUU4=\nstructuralObjectClass: organizationalRole\nentryUUID: 013a5ebe-f01c-103b-8cf8-cb31a244bcdd\ncreatorsName: cn=admin,dc=k8sengineers,dc=com\ncreateTimestamp: 20211213045057Z\nentryCSN: 20211213045057.128175Z#000000#000#000000\nmodifiersName: cn=admin,dc=k8sengineers,dc=com\nmodifyTimestamp: 20211213045057Z  </code></pre></p>"},{"location":"miscellaneous/ldap/#add-base-dn-for-users-and-groups","title":"Add base dn for Users and Groups","text":"<p>The next step is adding a base DN for users and groups. Create a file named basedn.ldif with the below contents:</p> <p><pre><code>vim basedn.ldif\n</code></pre> <pre><code>dn: ou=people,dc=k8sengineers,dc=com\nobjectClass: organizationalUnit\nou: people\n\ndn: ou=groups,dc=k8sengineers,dc=com\nobjectClass: organizationalUnit\nou: groups\n</code></pre> Replace k8sengineers and com with your correct domain components.</p> <p>Now add the file by running the command:</p> <p><pre><code>ldapadd -x -D cn=admin,dc=k8sengineers,dc=com -W -f basedn.ldif\n</code></pre> Enter LDAP Password:</p> Output <pre><code>adding new entry \"ou=people,dc=k8sengineers,dc=com\"\nadding new entry \"ou=groups,dc=k8sengineers,dc=com\"\n</code></pre>"},{"location":"miscellaneous/ldap/#add-user-accounts-and-groups","title":"Add User Accounts and Groups","text":"<p>Generate a password for the user account to add.</p> <p><pre><code>sudo slappasswd\n</code></pre> Set the Password</p> Output <pre><code>{SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx\n</code></pre> <p>Create a ldif file for adding users.</p> <p><pre><code>vim ldapusers.ldif\n</code></pre> <pre><code>dn: uid=admin,ou=people,dc=k8sengineers,dc=com\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\nobjectClass: shadowAccount\ncn: admin\nsn: Wiz\nuserPassword: {SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx\nloginShell: /bin/bash\nuidNumber: 2000\ngidNumber: 2000\nhomeDirectory: /home/admin\n</code></pre></p> <ul> <li>Replace admin with the username to add</li> <li>dc=k8sengineers,dc=com with your correct domain values.</li> <li>cn &amp; sn with your Username Values</li> <li>{SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx with your hashed password</li> </ul> <p>When done with the edit, add an account by running.</p> <pre><code>ldapadd -x -D cn=admin,dc=k8sengineers,dc=com -W -f ldapusers.ldif </code></pre> Output <pre><code>adding new entry \"uid=admin,ou=people,dc=k8sengineers,dc=com\"\n</code></pre> <p>Do the same for the group. Create a ldif file:</p> <p><pre><code>vim ldapgroups.ldif\n</code></pre> <pre><code>dn: cn=admin,ou=groups,dc=k8sengineers,dc=com\nobjectClass: posixGroup\ncn: admin\ngidNumber: 2000\nmemberUid: admin\n</code></pre> Add group:</p> <pre><code>ldapadd -x -D cn=admin,dc=k8sengineers,dc=com -W -f ldapgroups.ldif\n</code></pre> Output <pre><code>adding new entry \"cn=admin,ou=groups,dc=k8sengineers,dc=com\"\n</code></pre> <p>You can combine the two into single file.</p>"},{"location":"miscellaneous/ldap/#install-ldap-account-manager-on-ubuntu","title":"Install LDAP Account Manager on Ubuntu","text":"<p>Install Apache Web server &amp; PHP <pre><code>sudo apt -y install apache2 php php-cgi libapache2-mod-php php-mbstring php-common php-pear\n</code></pre></p> <ul> <li> <p>For Ubuntu 22.04:sudo a2enconf php8.0-cgi</p> </li> <li> <p>For Ubuntu 20.04:sudo a2enconf php7.4-cgi</p> </li> <li> <p>For Ubuntu 18.04: sudo a2enconf php7.2-cgi</p> </li> </ul> <p>Here I'm using Ubuntu 20.04:</p> <p><pre><code>sudo a2enconf php7.4-cgi\n</code></pre> <pre><code>sudo systemctl reload apache2\n</code></pre> Install LDAP Account Manager <pre><code>sudo apt -y install ldap-account-manager\n</code></pre> Configure LDAP Account Manager <pre><code>http://&lt; IP address &gt;/lam\n</code></pre> </p> <p>The LDAP Account Manager Login form will be shown. We need to set our LDAP server profile by clicking on[LAM configuration] at the upper right corner. </p> <p>Then click on, Edit server profiles</p> <p>This will ask you for the LAM Profile name Password</p> <p>Note</p> <p>The default password is lam</p> <p> </p> <p>The first thing to change is Profile Password, this is at the end of the General Settings page.</p> <p></p> <p>Next is to set the LDAP Server address and Tree suffix. Mine looks like below, you need to use your Domain components as set in the server hostname.</p> <p></p> <p>Set Dashboard login by specifying the admin user account and domain components under the \u201cSecurity settings\u201d section</p> <p></p> <p>Switch to the \u201cAccount types\u201d page and set Active account types LDAP suffix and List attributes.</p> <p></p>"},{"location":"miscellaneous/ldap/#add-user-accounts-and-groups-with-ldap-account-manager","title":"Add user accounts and groups with LDAP Account Manager","text":"<p>Log in with the account admin to the LAM dashboard to start managing user accounts and groups.</p> <p></p> <p>Add User Group</p> <p>Give the group a name, optional group ID, and description. </p> <p>Add User Accounts Once you have the groups for user accounts to be added, click on Users &gt; New user to add a new user account to your LDAP server. You have three sections for user management:</p> <ul> <li>Personal \u2013 This contains the user\u2019s personal information like the first name, last name, email, phone, department, address e.t.c</li> </ul> <p></p>"},{"location":"miscellaneous/ldap/#configure-your-ubuntu-220420041804-as-ldap-client","title":"Configure your Ubuntu 22.04|20.04|18.04 as LDAP Client","text":""},{"location":"miscellaneous/ldap/#the-last-step-is-to-configure-the-systems-in-your-network-to-authenticate-against-the-ldap-server-weve-just-configured","title":"The last step is to configure the systems in your network to authenticate against the LDAP server we\u2019ve just configured:","text":"<p>Add LDAP server address to /etc/hosts file if you don\u2019t have an active DNS server in your network. <pre><code>sudo vim /etc/hosts\n</code></pre> <pre><code>192.168.18.50 ldap.k8sengineers.com\n</code></pre> Install LDAP client utilities on your Ubuntu system:</p> <p><pre><code>sudo apt -y install libnss-ldap libpam-ldap ldap-utils\n</code></pre> Begin configuring the settings to look like below * Set LDAP URI- This can be IP address or hostname </p> <ul> <li> <p>Set a Distinguished name for the search base </p> </li> <li> <p>Select LDAP version 3</p> </li> <li>Select Yes for Make local root Database admin</li> <li>Answer No for Does the LDAP database require login?</li> <li>Set LDAP account for root, something like cn=admin,cd=k8sengineers,cn=com</li> <li>Provide LDAP root account Password</li> </ul> <p>After the installation, edit /etc/nsswitch.conf and add ldap authentication to passwd and group lines. <pre><code>vim /etc/nsswitch.conf\n</code></pre> <pre><code>passwd: compat systemd ldap\ngroup: compat systemd ldap\nshadow: compat\n</code></pre></p> <p>Modify the file /etc/pam.d/common-password Remove use_authtok on line 26 to look like below. <pre><code>vim /etc/pam.d/common-password\n</code></pre> <pre><code>password [success=1 user_unknown=ignore default=die] pam_ldap.so try_first_pass\n</code></pre></p> <p>Enable creation of home directory on the first login by adding the following line to the end of file /etc/pam.d/common-session <pre><code>session optional pam_mkhomedir.so skel=/etc/skel umask=077\n</code></pre> See the below screenshot: </p> <p>Test by switching to a user account on LDAP</p> <pre><code>sudo su - &lt;username&gt;\n</code></pre>"},{"location":"miscellaneous/nexus/","title":"Nexus Installation Setup","text":"Instance Details Resources CentOS - t2.medium 4GB Ram , 2 CPU Nexus Script <pre><code>#!/bin/bash\nyum install java-1.8.0-openjdk.x86_64 wget -y   \nmkdir -p /opt/nexus/   \nmkdir -p /tmp/nexus/                           cd /tmp/nexus\nNEXUSURL=\"https://download.sonatype.com/nexus/3/latest-unix.tar.gz\"\nwget $NEXUSURL -O nexus.tar.gz\nEXTOUT=`tar xzvf nexus.tar.gz`\nNEXUSDIR=`echo $EXTOUT | cut -d '/' -f1`\nrm -rf /tmp/nexus/nexus.tar.gz\nrsync -avzh /tmp/nexus/ /opt/nexus/\nuseradd nexus\nchown -R nexus.nexus /opt/nexus \ncat &lt;&lt;EOT&gt;&gt; /etc/systemd/system/nexus.service\n[Unit]                                                                          \nDescription=nexus service                                                       \nAfter=network.target                                                            \n\n[Service]                                                                       \nType=forking                                                                    \nLimitNOFILE=65536                                                               \nExecStart=/opt/nexus/$NEXUSDIR/bin/nexus start                                  \nExecStop=/opt/nexus/$NEXUSDIR/bin/nexus stop                                    \nUser=nexus                                                                      \nRestart=on-abort                                                                \n\n[Install]                                                                       \nWantedBy=multi-user.target                                                      \n\nEOT\n\necho 'run_as_user=\"nexus\"' &gt; /opt/nexus/$NEXUSDIR/bin/nexus.rc\nsystemctl daemon-reload\nsystemctl start nexus\nsystemctl enable nexus\n</code></pre>"},{"location":"miscellaneous/sonarqube/","title":"SonarQube Installation Script","text":"Instance Details Resources Ubuntu - t2.medium 4GB Ram , 2 CPU SonarQube Script <pre><code>#!/bin/bash\ncp /etc/sysctl.conf /root/sysctl.conf_backup\ncat &lt;&lt;EOT&gt; /etc/sysctl.conf\nvm.max_map_count=262144\nfs.file-max=65536\nulimit -n 65536\nulimit -u 4096\nEOT\ncp /etc/security/limits.conf /root/sec_limit.conf_backup\ncat &lt;&lt;EOT&gt; /etc/security/limits.conf\nsonarqube   -   nofile   65536\nsonarqube   -   nproc    409\nEOT\n\nsudo apt-get update -y\nsudo apt-get install openjdk-11-jdk -y\nsudo update-alternatives --config java\njava -version\n\nsudo apt update\nwget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add -\n\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" &gt;&gt; /etc/apt/sources.list.d/pgdg.list'\nsudo apt install postgresql postgresql-contrib -y\n#sudo -u postgres psql -c \"SELECT version();\"\nsudo systemctl enable postgresql.service\nsudo systemctl start  postgresql.service\nsudo echo \"postgres:admin123\" | chpasswd\nrunuser -l postgres -c \"createuser sonar\"\nsudo -i -u postgres psql -c \"ALTER USER sonar WITH ENCRYPTED PASSWORD 'admin123';\"\nsudo -i -u postgres psql -c \"CREATE DATABASE sonarqube OWNER sonar;\"\nsudo -i -u postgres psql -c \"GRANT ALL PRIVILEGES ON DATABASE sonarqube to sonar;\"\nsystemctl restart  postgresql\n#systemctl status -l   postgresql\nnetstat -tulpena | grep postgres\nsudo mkdir -p /sonarqube/\ncd /sonarqube/\nsudo curl -O https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.3.0.34182.zip\nsudo apt-get install zip -y\nsudo unzip -o sonarqube-8.3.0.34182.zip -d /opt/\nsudo mv /opt/sonarqube-8.3.0.34182/ /opt/sonarqube\nsudo groupadd sonar\nsudo useradd -c \"SonarQube - User\" -d /opt/sonarqube/ -g sonar sonar\nsudo chown sonar:sonar /opt/sonarqube/ -R\ncp /opt/sonarqube/conf/sonar.properties /root/sonar.properties_backup\ncat &lt;&lt;EOT&gt; /opt/sonarqube/conf/sonar.properties\nsonar.jdbc.username=sonar\nsonar.jdbc.password=admin123\nsonar.jdbc.url=jdbc:postgresql://localhost/sonarqube\nsonar.web.host=0.0.0.0\nsonar.web.port=9000\nsonar.web.javaAdditionalOpts=-server\nsonar.search.javaOpts=-Xmx512m -Xms512m -XX:+HeapDumpOnOutOfMemoryError\nsonar.log.level=INFO\nsonar.path.logs=logs\nEOT\n\ncat &lt;&lt;EOT&gt; /etc/systemd/system/sonarqube.service\n[Unit]\nDescription=SonarQube service\nAfter=syslog.target network.target\n\n[Service]\nType=forking\n\nExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start\nExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop\n\nUser=sonar\nGroup=sonar\nRestart=always\n\nLimitNOFILE=65536\nLimitNPROC=4096\n\n\n[Install]\nWantedBy=multi-user.target\nEOT\n\nsystemctl daemon-reload\nsystemctl enable sonarqube.service\n#systemctl start sonarqube.service\n#systemctl status -l sonarqube.service\napt-get install nginx -y\nrm -rf /etc/nginx/sites-enabled/default\nrm -rf /etc/nginx/sites-available/default\ncat &lt;&lt;EOT&gt; /etc/nginx/sites-available/sonarqube\nserver{\n    listen      80;\n    server_name sonarqube.groophy.in;\n\n    access_log  /var/log/nginx/sonar.access.log;\n    error_log   /var/log/nginx/sonar.error.log;\n\n    proxy_buffers 16 64k;\n    proxy_buffer_size 128k;\n\n    location / {\n        proxy_pass  http://127.0.0.1:9000;\n        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;\n        proxy_redirect off;\n\n        proxy_set_header    Host            \\$host;\n        proxy_set_header    X-Real-IP       \\$remote_addr;\n        proxy_set_header    X-Forwarded-For \\$proxy_add_x_forwarded_for;\n        proxy_set_header    X-Forwarded-Proto http;\n    }\n}\nEOT\nln -s /etc/nginx/sites-available/sonarqube /etc/nginx/sites-enabled/sonarqube\nsystemctl enable nginx.service\n#systemctl restart nginx.service\nsudo ufw allow 80,9000,9001/tcp\n\necho \"System reboot in 30 sec\"\nsleep 30\nreboot\n</code></pre> SonarQube-Analysis-Properties <pre><code>sonar.projectKey=vprofile\nsonar.projectName=vprofile-repo\nsonar.projectVersion=1.0\nsonar.sources=src/\nsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/\nsonar.junit.reportsPath=target/surefire-reports/\nsonar.jacoco.reportsPath=target/jacoco.exec\nsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml\n</code></pre>"}]}